<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[新品推广思路]]></title>
    <url>%2F2019%2F04%2F25%2F%E7%94%B5%E5%95%86%E8%BF%90%E8%90%A5%2F%E6%96%B0%E5%93%81%E6%8E%A8%E5%B9%BF%E6%80%9D%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[新品推广打造爆款思路 运营思路第一阶段（上架前）标题策划 可以在淘宝首页搜索“密码门锁” 收集下拉推荐词 在市场洞察搜索分析里搜索“密码门锁”，收集在线商品数/搜索人气比值较高的关键词 收集竞争对手的成交关键词、引流关键词 在收集的关键词里筛选合适的关键词组成标题 产品上架后根据数据分析标题的效果，对关键词进行优化 主图策划 定位市场，锁定对手，全方位分析对手的主图，以求超越 根据对手主图及自身产品特点对产品图进行创新 对文案进行创新，策划出用户感兴趣的、对用户有价值的文案 利用调色轮对背景色进行创新，与对手进行区分 利用直通车测试主图点击率，对比自己和对手主图的点击率，改进优化主图 详情页策划 调研竞品的详情页，提炼他们的卖点，研究他们的销售逻辑，寻找它们可能存在的薄弱点 调研用户的需求和特点： 通过分析竞品的评价、问大家 通过客服聊天记录寻找用户关心点 通过自己对于产品的经验积累 列出产品优势，提炼出差异化的购买理由： 大小、使用价值、材质、设计款式、包装、安全性、价格、品牌、服务、工艺、赠品等 利用符号传递购买理由和价值 欧美符号、大牌符号、重量符号、奢华符号、用户身边的比用户高一个层级的符号（激发客户的梦想） 解答核心词（非必选） 消除消费者心中最大的疑问，放在详情页的第一屏 调动、放大消费者情绪，尽量与对手差异化： 从众（我们的产品销售过万、我们的产品被抢疯了、看看我们的产品有多红、众多明星的选择） 紧迫感（历史最低价活动结束立刻涨价、仅此一次）、 稀缺（产品有限即将断货）、 恐吓（你还在钥匙丢失带来的烦恼吗？你还在忍受专业窃贼小黑盒开锁带来的困扰吗） 占便宜（亏本大赠送、今天聚划算好礼送不停） 教育客户（德国品牌安全稳定，我们更放心）、 幻想（激发客户的幻想，到国际大牌官网查找相匹配的高档点）、 话题（拿什么拯救你我的身材） 提高客户信任度 客户见证（客户级别展示众多淘宝VIP的选择、黄钻会员评价见证、评分图、火爆咨询图、感谢信见证 、老客户成交纪录见证、买家秀见证、各平台的好评、回头客的展示、展现回头客数据） 权威证明（产品证书证明、授权书、各种证书奖状、设计专利证书、全国xx家实体店同步推荐、质检报告） 销量展示（全网销量、人气指数、搜索排行第一、展示销售速度） 借力大牌（同品比价、国际一线品牌材料同步） 借力明星（明星同款、明星代言、创始人合照） 优劣对比（优劣图对比、盗版原创声明） 借力媒体（央视广告、媒体广告、线下广告） 借力实体店（商场零售价格对比、线下实体店） 借力知识（权威百科平台知识介绍） 产品背后的故事（展现开发者/设计师身份、展现原材料来源、产品解剖图、生产工艺揭秘、大学生/农民创业、企业团队展示、企业历史、品牌故事） 服务展示（先试用后买单、专业高档的包装、店铺质量得分、零风险购物终生无理由退换货、客服群号展示、） 借力国外（国外价格对比、国外独家授权） 产品细节展示 产品的颜色、使用方法、设计理念、常见问题 利用Axure设计详情页初稿，与美工沟通实现 手淘详情页排版优化 手机屏幕尺寸： 宽：长=3：4，以苹果6s为准 文字：字体要大，中文字体&gt;=30号字，英文和数字&gt;=20号字 图片：要有整体感，手机一屏高度=PC两屏高度 排版：尽量上下排版，一个特点占一屏，小特点占半屏 页面：6屏左右，只讲核心，主图的补充 宝贝评价：前三条，高信誉卖家，表现产品特点 引导客户收藏加购 主图视频策划 用视频更好的表达产品的卖点以及很难用页面去描述清楚的一些特征 把详情页压缩成主图视频（品牌、款式、功能、效果、材质、赠品、颜色、工艺、产品技术、使用方法、买家秀、配件、售后、包装、快递） 讲有竞争力的核心卖点、独特的功能和效果、差异化的购买理由 围绕产品周边的配件，选择最匹配产品的符号化的场景 评价准备 收集竞品的比较优秀的、适合自己产品的评价，加以修改优化，为后面补单评价作准备 确定补单计划 锁定对手，以他的销量作为参考标准 初始的UV价值和支付转化率可以参考同行的平均水平，比对方高即可； 第一周的销量保持销量递增，人工干预销量，等待真实的出单 前3天的基础销量不需要过于看重有多少访客，第一周内销量递增，第一周内最后一天日销量达到行业平均水平 3天以后保持销量递增，销量无规律递增 前面工作到位的情况下，一般7~10天开始真实出单，当真实出单后依据数据变化决定补多少单，具体数据见第一条 干预销量的工作要持续一个月，保证产品上架后的第一个月数据要非常稳定 安排进店的途径分散，不要集中 补单尽量分布在不同时间段，降低风险 后面的真实转化率要跟上，否则如果正常操作到两周还是没有起色就需要反思是产品问题还是基本工作与节奏没有把握好，完全依赖于补单不可取 收藏加购配合计划 需要提前准备大量的人愿意配合操作 一般在约7–10天后收藏与加购同类产品（11月1日要安排补3单，那么这三个人提前三天之内与收藏与加购同类产品，11月1日再到自己店铺购买） 安排少部分的人购买同行产品后再申请退款后再拍自己的产品 收藏加购率大概5%–10% 第二阶段（营销推广引流）直通车推广 收集产品关键词 直通车系统推荐 淘宝首页类目关键词 淘宝热搜排行榜 淘宝搜索下拉菜单 生意参谋中-市场行情中的搜索词分析 自己店铺的流量词和成交词 竞争对手的流量词成交词 删选关键词 相关性 展现数 点击指数 点击率 转化率 新建推广计划、选择推广宝贝 选择关键词、修改出价 设置日限额、投放平台、投放时间、投放地域 设置匹配方式（广泛匹配、精选匹配） 设置添加人群，修改溢价，精准营销 测试、优化车图及文案 建立定向推广计划 人群定向（访客定向、购物意图定向、搜索重定向、智能投放） 位置定向（淘宝活动、猜你喜欢、掌柜热卖、物流详情、已买宝贝） 根据数据报表进行优化 钻展推广 设置推广计划 圈定投放人群、位置 设置钻展扣费方式 CPC（点击收费） CPM（千次展现收费） 设计钻展创意图片、设置承接页 投放主动寻找目标客户 淘宝客推广 设置推广计划、佣金比例、优惠券 选图及策划文案配合淘宝客推广 参加淘宝客活动 删选出优质淘客并制定维护策略 建立通过淘客成交客户的客户鱼塘 淘内营销活动推广（淘抢购、聚划算、618、双11等） 活动策划、提前预热 设置关联宝贝 打造营销活动氛围 首页专题、活动专题页、详情页都做好聚划算活动的相关指引，并创造紧张抢拍的气氛。 老客营销（提前短信、邮件、旺旺、微信通知老客户） 达人、直播、第三方平台推广抖音、小红书、微博等 微信营销 养号维护 微信个人形象和背景故事的设计 朋友圈文案布局：40%日常生活，20%正能量，20%产品，20%团队 引流成交 引流加粉设计引导顾客加微信的话术 建立信任，根据客户特点主动与其聊天 建立标签，通过客户的属性喜好和消费能力对客户进行细分 寻找适合自己用户群体的产品，通过朋友圈对产品进行测试 复购加绑定客户终生价值 培养用户的复购、客单价的递增、交易次数的累积、丰富产品的多样性 策划合理的会员机制，绑定客户，实现终生价值的影响 第三阶段（数据分析与优化） 利用生意参谋监控自身产品与竞品的各项数据：销售额、访客数、访问深度、支付转化率、客单价、收藏加购率、UV价值 监控竞争对手的最新营销动向，及时跟进或超越 监控产品评价与售后，及时解决相应问题 想法 该产品在“销量排序”下排到十几名，但价格相对头部卖家来说还比较有优势，目前可以继续低价冲销量，等销量达到前几名时可以把日常售价提到1500左右 颜色款式较少，只有铜和金两种，可以生产一些同行卖的比较好的颜色款式 “问大家”里不少用户比较关心开锁时的声音大小，可以加进详情页中 没有设置买家秀，可以将可以将一些优质的图片评价放进买家秀 主图视频只介绍了产品的设计理念，应该把用户比较关心的卖点添加进视频，让用户更加方便地了解这款产品 这款产品评价里存在没有妥善处理的差评，影响产品的转化与店铺评分，解决方法： 旺旺或电话及时主动联系顾客，沟通解决方案，让顾客删掉差评，然后改善自身问题，固化类似问题解决方案 店主对差评进行回复，态度要诚恳，表示该问题已妥善解决 差评优化，在差评里刷好评，把差评压下去 用第三方技术删除或遮挡差评（存在一定风险） 该产品后台没有设置自动回复，可以搜集常见问题设置自动回复，提高回复速度，降低客服压力 店侦探显示此产品没有开通直通车，可以开通直通车推广引流，控制好ROI 这款产品已经有了一定销量，应加大付费推广引流力度，积极参与各种活动，做好关联营销及搭配带动店里其他几款销量较低的产品]]></content>
      <categories>
        <category>电商运营</category>
      </categories>
      <tags>
        <tag>电商运营</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[淘宝天猫运营学习笔记]]></title>
    <url>%2F2019%2F04%2F25%2F%E7%94%B5%E5%95%86%E8%BF%90%E8%90%A5%2F%E6%B7%98%E5%AE%9D%E5%A4%A9%E7%8C%AB%E8%BF%90%E8%90%A5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[淘宝天猫运营学习笔记 淘宝天猫爆款1.基础操作pass 2.判断产品能不能做1.通过关键词分析市场特点和需求生意参谋 – 市场行情 – 搜索词查询 如何策划主题详情页： ​ 根据搜索词判断需求 2.分析市场竞争度，选择市场的标准关键词倍数 = 每日搜索人气 / 在线商品数 商场点击占比 &lt; 40% 1000 &lt;= 每日搜索人气 &lt;= 10000 最少三个关键词，最好3 - 6个 在线商品数最好在5000以内 有空白价格带，宽度最少三四十元 3.如何寻找新的细分市场大类目中找小类目，小类目中找细分 从“发布宝贝”中选择细分市场，往往二级类目的类目词就是这个类目的核心词 在行业粒度中，看子类目的竞品流量，关键词，销量，利润 在淘宝主页中搜索关键词，看类目是否有主流品牌，价格带利润分析，销量分析 4.确定词语下的弱对手查看商品的销量前30名，统计他们的销量、价格、C店or天猫、营业额、地域 先定价，再定品（更适合于标品） 5.如何扒光对手的数据分析竞品流量来源，（市场行情 –&gt; 行业粒度） 分析竞品引流关键词 用第三方工具综合分析商品 6.为什么不能进入高竞争领域高竞争领域的特征：资金投入过大 供应商起步生产量较高 行业起步销量高 店铺整体要求细节高 对运营技术要求高 上游核心资源已经被对方占领 对推广资源要求高（直播、达人、淘宝客） 7.低竞争行业的特征新型产品（关注热门：动画片、电视剧、科技新闻等） 季节性产品（灭蝇枪） 搜索指数大，商城点击少 页面没有专业的设计师 产品毛利润比较高 8.同质同价，应如何应对（在竞争不强的领域，很多买家同质同价也能存活，但厮杀随时会到来，可能会变成恶意竞价） 1.前期低价冲销量（可能会引起同行降价竞争），销量上去后与同行共同涨价 2.挂羊头，卖全羊（用A产品低价引流，主卖B产品） 3.多卖法（满7送1 –&gt; 满8送7) 4.策划法（主图PCI方法（商品一样，顾客先点谁就买谁）） 5.淘客法（拉动销量） 6.降成本（ ​ 产品成本（材料，包装盒） ​ 仓库成本（重新摆放（根据销量）、让厂家承担一部分仓库，拉长进货周期） ​ 快递成本（根据销量每阶段谈价格，多方询价，调整结算方式，延长结算周期） ​ 其他成本（办公地点，人员效率） 9.如何把产品卖贵先定客户群，再定价（聚焦那些能够付得起高价格的小众） （分析谁家卖的贵，他的客户群是谁？怎样能在他的基础上微创新，更好的满足该客户群？从而找到这些更好的货源 或 改进自己的产品与服务） 3.产品流量如何获取1.流量分配逻辑不同类目侧重点不同，同样的原则在不同类目里权重不同 了解规则核心是前提 根据规则积极测试 根据测试结果再调整 2.淘宝天猫流量渠道介绍pass 3.搜索个性化介绍流量渠道越碎片化，机会越多 卖产品前要充分分析市场，找准自己的定位 4.综合排序介绍1.影响综合排序的因素： 店铺数据：动态评分、退款纠纷率、投诉维权处罚等 商品质量：标题、关键词、主图、详情图片、价格、属性等 商品数据：销量、人气、收藏、转化率、客单价、下架时间等 处罚类：店铺违规，虚假交易、处罚、低价交易、假货、历史违规等 5.销量排序介绍1.综合排名下，页面显示的付款人数的计算方式： 综合排名下显示的是近30天已经付款的买家人数，去除重复购买，去除退货退款,去除不计销量订单。双11当天所有商品成交销量不计入搜索销量。近30天内，同一会员购买多笔只记一人，购买多次也只记一人。 2.按销量排名，页面显示的收货人数计算方式： 统计的是近30天确认收货的买家人数，去除重复购买，去除退货退款,去除不计销量订单，近30天内，同一会员购买多笔只记一人，购买多次也只记一人。 3.宝贝首页的月销笔数计算方式： 显示近30天已付款的销售件数 4.销量排序的个性化 销量排序下也会存在个性化，但是个性化的大小和类目有关系 6.手淘搜索流量如何获取搜索流量获取 ——&gt; 发布产品（找准市场，切入自己最有把握的市场）——&gt; 系统识别产品，分配展现量（点击率、收藏率、加购率）——&gt; 产品被客户搜索看到（类目正确、关键词找对）——&gt; 产品被意向客户点开&lt;点击量、点击率&gt;（主图很主要、合适的定价、销量高低、天猫、品牌）——&gt; 转化成交&lt;转化率、销售额&gt;（比拼内功各个环节、产品、图片、评价、销量、价格、服务、客服） 4.如何写好标题1.标题的作用1.宏观作用： 标题 = N个词 = N个市场 = N种人群 = N个价格段 找到合适的词，满足词语背后这类人的需求 2.具体作用： 词语让系统识别出产品，并将产品展示出来 每个关键词都是一个渠道，一个获得流量的渠道 2.如何寻找合适的关键词1.方法： 市场洞察 下拉框推荐词 2.原则： 与产品匹配 类目选择正确（填写数值更好的类目） 数据筛选 3.如何组合关键词1.符合搜索习惯 2.符合阅读习惯 3.包含必要信息 4.如何分析标题效果1.分析什么： 关键词能否带来曝光（流量） 关键词带来的曝光（流量）是否精准 2.如何分析： 订购流量纵横 生意参谋——&gt;商品效果——&gt;单品分析（7天）——&gt;详情（访客数、收藏数、加购数、支付买家数、支付转化率） 5.新品712引爆方法（新品推广）1.确认目标竞品确认关键词——&gt; 产品定价——&gt; 价格/销量/品牌/功能/风格/材质…等维度进行判断——&gt; 锁定竞品 统计数据，各个价格段的销售和竞争情况（价格、收货人数、销售额、店铺名） 确定进入哪个价格段 依据标定的对手和成本情况进行定价 2.确定单量计划原则： 锁定对手，以他的销量作为参考标准 订购市场行情或竞争情报，可以查看对手每天的销售件数 ，更准确的计算月销量（市场行情–&gt;商品店铺榜–&gt;行业粒度–&gt;流量商品榜） 对手真实的销量在收货人数的基础上上浮月20% 补单方法： 核心是保持每天的产出稳定，关注UV价值、支付金额、支付转化率、加购&amp;收藏率这几个数据 初始的UV价值和支付转化率可以参考同行的平均水平，比对方高即可； 第一周的销量保持销量递增，人工干预销量，等待真实的出单 前3天的基础销量不需要过于看重有多少访客，第一周内销量递增，第一周内最后一天日销量达到行业平均水平 3天以后保持销量递增，销量无规律递增 前面工作到位的情况下，一般7~10天开始真实出单，当真实出单后依据数据变化决定补多少单，具体数据见第一条 干预销量的工作要持续一个月，保证产品上架后的第一个月数据要非常稳定 安排进店的途径分散，不要集中 补单尽量分布在不同时间段，降低风险 后面的真实转化率要跟上，否则如果正常操作到两周还是没有起色就需要反思是产品问题还是基本工作与节奏没有把握好，完全依赖于补单不可取 3.初期客户进店方式关键词进店主要分为两种: 1.全标题方式： 一般更适合有风格、有调性的产品，会存在较多的长尾词，例如各种服饰 不适合类似标品这样关键词较少的产品 全标题进店方式需要详细记录给出展现的词语有哪些，从第四天开始 每一个有展现的词语成交2–3单，继续记录第二天的数据情况 重复以上过程，直至关键词稳定 2.长尾词方式： 一般更适合词语不多，没有明显风格、调性的产品 比如“结婚礼物”这个词是产品的核心词 可以延伸出“结婚礼物 实用 闺蜜“、”结婚礼物 创意 高档“、”结婚礼物 送礼“等词语 无论是什么词语，都必须与产品匹配 前三天分布5–8个关键词 第四天开始需要记录数据，看那些词语获得展现，然后围绕这些词语获得2–3单，继续记录数据 重复上述过程，直至关键词稳定 直通车跟进成交词 将有成交的词语全部添加进直通车 让成交词语获得更多曝光，加速成交 修正人群标签 小结： 所有的早期工作都是为了让系统尽快的识别出自己的产品，关键词就是媒介 开始中长尾的词语带来的成交帮助相对会更大 最终中长尾词积累的销量优势会累积給大词，即产品销量稳定后成交主要集中在几个大词上 标品因为词语上，大词竞争激烈，初期推广的成本会更大 4.收藏加购配合计划重要注意事项： 需要提前准备大量的人愿意配合操作 一般在约7–10天后收藏与加购同类产品（11月1日要安排补3单，那么这三个人提前三天之内与收藏与加购同类产品，11月1日再到自己店铺购买） 安排少部分的人购买同行产品后再申请退款后再拍自己的产品 收藏加购率大概5%–10% 5.常见安全账号获取途径100单以下解决方案： 方案一：从身边的亲戚朋友入手 流程说明： a）寻找身边的亲戚、朋友、员工帮助自己完成最初的销量和评价，每人找3–5个人 b）询问其中是否有人愿意介绍身边更多的人帮忙，给予合适的利益 方案二：加入各种垂直群（以家长群为例） 流程说明： a）核心是赠品上要有针对性 b）给有意愿的其他家长送质量较好的毛巾，他们会自发传播 方案三：从老顾客中筛选 流程说明： a）针对有积攒过老顾客的店铺 b）通过微信朋友圈，筛选出愿意配合做销量的老顾客 c）内容要有诱惑，折扣在允许范围内足够大，保证吸引力 d）如果开始朋友圈的人非常少，就直接把利润全部让掉保证有足够的吸引力或者群发 方案四：参加试客平台 流程说明： a）商家提供产品给平台，平台通过赠送或者高折扣的机制给商家带来真实的访客和成交 b）合作的平台：聚折良品&amp;琳琅秀 100—1000单解决方案： 方案一：给已经成交的客户留言 a）已经成交的客户，立刻旺旺留言，询问是否愿意参加免费试用的活动 b）同时通过短信同步发送给客户，并留下微信号 c）不断地把成交的客户攒到微信里 方案二：商品包装里留下刮刮卡 a）产品包装里预留刮刮卡，刮刮卡的设计形式需要多测试 b）优惠和预留的二维码一定要非常醒目 c）具体给多少优惠，这个根据各自的产品利润和推广预算灵活调整 方案三：给客户打电话 a）安排专人给客人互打电话回访，询问产品使用情况。客户的反应只有两种：满意或者不满意 b）针对满意的客户可以引导其是否愿意参加免费试用的活动，愿意的加微信 c）针对不满意的客户，那么加微信后帮助他解决问题 d）打电话的节点，发货后的2—5天之内为最佳 e）订单量少的，建议全部电话，订单量多的，简历三种方式相互配合，电话针对其中黄钻以上的用户 方案四：通过公众号积累粉丝 a）一般适合店铺粉丝积累多，产品会经常上新的店铺 b）公众号里直接发文做产品预售，优惠销售，快速获得销量和评价 方案五：针对校园大学生 a）首先要看自己的客户人群是否与大学生的群体特征匹配 b）联系学校的外联部负责人，给予适当费用让其帮助推广 c）校园内通过送赠品加微信 1000单以上的解决方案 方案一：AB店模式 a）手上至少拥有两个店铺，店铺卖同类产品 b）A店铺目的是获取客户资料，筛选出愿意配合的人，提供给B店铺使用 c）B店铺要做有利润有竞争力的产品 d）A店铺只要保证不亏损即可，从客户中筛选账号加微信的方法参照前面 方案二：获取客户资料 a）适合做标品，以及类目里TOP级卖家 b）与后面销售业绩一般的卖家进行谈判，花钱买客户资料 c）拿到资料后安排专人逐一打电话筛选 6.微信操作技巧技巧一：朋友圈内容要点 a）发朋友圈的目的是从微信里筛选人，挑愿意配合的人 b）内容要有诱惑，折扣在利润允许范围内足够大，保证吸引力 c）如果开始朋友圈的人非常少，就直接把利润全部让掉保证有足够的吸引力或者群发 技巧二：准备多个微信号 a）一个微信每天能加的人有限，通过的人数有限制，同一时间加人的数量会有限制，保证有五个左右的微信号 b）对所有人一对一发布刷单流程的时候，建议一律采用语音的形式，降低被某些不讲信用的人截图投诉敲诈的风险 技巧三：对愿意配合做销量的人进行分层 a）针对部分特别配合的人，让他单独加另一个微信。这个单独加的微信，起一个比较牛逼的名字，比如【xxxVIP专享】 b）对于这些配合度较高的人，给予最高的优惠力度，不断的攒 7.如何全面监控产品和竞品数据访客数、浏览量、访问深度、支付转化率、支付金额、客单价、加购人数、收藏人数、加购率、收藏率、UV价值 8.新品打造的712法则 第一阶段 70%：（上架前） 市场分析、竞品分析、客户定位、产品定位、主图策划、页面策划、评价准备、标题优化、上架时间 第二阶段 10%： 基础销量、买家秀 第三阶段 20%： 数据分析、渠道分析、页面优化 8.如何打开所有的流量渠道每一个独立关键词 都是一个独立渠道 优化标题就是打开更多的更有竞争力的流量渠道 方法： 查看搜索词（生意参谋–&gt;市场行情–&gt;搜索词查询–&gt;搜索词详情）——&gt;找到搜索词所属类目——&gt;查看引流关键词与成交关键词（生意参谋–&gt;市场行情–&gt;行业粒度–&gt;热销商品榜–&gt;详情–&gt;引流关键词、成交关键词） 9.如何筛选匹配类目的关键词1.大类目相同、子类目不同：可以进一步考虑 2.大类目不相同：就不可以用 10.产品不匹配用户需求pass 11.竞品流量来源的分析（生意参谋–&gt;市场行情–&gt;行业粒度–&gt;热销商品榜–&gt;详情） 手淘搜索or直通车or淘宝客or其他 12.竞品转化率的分析每个流量渠道（关键词）的转化率（引流转化率、成交转化率） 查看地址：行业粒度 13.竞品点击率的分析将竞争对手的主图（原图或者模仿）与自己的主图一起放到直通车上投放（屏蔽竞争对手所在的地区），比较点击率、收藏加购率 14.竞品收藏加购率的分析行业大盘加购收藏率（市场行情–&gt;行业大盘–&gt;大盘走势–&gt;访客数、收藏人数、加购数–&gt;统计到表格中） 竞品的收藏加购率（无法直接看到）（可以每天记录竞品的收藏宝贝增加量（竞品宝贝首页），算出每天的收藏数，安排比率判断加购率） 自己的收藏加购率（生意参谋–&gt;经营分析–&gt;商品效果） 行业大盘数据的1.5–2倍左右为最优数据 15.如何提高首图的点击率（首图点击率非常重要）展示产品的核心卖点（买家最关心的部分） 注意字体大小适应手机淘宝 16.如何确定首图的购买理由1.将竞争对手的主图的购买理由全部统计出来 （注意：商家的购买理由并不一定是买家最关心的购买理由） 2.统计竞品的宝贝评价与问大家 17.如何提高宝贝转化率首图影响点击率，2、3、4、5图影响转化率 把销量最高的sku的图片设为首图 把主图当做详情页设计 手淘转化率 = 2345图 + 宝贝评价 + 详情页 18.如何打造优质宝贝评价宝贝评价的活跃度对宝贝人气有一定影响： 主动评价：评价率高权重也会更高 晒图占比 追评占比 掌柜回复：针对性回复 19.如何提高加购收藏率引导买家先加购收藏 主图上引导（注意按钮遮挡图片重要信息、把字体放下面） 促销标签：先加购收藏 单独添加一个sku 唤醒老顾客加购收藏 20.直通车投放影响手淘排名直通车对自然搜索加权 但有时会影响自然搜索 21.如何恢复宝贝标签 直通车圈人 投放五个左右的精准关键词，卡到前三位，吸引真实流量 开通搜索定向 钻展圈人 定向投放同一价格段、同一风格、相似店铺 6.电商摄影技术pass 7.主图PCI策划方法1.主图PCI三步法定位市场、明确对手、三种创新 2.定位市场清楚产品的人群市场 3.明确对手锁定对手，进行360度全方位分析，才能全面超越 4.产品图创新（P)5.文案创新（C)不要写华丽的自嗨型的、让用户无法感知的文案 多思考用户考虑的是什么，你能给用户带来什么样的价值 6.背景色创新（I)利用调色轮找出对比色，让用户快速区分出你的产品和对手的产品 8.产品页面策划1.确定&amp;分析你的对手 为什么要先确定与分析对手： 找到正确的对手，才能制定出合理的策略 提炼同行的买点，研究他们的销售逻辑 寻找它们可能存在的薄弱点，作为自己的突破口 如何找到你的对手： 通过你的主打关键词在淘宝上搜索，主要看价格段，产品页面 找到和你店铺类似或相同的的同行店铺 分析不同价格段的同行店铺销量多少 价格相近、产品类似、销量比你略多的同行是你的主要竞争对手 2.调研同行的卖点查看对手的详情页面、评价、问大家，逐条记录在案 3.调研用户需求和特点为什么要调研用户需求和特点： 页面就是你产品的销售员，展示产品的窗口 了解客户，把握他们的需求和特点是让客户下单的前提 调研用户需求和特点的方法： 通过客服聊天记录来寻找客户最关心的问题点 通过客户评价和问大家来挖掘 基于自己在行业内的经验积累，把握客户的需求 列出产品优势、提炼产品购买理由 4.详情页初稿设计（Axure）初始尺寸375*667 图片素材：花瓣网、站酷、浏览器的图片搜索功能 5.实现产品差异化的两种思路 功能性购买理由（大小、使用价值、材质、设计款式、包装、安全性、价格、品牌、服务、工艺、赠品等） 情感性购买理由（自信、喜悦、激情、性感等） 6.如何设计产品的购买理由 通过提出差异化的购买理由，来打动目标销售人群 我们不是要做最好的，而是要做某个点最好的，我们的差异化不可能满足的是全部的用户，我们满足的是一小部分认同我们产品的客户 7.如何利用符号实现产品购买理由价值的传递为什么要用符号： 消费者是懒惰的，他不希望通过很复杂的方式来了解产品的购买理由； 人的潜意识更愿意用符号来了解信息 用那些符号： 欧美符号、大牌符号、重量符号、奢华符号、用户身边的比用户高一个层级的符号（激发客户的梦想） 8.核心词的解答（看情况，非必选） 放在详情页的第一屏，消除消费者心中最大的疑问 9.需求（情绪）调动： 调动情绪的部分尽量控制在2—3屏内 抓住该产品该购物场景中最需要激发的情绪进行放大 调动情绪的方法尽量与对手差异化 确保你的策略是有效的购买理由 调动方法： 从众（我们的产品销售过万、我们的产品被抢疯了、看看我们的产品有多红、众多明星的选择） 紧迫感（历史最低价活动结束立刻涨价、仅此一次）、 稀缺（产品有限即将断货）、 恐吓（你是否也被这些问题困扰、您还在吃致癌的烘烤食品吗）、 占便宜（亏本大赠送、今天聚划算好礼送不停 教育客户（天冷了你为父母准备了什么、三个月不晒被六百万螨虫陪你睡）、 幻想（激发客户的幻想，可到国际大牌官网查找相匹配的高档点）、 话题（拿什么拯救你我的身材） 10.客户信任塑造 客户见证 （客户级别展示众多淘宝VIP的选择、黄钻会员评价见证、评分图、火爆咨询图、感谢信见证 、老客户成交纪录见证、买家秀见证、各平台的好评、回头客的展示、展现回头客数据） 权威证明（产品证书证明、授权书、各种证书奖状、设计专利证书、全国xx家实体店同步推荐、质检报告） 销量展示（全网销量、人气指数、搜索排行第一、展示销售速度） 借力大牌（同品比价、国际一线品牌材料同步） 借力明星（明星同款、明星代言、创始人合照） 优劣对比（优劣图对比、盗版原创声明） 借力媒体（央视广告、媒体广告、线下广告） 借力实体店（商场零售价格对比、线下实体店） 借力知识（权威百科平台知识介绍） 产品背后的故事（展现开发者/设计师身份、展现原材料来源、产品解剖图、生产工艺揭秘、大学生/农民创业、企业团队展示、企业历史、品牌故事） 服务展示（先试用后买单、专业高档的包装、店铺质量得分、零风险购物终生无理由退换货、客服群号展示、） 借力国外（国外价格对比、国外独家授权） 11.展示产品价值整体表现的价值： 排版方式： 图与字： 左图右字，右图左字； 上图下字，上字下图； 底图上字 字与字： 深色、大字突出重点文字； 浅色、小字弱化非重点元素 每个细节的价值 需要对想要表达的产品价值更加详尽的讲解一般我们用一屏空间来展示这个特点 要讲的方面： 产品的颜色 产品的使用方法 产品的设计理念（可以将图片素描化作为设计手稿） 产品给谁用？送给谁？ 产品的常见问题 对比而来的价值 一般来说左右对比 简洁突出重点 9.手机端短视频策划1.为什么用视频可以更好的表达产品的卖点以及很难用页面去描述清楚的一些特征 给用户良好的用户体验 提高无线端的转化率 无线端的视频化是一个必然趋势 2.如何策划头图视频的卖点只讲有竞争力的核心卖点 讲独特的功能和效果 讲差异化的购买理由 3.如何将宝贝的卖点视频化把宝贝购买理由视频化 围绕产品周边的配件，选择最匹配产品的符号化的场景 把详情页压缩成主图视频（品牌、款式、功能、效果、材质、赠品、颜色、工艺、产品技术、使用方法、买家秀、配件、售后、包装、快递） 4.设置全景图的好处 客户可以全方位的看到产品细节，解决了手机端图片浏览的局限性 吸引顾客点击，提高消费者对产品的喜好度，并提高转化率 节约客户浏览时间，给客户带来实体购物体验，降低跳失率 10.手淘页面排版优化细节 手机屏幕尺寸： 宽：长=3：4，以苹果6s为准 文字：字体要大，中文字体&gt;=30号字，英文和数字&gt;=20号字 图片：要有整体感，手机一屏高度=PC两屏高度 排版：尽量上下排版，一个特点占一屏，小特点占半屏 页面：6屏左右，只讲核心，主图的补充 宝贝评价：前三条，高信誉卖家，表现产品特点 11.店铺细节设置技巧 12.如何对付中差评 13.客服优化细节 14.生意参谋诊断店铺 15.手淘首页推荐怎么上 16.淘宝活动怎么上 17.猜你喜欢 18.直通车运营技术 19.如何选择爆款 20.进货能力提升 21.电商管理能力 22.服装店从0怎么做 23.天猫品类如何运营 24.工具课程 25.ps课程 26.钻展 电商成功打造产品的正确流程： 渠道分析，分析出竞争渠道 选品+选款，匹配渠道特点 页面策划 摄影，能够表达出策划核心的摄影 页面组合 基础销量、基础评价 客服培训 流量引入、积累目标销量（补单、广告） 占领排名、稳定流量 扩展产品]]></content>
      <categories>
        <category>电商运营</category>
      </categories>
      <tags>
        <tag>电商运营</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Selenium的使用]]></title>
    <url>%2F2018%2F12%2F23%2F%E7%BC%96%E7%A8%8B%2F%E5%8A%A8%E6%80%81%E9%A1%B5%E9%9D%A2%E6%8A%93%E5%8F%96%2FSelenium%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[} Selenium简介Selenium是一个自动化测试工具，利用它可以驱动浏览器执行特定的动作，如点击下拉等操作，同时还可以获取浏览器呈现的页面的源代码，做到可见即可爬。对于一些 JS 动态渲染的页面来说，此种方式非常有效。 基本使用声明浏览器对象Selenium支持非常多的浏览器，如Chrome、Firefox、Edge等，还有Android、BlackBerry等手机端的浏览器。另外，也支持无界面浏览器PhantomJS。 我们可以通过如下方法调用浏览器： 12345from selenium import webdriver browser = webdriver.Chrome()browser = webdriver.Firefox()browser = webdriver.PhantomJS() 访问页面可以用get()方法来请求网页，参数传入链接URL即可： 123456from selenium import webdriver browser = webdriver.Chrome()browser.get('https://www.taobao.com')print(browser.page_source)browser.close() 查找节点单个节点 find_element_by_xx 12345678find_element_by_idfind_element_by_namefind_element_by_xpathfind_element_by_link_textfind_element_by_partial_link_textfind_element_by_tag_namefind_element_by_class_namefind_element_by_css_selector find_element() 12345from selenium import webdriverfrom selenium.webdriver.common.by import Bybrowser.find_element(By.ID, 'xx')browser.find_element(By.NAME, 'xx') 多个节点得到的内容会变成列表类型，列表中的每个节点都是WebElement类型。 12345678910111213# find_element_by_xxfind_elements_by_idfind_elements_by_namefind_elements_by_xpathfind_elements_by_link_textfind_elements_by_partial_link_textfind_elements_by_tag_namefind_elements_by_class_namefind_elements_by_css_selector# find_elements()browser.find_elements(By.ID, 'xx')browser.find_elements(By.NAME, 'xx') 节点交互让浏览器模拟执行一些动作，常用方法有： 输入文字send_keys()方法 清空文字clear()方法 点击按钮click()方法 更多的操作可以参见官方文档的交互动作介绍：http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.remote.webelement 动作链上面的交互动作都是针对某个节点执行的。比如，对于输入框，我们就调用它的输入文字和清空文字方法；对于按钮，就调用它的点击方法。其实，还有另外一些操作，它们没有特定的执行对象，比如鼠标拖曳、键盘按键等，这些动作用另一种方式来执行，那就是动作链。 比如，实现一个节点的拖曳操作，将某个节点从一处拖曳到另外一处，可以这样实现： 123456789101112from selenium import webdriverfrom selenium.webdriver import ActionChains browser = webdriver.Chrome()url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'browser.get(url)browser.switch_to.frame('iframeResult')source = browser.find_element_by_css_selector('#draggable')target = browser.find_element_by_css_selector('#droppable')actions = ActionChains(browser)actions.drag_and_drop(source, target)actions.perform() 更多的动作链操作可以参考官方文档：http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains 执行JavaScript对于某些操作，Selenium API并没有提供。比如，下拉进度条，它可以直接模拟运行JavaScript，此时使用execute_script()方法即可实现，代码如下： 123456from selenium import webdriver browser = webdriver.Chrome()browser.get('https://www.zhihu.com/explore')browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')browser.execute_script('alert("To Bottom")') 这里就利用execute_script()方法将进度条下拉到最底部，然后弹出alert提示框。 所以说有了这个方法，基本上API没有提供的所有功能都可以用执行JavaScript的方式来实现了。 获取节点信息获取属性我们可以使用get_attribute()方法来获取节点的属性，但是其前提是先选中这个节点，示例如下： 123456789101112131415from selenium import webdriverfrom selenium.webdriver import ActionChains browser = webdriver.Chrome()url = 'https://www.zhihu.com/explore'browser.get(url)logo = browser.find_element_by_id('zh-top-link-logo')print(logo)print(logo.get_attribute('class'))# 输出如下：&lt;selenium.webdriver.remote.webelement.WebElement (session="e08c0f28d7f44d75ccd50df6bb676104", element="0.7236390660048155-1")&gt;zu-top-link-logo 运行之后，程序便会驱动浏览器打开知乎页面，然后获取知乎的logo节点，最后打印出它的class。 获取文本值每个WebElement节点都有text属性，直接调用这个属性就可以得到节点内部的文本信息，这相当于Beautiful Soup的get_text()方法、pyquery的text()方法，示例如下： 1234567891011from selenium import webdriver browser = webdriver.Chrome()url = 'https://www.zhihu.com/explore'browser.get(url)input = browser.find_element_by_class_name('zu-top-add-question')print(input.text)# 输出如下：提问 这里依然先打开知乎页面，然后获取“提问”按钮这个节点，再将其文本值打印出来。 获取id、位置、标签名和大小123456789101112131415161718192021from selenium import webdriver browser = webdriver.Chrome()url = 'https://www.zhihu.com/explore'browser.get(url)input = browser.find_element_by_class_name('zu-top-add-question')# 获取节点idprint(input.id)# 获取节点在页面中的相对位置print(input.location)# 获取标签名称print(input.tag_name)# 获取节点的大小print(input.size)# 输出如下：0.882088515167762-1&#123;'x': 681, 'y': 7&#125;button&#123;'height': 32, 'width': 66&#125; 切换Frame我们知道网页中有一种节点叫作iframe，也就是子Frame，相当于页面的子页面，它的结构和外部网页的结构完全一致。Selenium打开页面后，它默认是在父级Frame里面操作，而此时如果页面中还有子Frame，它是不能获取到子Frame里面的节点的。这时就需要使用switch_to.frame()方法来切换Frame。示例如下： 1234567891011121314151617181920212223import timefrom selenium import webdriverfrom selenium.common.exceptions import NoSuchElementException browser = webdriver.Chrome()url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'browser.get(url)browser.switch_to.frame('iframeResult')try: logo = browser.find_element_by_class_name('logo')except NoSuchElementException: print('NO LOGO')browser.switch_to.parent_frame()logo = browser.find_element_by_class_name('logo')print(logo)print(logo.text)# 输出如下：NO LOGO&lt;selenium.webdriver.remote.webelement.WebElement (session="4bb8ac03ced4ecbdefef03ffdc0e4ccd", element="0.13792611320464965-2")&gt;RUNOOB.COM 延时等待在Selenium中，get()方法会在网页框架加载结束后结束执行，此时如果获取page_source，可能并不是浏览器完全加载完成的页面，如果某些页面有额外的Ajax请求，我们在网页源代码中也不一定能成功获取到。所以，这里需要延时等待一定时间，确保节点已经加载出来。 这里等待的方式有两种：一种是隐式等待，一种是显式等待。 隐式等待当使用隐式等待执行测试的时候，如果Selenium没有在DOM中找到节点，将继续等待，超出设定时间后，则抛出找不到节点的异常。换句话说，当查找节点而节点并没有立即出现的时候，隐式等待将等待一段时间再查找DOM，默认的时间是0。示例如下： 1234567from selenium import webdriver browser = webdriver.Chrome()browser.implicitly_wait(10)browser.get('https://www.zhihu.com/explore')input = browser.find_element_by_class_name('zu-top-add-question')print(input) 这里我们用implicitly_wait()方法实现了隐式等待。 显式等待隐式等待的效果其实并没有那么好，因为我们只规定了一个固定时间，而页面的加载时间会受到网络条件的影响。 这里还有一种更合适的显式等待方法，它指定要查找的节点，然后指定一个最长等待时间。如果在规定时间内加载出来了这个节点，就返回查找的节点；如果到了规定时间依然没有加载出该节点，则抛出超时异常。示例如下： 123456789101112131415161718192021222324from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as EC browser = webdriver.Chrome()browser.get('https://www.taobao.com/')wait = WebDriverWait(browser, 10)input = wait.until(EC.presence_of_element_located((By.ID, 'q')))button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.btn-search')))print(input, button)# 当网速较好时能够加载完成，输出如下：&lt;selenium.webdriver.remote.webelement.WebElement (session="07dd2fbc2d5b1ce40e82b9754aba8fa8", element="0.5642646294074107-1")&gt;&lt;selenium.webdriver.remote.webelement.WebElement (session="07dd2fbc2d5b1ce40e82b9754aba8fa8", element="0.5642646294074107-2")&gt;# 当网络有问题指定时间没有加载完成，输出如下：TimeoutException Traceback (most recent call last)&lt;ipython-input-4-f3d73973b223&gt; in &lt;module&gt;() 7 browser.get('https://www.taobao.com/') 8 wait = WebDriverWait(browser, 10)----&gt; 9 input = wait.until(EC.presence_of_element_located((By.ID, 'q'))) 这里首先引入WebDriverWait这个对象，指定最长等待时间，然后调用它的until()方法，传入要等待条件expected_conditions。比如，这里传入了presence_of_element_located这个条件，代表节点出现的意思，其参数是节点的定位元组，也就是ID为q的节点搜索框。 这样可以做到的效果就是，在10秒内如果ID为q的节点（即搜索框）成功加载出来，就返回该节点；如果超过10秒还没有加载出来，就抛出异常。 对于按钮，可以更改一下等待条件，比如改为element_to_be_clickable，也就是可点击，所以查找按钮时查找CSS选择器为.btn-search的按钮，如果10秒内它是可点击的，也就是成功加载出来了，就返回这个按钮节点；如果超过10秒还不可点击，也就是没有加载出来，就抛出异常。 其他的等待条件及其含义： 等待条件 含义 title_is 标题是某内容 title_contains 标题包含某内容 presence_of_element_located 节点加载出来，传入定位元组，如(By.ID, &#39;p&#39;) visibility_of_element_located 节点可见，传入定位元组 visibility_of 可见，传入节点对象 presence_of_all_elements_located 所有节点加载出来 text_to_be_present_in_element 某个节点文本包含某文字 text_to_be_present_in_element_value 某个节点值包含某文字 frame_to_be_available_and_switch_to_it 加载并切换 invisibility_of_element_located 节点不可见 element_to_be_clickable 节点可点击 staleness_of 判断一个节点是否仍在DOM，可判断页面是否已经刷新 element_to_be_selected 节点可选择，传节点对象 element_located_to_be_selected 节点可选择，传入定位元组 element_selection_state_to_be 传入节点对象以及状态，相等返回True，否则返回False element_located_selection_state_to_be 传入定位元组以及状态，相等返回True，否则返回False alert_is_present 是否出现警告 关于更多等待条件的参数及用法，可以参考官方文档：http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.support.expected_conditions 前进和后退平常使用浏览器时都有前进和后退功能，Selenium也可以完成这个操作，它使用back()方法后退，使用forward()方法前进。示例如下： 1234567891011import timefrom selenium import webdriver browser = webdriver.Chrome()browser.get('https://www.baidu.com/')browser.get('https://www.taobao.com/')browser.get('https://www.python.org/')browser.back()time.sleep(1)browser.forward()browser.close() Cookies使用Selenium，还可以方便地对Cookies进行操作，例如获取、添加、删除Cookies等。示例如下： 12345678910111213141516from selenium import webdriver browser = webdriver.Chrome()browser.get('https://www.zhihu.com/explore')print(browser.get_cookies())browser.add_cookie(&#123;'name': 'name', 'domain': 'www.zhihu.com', 'value': 'germey'&#125;)print(browser.get_cookies())browser.delete_all_cookies()print(browser.get_cookies())# 输出如下：[&#123;'secure': False, 'value': '"NGM0ZTM5NDAwMWEyNDQwNDk5ODlkZWY3OTkxY2I0NDY=|1491604091|236e34290a6f407bfbb517888849ea509ac366d0"', 'domain': '.zhihu.com', 'path': '/', 'httpOnly': False, 'name': 'l_cap_id', 'expiry': 1494196091.403418&#125;][&#123;'secure': False, 'value': 'germey', 'domain': '.www.zhihu.com', 'path': '/', 'httpOnly': False, 'name': 'name'&#125;, &#123;'secure': False, 'value': '"NGM0ZTM5NDAwMWEyNDQwNDk5ODlkZWY3OTkxY2I0NDY=|1491604091|236e34290a6f407bfbb517888849ea509ac366d0"', 'domain': '.zhihu.com', 'path': '/', 'httpOnly': False, 'name': 'l_cap_id', 'expiry': 1494196091.403418&#125;][] 首先，我们访问了知乎。加载完成后，浏览器实际上已经生成Cookies了。接着，调用get_cookies()方法获取所有的Cookies。然后，我们添加一个Cookie，这里传入一个字典，有name、domain和value等内容。接下来，再次获取所有的Cookies。可以发现，结果就多了这一项新加的Cookie。最后，调用delete_all_cookies()方法删除所有的Cookies。再重新获取，发现结果就为空了。 选项卡管理在访问网页的时候，会开启一个个选项卡。在Selenium中，我们也可以对选项卡进行操作。示例如下： 1234567891011121314151617import timefrom selenium import webdriver browser = webdriver.Chrome()browser.get('https://www.baidu.com')browser.execute_script('window.open()')print(browser.window_handles)browser.switch_to_window(browser.window_handles[1])browser.get('https://www.taobao.com')time.sleep(1)browser.switch_to_window(browser.window_handles[0])browser.get('https://python.org')# 输出如下：['CDwindow-4f58e3a7-7167-4587-bedf-9cd8c867f435', 'CDwindow-6e05f076-6d77-453a-a36c-32baacc447df'] 异常处理在使用Selenium的过程中，难免会遇到一些异常，例如超时、节点未找到等错误，一旦出现此类错误，程序便不会继续运行了。这里我们可以使用try except语句来捕获各种异常。 首先，演示一下节点未找到的异常，尝试选择一个并不存在的节点，示例如下： 12345678910111213from selenium import webdriver browser = webdriver.Chrome()browser.get('https://www.baidu.com')browser.find_element_by_id('hello')# 输出如下：NoSuchElementException Traceback (most recent call last)&lt;ipython-input-23-978945848a1b&gt; in &lt;module&gt;() 3 browser = webdriver.Chrome() 4 browser.get('https://www.baidu.com')----&gt; 5 browser.find_element_by_id('hello') 可以看到，这里抛出了NoSuchElementException异常，这通常是节点未找到的异常。为了防止程序遇到异常而中断，我们需要捕获这些异常，示例如下： 12345678910111213141516171819from selenium import webdriverfrom selenium.common.exceptions import TimeoutException, NoSuchElementException browser = webdriver.Chrome()try: browser.get('https://www.baidu.com')except TimeoutException: print('Time Out')try: browser.find_element_by_id('hello')except NoSuchElementException: print('No Element')finally: browser.close() # 输出如下：No Element 这里我们使用try except来捕获各类异常。比如，我们对find_element_by_id()查找节点的方法捕获NoSuchElementException异常，这样一旦出现这样的错误，就进行异常处理，程序也不会中断了。 关于更多的异常类，可以参考官方文档：http://selenium-python.readthedocs.io/api.html#module-selenium.common.exceptions]]></content>
      <categories>
        <category>Spider</category>
        <category>动态页面抓取</category>
      </categories>
      <tags>
        <tag>Selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ajax实践_抓取今日头条]]></title>
    <url>%2F2018%2F12%2F22%2F%E7%BC%96%E7%A8%8B%2F%E5%8A%A8%E6%80%81%E9%A1%B5%E9%9D%A2%E6%8A%93%E5%8F%96%2FAjax%E6%8A%93%E5%8F%96%E4%BB%8A%E6%97%A5%E5%A4%B4%E6%9D%A1%2F</url>
    <content type="text"><![CDATA[抓取分析打开今日头条的首页http://www.toutiao.com/ 右上角有一个搜索入口，这里尝试抓取街拍美图，所以输入“街拍”二字搜索一下 这时我们可以打开开发者工具查看所有网络请求中的第一个请求，查看他的 Response Body，我们可以发现网页中的内容并不包含在这条请求中，所以我们初步判断 这些内容是由 Ajax 加载，然后用 JS 渲染出来的。然后我们切换到 XHR 过滤选项卡，这时可以看到有 Ajax 请求。 点击data字段展开，发现这里有许多条数据。点击第一条展开，可以发现有一个title字段，它的值正好就是页面中第一条数据的标题。再检查一下其他数据，也正好是一一对应的。如下图： 这就确定了这些数据确实是由Ajax加载的。 我们的目的是要抓取其中的美图，这里一组图就对应前面data字段中的一条数据。每条数据还有一个image_detail字段，它是列表形式，这其中就包含了组图的所有图片列表，如下图： 因此，我们只需要将列表中的url字段提取出来并下载下来就好了。每一组图都建立一个文件夹，文件夹的名称就为组图的标题。 接下来，就可以直接用Python来模拟这个Ajax请求，然后提取出相关美图链接并下载。但是在这之前，我们还需要分析一下URL的规律。 切换回Headers选项卡，观察一下它的请求URL和Headers信息，如下图： 可以看到，这是一个GET请求，请求URL的参数有offset、format、keyword、autoload、count、cur_tab和from等。我们需要找出这些参数的规律，因为这样才可以方便地用程序构造出来。 接下来，可以滑动页面，多加载一些新结果。在加载的同时可以发现，Network中又出现了许多Ajax请求，如下图： 这里观察一下后续链接的参数，发现变化的参数只有offset，其他参数都没有变化，而且第二次请求的offset值为20，第三次为40，第四次为60，所以可以发现规律，这个offset值就是偏移量，进而可以推断出count参数就是一次性获取的数据条数。因此，我们可以用offset参数来控制数据分页。这样一来，我们就可以通过接口批量获取数据了，然后将数据解析，将图片下载下来即可。 抓取实现首先，实现方法get_page()来加载单个Ajax请求的结果。其中唯一变化的参数就是offset，所以我们将它当作参数传递，实现如下： 12345678910111213141516171819202122232425262728import requestsfrom urllib.parse import urlencode headers = &#123; 'user-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36 ' 'cookie': 'tt_webid=6637749626995492359; WEATHER_CITY=%E5%8C%97%E4%BA%AC; UM_distinctid=167d547f1b3460-020c50f8b937b2-424e0b28-1fa400-167d547f1b489; CNZZDATA1259612802=1003758837-1545470092-%7C1545470092; tt_webid=6637749626995492359; __tasessionId=hkxc5jku31545471521344; csrftoken=6767aaa50120c05ed4a74271670aef22' 'referer': 'https://www.toutiao.com/search/?keyword=%E8%A1%97%E6%8B%8D' 'x-requested-with': 'XMLHttpRequest' &#125; def get_page(offset): params = &#123; 'offset': offset, 'format': 'json', 'keyword': '街拍', 'autoload': 'true', 'count': '20', 'cur_tab': '1', 'from': 'search_tab' &#125; base_url = 'http://www.toutiao.com/search_content/?' url = base_url + urlencode(params) try: response = requests.get(url，headers=headers) if response.status_code == 200: return response.json() except requests.ConnectionError: return None 这里我们用urlencode()方法构造请求的GET参数，然后用requests请求这个链接，如果返回状态码为200，则调用response的json()方法将结果转为JSON格式，然后返回。 接下来，再实现一个解析方法：提取每条数据的image_detail字段中的每一张图片链接，将图片链接和图片所属的标题一并返回，此时可以构造一个生成器。实现代码如下： 12345678910111213def get_images(json): if json.get('data'): data = json.get('data') for item in data: if item.get('cell_type') is not None: continue title = item.get('title') images = item.get('image_list') for image in images: yield &#123; 'image': 'https:' + image.get('url'), 'title': title &#125; 接下来，实现一个保存图片的方法save_image()，其中item就是前面get_images()方法返回的一个字典。在该方法中，首先根据item的title来创建文件夹，然后请求这个图片链接，获取图片的二进制数据，以二进制的形式写入文件。图片的名称可以使用其内容的MD5值，这样可以去除重复。相关代码如下： 123456789101112131415161718192021import osfrom hashlib import md5# 保存图片至指定路径def save_image(item): img_path = 'D:/imagedata/&#123;title&#125;'.format(title=item.get('title')) if not os.path.exists(img_path): os.makedirs(img_path) try: pic = requests.get(item.get('image')) if pic.status_code == 200: file_path = img_path + '&#123;file_name&#125;.jpg'.format( file_name=md5(pic.content).hexdigest()) if not os.path.exists(file_path): with open(file_path, 'wb') as f: f.write(pic.content) print('%s Save Successful' % file_path) else: print('Already Downloaded', file_path) except requests.ConnectionError: print('Failed to Save Image，item %s' % item) 最后，只需要构造一个offset数组，遍历offset，提取图片链接，并将其下载即可： 1234567891011121314151617from multiprocessing.pool import pooldef main(offset): json = get_page(offset) for item in get_image(json): print(item) save_image(item) GROUP_START = 1GROUP_END = 20if __name__ == "__main__": pool = pool() groups = ([i * 20 for i in range(GROUP_START,GROUP_END + 1)]) pool.map(main,groups) pool.close() pool.join() 这里定义了分页的起始页数和终止页数，分别为GROUP_START和GROUP_END，还利用了多线程的线程池，调用其map()方法实现多线程下载。 这样整个程序就完成了，运行之后可以发现街拍美图都分文件夹保存下来了。 运行结果如下： 完整代码地址：github.com/wshaoxin/Spider/toutiao_jiepai.py]]></content>
      <categories>
        <category>Spider</category>
        <category>动态页面抓取</category>
      </categories>
      <tags>
        <tag>Ajax</tag>
        <tag>requests</tag>
        <tag>项目</tag>
        <tag>今日头条</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[选择排序]]></title>
    <url>%2F2018%2F12%2F21%2F%E7%BC%96%E7%A8%8B%2F%E7%AE%97%E6%B3%95%2F%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[原理： 选择排序比较好理解，好像是在一堆大小不一的球中进行选择（以从小到大，先选最小球为例）： 选择一个基准球 将基准球和余下的球进行一一比较，如果比基准球小，则进行交换 第一轮过后获得最小的球 在挑一个基准球，执行相同的动作得到次小的球 继续执行4，直到排序好 时间复杂度： O(n^2) 需要进行的比较次数为第一轮 n-1，n-2….1, 总的比较次数为 n*(n-1)/2 实现： 12345678910111213141516171819202122def selectedSort(myList): #获取list的长度 length = len(myList) #一共进行多少轮比较 for i in range(0,length-1): #默认设置最小值得index为当前值 smallest = i #用当先最小index的值分别与后面的值进行比较,以便获取最小index for j in range(i+1,length): #如果找到比当前值小的index,则进行两值交换 if myList[j]&lt;myList[smallest]: tmp = myList[j] myList[j] = myList[smallest] myList[smallest]=tmp #打印每一轮比较好的列表 print("Round ",i,": ",myList)myList = [1,4,5,0,6]print("Selected Sort: ")selectedSort(myList)]]></content>
      <categories>
        <category>每日算法</category>
      </categories>
      <tags>
        <tag>每日算法</tag>
        <tag>选择排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ajax简介]]></title>
    <url>%2F2018%2F12%2F21%2F%E7%BC%96%E7%A8%8B%2F%E5%8A%A8%E6%80%81%E9%A1%B5%E9%9D%A2%E6%8A%93%E5%8F%96%2FAjax%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[有时候我们在用requests抓取页面的时候，得到的结果可能和在浏览器中看到的不一样：在浏览器中可以看到正常显示的页面数据，但是使用requests得到的结果并没有。这是因为requests获取的都是原始的HTML文档，而浏览器中的页面则是经过JavaScript处理数据后生成的结果，这些数据的来源有多种，可能是通过Ajax加载的，可能是包含在HTML文档中的，也可能是经过JavaScript和特定算法计算后生成的。 对于第一种情况，数据加载是一种异步加载方式，原始的页面最初不会包含某些数据，原始页面加载完后，会再向服务器请求某个接口获取数据，然后数据才被处理从而呈现到网页上，这其实就是发送了一个Ajax请求。 照Web发展的趋势来看，这种形式的页面越来越多。网页的原始HTML文档不会包含任何数据，数据都是通过Ajax统一加载后再呈现出来的，这样在Web开发上可以做到前后端分离，而且降低服务器直接渲染页面带来的压力。 所以如果遇到这样的页面，直接利用requests等库来抓取原始页面，是无法获取到有效数据的，这时需要分析网页后台向接口发送的Ajax请求，如果可以用requests来模拟Ajax请求，那么就可以成功抓取了。 什么是AjaxAjax，全称为Asynchronous JavaScript and XML，即异步的JavaScript和XML。它不是一门编程语言，而是利用JavaScript在保证页面不被刷新、页面链接不改变的情况下与服务器交换数据并更新部分网页的技术。 对于传统的网页，如果想更新其内容，那么必须要刷新整个页面，但有了Ajax，便可以在页面不被全部刷新的情况下更新其内容。在这个过程中，页面实际上是在后台与服务器进行了数据交互，获取到数据之后，再利用JavaScript改变网页，这样网页内容就会更新了。 基本原理初步了解了Ajax之后，我们再来详细了解它的基本原理。发送Ajax请求到网页更新的这个过程可以简单分为以下3步： (1) 发送请求； (2) 解析内容； (3) 渲染网页。 下面我们分别来详细介绍这几个过程。 发送请求我们知道JavaScript可以实现页面的各种交互功能，Ajax也不例外，它也是由JavaScript实现的，实际上执行了如下代码： 1234567891011121314var xmlhttp;if (window.XMLHttpRequest) &#123; // code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest();&#125; else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject("Microsoft.XMLHTTP");&#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById("myDiv").innerHTML=xmlhttp.responseText; &#125;&#125;xmlhttp.open("POST","/ajax/",true);xmlhttp.send(); 这是JavaScript对Ajax最底层的实现，实际上就是新建了XMLHttpRequest对象，然后调用onreadystatechange属性设置了监听，然后调用open()和send()方法向某个链接（也就是服务器）发送了请求。前面用Python实现请求发送之后，可以得到响应结果，但这里请求的发送变成JavaScript来完成.由于设置了监听，所以当服务器返回响应时，onreadystatechange对应的方法便会被触发，然后在这个方法里面解析响应内容即可。 解析内容得到响应之后，onreadystatechange属性对应的方法便会被触发，此时利用xmlhttp的responseText属性便可取到响应内容。这类似于Python中利用requests向服务器发起请求，然后得到响应的过程。那么返回内容可能是HTML，可能是JSON，接下来只需要在方法中用JavaScript进一步处理即可。比如，如果是JSON的话，可以进行解析和转化。 渲染网页JavaScript有改变网页内容的能力，解析完响应内容之后，就可以调用JavaScript来针对解析完的内容对网页进行下一步处理了。比如，通过document.getElementById().innerHTML这样的操作，便可以对某个元素内的源代码进行更改，这样网页显示的内容就改变了，这样的操作也被称作DOM操作，即对Document网页文档进行操作，如更改、删除等。 上例中，document.getElementById(&quot;myDiv&quot;).innerHTML=xmlhttp.responseText便将ID为myDiv的节点内部的HTML代码更改为服务器返回的内容，这样myDiv元素内部便会呈现出服务器返回的新数据，网页的部分内容看上去就更新了。 我们观察到，这3个步骤其实都是由JavaScript完成的，它完成了整个请求、解析和渲染的过程。 再回想微博的下拉刷新，这其实就是JavaScript向服务器发送了一个Ajax请求，然后获取新的微博数据，将其解析，并将其渲染在网页中。 因此，我们知道，真实的数据其实都是一次次Ajax请求得到的，如果想要抓取这些数据，需要知道这些请求到底是怎么发送的，发往哪里，发了哪些参数。如果我们知道了这些，就可以用Python模拟这个发送操作，获取到其中的结果。]]></content>
      <categories>
        <category>Spider</category>
        <category>动态页面抓取</category>
      </categories>
      <tags>
        <tag>Ajax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速排序]]></title>
    <url>%2F2018%2F12%2F20%2F%E7%BC%96%E7%A8%8B%2F%E7%AE%97%E6%B3%95%2F%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[基本思想是： 通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。 如序列[6，8，1，4，3，9]，选择6作为基准数。从右向左扫描，寻找比基准数小的数字为3，交换6和3的位置，[3，8，1，4，6，9]，接着从左向右扫描，寻找比基准数大的数字为8，交换6和8的位置，[3，6，1，4，8，9]。重复上述过程，直到基准数左边的数字都比其小，右边的数字都比其大。然后分别对基准数左边和右边的序列递归进行上述方法。 实现代码如下： 123456789101112131415161718192021222324252627282930313233 def quick_sort(alist， start， end): """快速排序""" # 递归的退出条件 if start &gt;= end: return # 设定起始元素为要寻找位置的基准元素 mid = alist[start] # low 为序列左边的由左向右移动的游标 low = start # high 为序列右边的由右向左移动的游标 high = end while low &lt; high: # 如果 low 与 high 未重合，high 指向的元素不比基准元素小，则 high 向左移动 while low &lt; high and alist[high] &gt;= mid: high -= 1 # 将 high 指向的元素放到 low 的位置上 alist[low] = alist[high] # 如果 low 与 high 未重合，low 指向的元素比基准元素小，则 low 向右移动 while low &lt; high and alist[low] &lt; mid: low += 1 # 将 low 指向的元素放到 high 的位置上 alist[high] = alist[low] # 退出循环后，low 与 high 重合，此时所指位置为基准元素的正确位置 # 将基准元素放到该位置 alist[low] = mid # 对基准元素左边的子序列进行快速排序 quick_sort(alist， start， low-1) # 对基准元素右边的子序列进行快速排序 quick_sort(alist， low+1， end) alist = [54，26，93，17，77，31，44，55，20]quick_sort(alist，0，len(alist)-1)print(alist)]]></content>
      <categories>
        <category>每日算法</category>
      </categories>
      <tags>
        <tag>每日算法</tag>
        <tag>快排</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasiticsearch简介]]></title>
    <url>%2F2018%2F12%2F20%2F%E7%BC%96%E7%A8%8B%2FElasticsearch%2FElasiticsearch%2F</url>
    <content type="text"><![CDATA[什么是搜索 百度搜索：当我们想搜索任何的信息的时候，“百度一下，你就知道” 百度 != 搜索 垂直搜索（站内搜索） 互联网的搜索：电商网站，招聘网站，新闻网站，各种app… IT系统的搜索：OA软件，办公自动化软件 搜索就是在任何场景下，找寻你想要的信息。这个时候，输入一些搜索关键字，然后期望找到这个关键字相关的某些信息。 为什么不用数据库做搜索 比如说，每条记录的指定字段的文本可能会很长，“商品描述”的字段的长度可能长达数千甚至上万个字符，这时每次搜索都要对每条记录的所有文本进行扫描判断 还不能将搜索词拆分开来，尽可能去搜索更多的你期望的结果，比如搜索“生化机”就搜不出来“生化危机” 所以用数据库做搜索是不靠谱的性能很差 什么是全文检索 全文检索 什么是ElasticsearchElasticsearch 是一个开源的搜索引擎，建立在一个全文搜索引擎库 Apache Lucene™ 基础之上。 Lucene 可能是目前存在的拥有最先进，高性能和全功能搜索引擎功能的库。要用上 Lucene，我们需要编写 Java 并引用 Lucene 包才可以，而且我们需要对信息检索有一定程度的理解才能明白 Lucene 是怎么工作的，反正用起来没那么简单。 那么为了解决这个问题，Elasticsearch 就诞生了。Elasticsearch 也是使用 Java 编写的，它的内部使用 Lucene 做索引与搜索，但是它的目标是使全文检索变得简单，相当于 Lucene 的一层封装，它提供了一套简单一致的 RESTful API 来帮助我们实现存储和检索。 功能 分布式的实时文档存储，全文检索，结构化检索 一个分布式实时分析搜索引擎 能胜任上百个服务节点的扩展，并支持 PB 级别的结构化或者非结构化数据 适用场景 维基百科：全文检索，高亮，搜索推荐 Stack Overflow：全文检索，搜索相关问题和答案 Github：上千亿行代码的搜索 电商网站 招聘、门户 Elasticsearch的安装官方下载地址：https://www.elastic.co/downloads/elasticsearch]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
        <tag>搜索引擎</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[冒泡排序]]></title>
    <url>%2F2018%2F12%2F19%2F%E7%BC%96%E7%A8%8B%2F%E7%AE%97%E6%B3%95%2F%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[原理： 比较两个相邻的元素，将值大的元素交换至右端。 思路： 依次比较相邻的两个数，将小数放在前面，大数放在后面。即在第一趟：首先比较第1个和第2个数，将小数放前，大数放后。然后比较第2个数和第3个数，将小数放前，大数放后，如此继续，直至比较最后两个数，将小数放前，大数放后。重复第一趟步骤，直至全部排序完成。 第一趟比较完成后，最后一个数一定是数组中最大的一个数，所以第二趟比较的时候最后一个数不参与比较； 第二趟比较完成后，倒数第二个数也一定是数组中第二大的数，所以第三趟比较的时候最后两个数不参与比较； 依次类推，每一趟比较次数-1； 时间复杂度： 1.如果我们的数据正序，时间复杂度为O(n)。 2.如果我们的数据反序，时间复杂度为O(n2)。 综合之后时间复杂度为O(n2)。 python实现： 123456789def bubble_sort(list): for j in range(len(list)-1, 0, -1): for i in range(0,j): if list[i] &gt; list[i]: list[i], list[i + 1] = list[i + 1], list[i] return listlist = [5,2,18,0,36,9,7,45]print(bubble_sort(list)) 优化： 如果我们的数据正序，只需要一次循环就够了，所以我们针对特殊情况进行优化： 123456789101112def bubble_sort(list): for j in range(len(list)-1, 0, -1): count = 0 for i in range(0,j): if list[i] &gt; list[i]: list[i], list[i + 1] = list[i + 1], list[i] count += 1 if count == 0: returnlist = [5,2,18,0,36,9,7,45]print(bubble_sort(list))]]></content>
      <categories>
        <category>每日算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>冒泡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BeautifulSoup的使用]]></title>
    <url>%2F2018%2F12%2F17%2F%E7%BC%96%E7%A8%8B%2F%E8%A7%A3%E6%9E%90%2FBeautifulSoup%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[简介 Beautiful Soup是Python的一个HTML或XML的解析库，可以用它来方便地从网页中提取数据。 它借助网页的结构和属性等特性来解析网页。有了它，我们不用再去写一些复杂的正则表达式，只需要简单的几条语句，就可以完成网页中某个元素的提取。 官方的解释如下： Beautiful Soup提供一些简单的、Python式的函数来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。 Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为UTF-8编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时你仅仅需要说明一下原始编码方式就可以了。 Beautiful Soup已成为和lxml、html6lib一样出色的Python解释器，为用户灵活地提供不同的解析策略或强劲的速度。 解析器Beautiful Soup在解析时实际上依赖解析器，它除了支持Python标准库中的HTML解析器外，还支持一些第三方解析器（比如lxml）。 BeautifulSoup支持的解析器 解析器 使用方法 优势 劣势 Python标准库 BeautifulSoup(markup, &quot;html.parser&quot;) Python的内置标准库、执行速度适中、文档容错能力强 Python 2.7.3及Python 3.2.2之前的版本文档容错能力差 lxml HTML解析器 BeautifulSoup(markup, &quot;lxml&quot;) 速度快、文档容错能力强 需要安装C语言库 lxml XML解析器 BeautifulSoup(markup, &quot;xml&quot;) 速度快、唯一支持XML的解析器 需要安装C语言库 html5lib BeautifulSoup(markup, &quot;html5lib&quot;) 最好的容错性、以浏览器的方式解析文档、生成HTML5格式的文档 速度慢、不依赖外部扩展 推荐使用 lxml 作为解析器,因为效率更高. 在 Python2.7.3 之前的版本和 Python3中3.2.2 之前的版本,必须安装 lxml 或 html5lib, 因为那些 Python 版本的标准库中内置的 HTML 解析方法不够稳定. 基本用法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455from bs4 import BeautifulSouphtml = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;"""soup = BeautifulSoup(html, 'lxml')print(soup.prettify())print(soup.title.string)# 输出如下：&lt;html&gt; &lt;head&gt; &lt;title&gt; The Dormouse's story &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="title" name="dromouse"&gt; &lt;b&gt; The Dormouse's story &lt;/b&gt; &lt;/p&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt; &lt;!-- Elsie --&gt; &lt;/a&gt; , &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt; Lacie &lt;/a&gt; and &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt; Tillie &lt;/a&gt; ;and they lived at the bottom of a well. &lt;/p&gt; &lt;p class="story"&gt; ... &lt;/p&gt; &lt;/body&gt;&lt;/html&gt;The Dormouse's story 这里首先声明变量html，它是一个不完整的 HTML 字符串。将它当作第一个参数传给BeautifulSoup对象，第二个参数为解析器的类型（这里使用lxml），此时就完成了BeaufulSoup对象的初始化。 首先，调用prettify()方法。这个方法可以把要解析的字符串以标准的缩进格式输出。对于不标准的HTML字符串BeautifulSoup可以自动更正格式。这一步不是由prettify()方法做的，而是在初始化BeautifulSoup时就完成了。 然后调用soup.title.string，输出HTML中title节点的文本内容。 节点选择器适用于单个节点非常清晰的情况 选择元素123456789101112131415161718192021222324252627html = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;"""from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.title)print(type(soup.title))print(soup.title.string)print(soup.head)print(soup.p)# 输出如下：&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;class 'bs4.element.Tag'&gt;The Dormouse's story&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt; 首先打印输出title节点的选择结果，接下来输出它的类型，是bs4.element.Tag类型，这是Beautiful Soup中一个重要的数据结构。经过选择器选择后，选择结果都是这种Tag类型。Tag具有一些属性，比如string属性，调用该属性，可以得到节点的文本内容。 然后选择head节点，结果也是节点加其内部的所有内容。 最后，选择了p节点，只输出了第一个节点的内容。当有多个节点时，这种选择方式只会选择到第一个匹配的节点。 提取信息利用name属性获取节点的名称；在节点元素后面加中括号，传入属性名就可以获取属性值；利用string属性获取节点元素包含的文本内容： 1234567891011print(soup.title.name)print(soup.p['name'])print(soup.p['class'])print(soup.p.string)# 输出如下：titledromouse['title']The Dormouse's story 关联选择1.子节点和子孙节点： 选取节点元素之后，如果想要获取它的直接子节点，可以调用contents属性： 12345678910111213141516171819202122232425html = """&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and &lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class="story"&gt;...&lt;/p&gt;"""# 输出如下：['\n Once upon a time there were three little sisters; and their names were\n ', &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;, '\n', &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;, ' \n and\n ', &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;, '\n and they lived at the bottom of a well.\n '] 可以看到，返回结果是列表形式。p节点里既包含文本，又包含节点，最后会将它们以列表形式统一返回。 需要注意的是，列表中的每个元素都是p节点的直接子节点。比如第一个a节点里面包含一层span节点，这相当于孙子节点了，但是返回结果并没有单独把span节点选出来。所以说，contents属性得到的结果是直接子节点的列表。 同样，我们可以调用children属性得到相应的结果： 123456789101112131415161718192021222324from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.p.children)for i, child in enumerate(soup.p.children): print(i, child) # 输出如下&lt;list_iterator object at 0x1064f7dd8&gt;0 Once upon a time there were three little sisters; and their names were 1 &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;2 3 &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;4 and 5 &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;6 and they lived at the bottom of a well. 还是同样的HTML文本，这里调用了children属性来选择，返回结果是生成器类型。接下来，我们用for循环输出相应的内容。 如果要得到所有的子孙节点的话，可以调用descendants属性： 1234567891011121314151617181920212223242526272829303132from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.p.descendants)for i, child in enumerate(soup.p.descendants): print(i, child) # 输出如下：&lt;generator object descendants at 0x10650e678&gt;0 Once upon a time there were three little sisters; and their names were 1 &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;2 3 &lt;span&gt;Elsie&lt;/span&gt;4 Elsie5 6 7 &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;8 Lacie9 and 10 &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;11 Tillie12 and they lived at the bottom of a well. 此时返回结果还是生成器。遍历输出一下可以看到，这次的输出结果就包含了span节点。descendants会递归查询所有子节点，得到所有的子孙节点。 2.父节点和祖先节点 如果要获取某个节点元素的父节点，可以调用parent属性： 123456789101112131415161718192021222324252627html = """&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;/p&gt; &lt;p class="story"&gt;...&lt;/p&gt;"""from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.a.parent)#输出如下：&lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;&lt;/p&gt; 这里我们选择的是第一个a节点的父节点元素。很明显，它的父节点是p节点，输出结果便是p节点及其内部的内容。 需要注意的是，这里输出的仅仅是a节点的直接父节点，而没有再向外寻找父节点的祖先节点。如果想获取所有的祖先节点，可以调用parents属性： 12345678910111213141516171819202122232425262728293031323334353637383940414243html = """&lt;html&gt; &lt;body&gt; &lt;p class="story"&gt; &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;/p&gt;"""from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(type(soup.a.parents))print(list(enumerate(soup.a.parents)))# 输出如下：&lt;class 'generator'&gt;[(0, &lt;p class="story"&gt;&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;), (1, &lt;body&gt;&lt;p class="story"&gt;&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/body&gt;), (2, &lt;html&gt;&lt;body&gt;&lt;p class="story"&gt;&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;), (3, &lt;html&gt;&lt;body&gt;&lt;p class="story"&gt;&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;)] 可以发现，返回结果是生成器类型。这里用列表输出了它的索引和内容，而列表中的元素就是a节点的祖先节点。 3.兄弟节点： 12345678910111213141516171819202122232425262728293031323334html = """&lt;html&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; Hello &lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and &lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt;"""from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print('Next Sibling', soup.a.next_sibling)print('Prev Sibling', soup.a.previous_sibling)print('Next Siblings', list(enumerate(soup.a.next_siblings)))print('Prev Siblings', list(enumerate(soup.a.previous_siblings)))#输出如下：Next Sibling Hello Prev Sibling Once upon a time there were three little sisters; and their names were Next Siblings [(0, '\n Hello\n '), (1, &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;), (2, ' \n and\n '), (3, &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;), (4, '\n and they lived at the bottom of a well.\n ')]Prev Siblings [(0, '\n Once upon a time there were three little sisters; and their names were\n ')] 可以看到，这里调用了4个属性，其中next_sibling和previous_sibling分别获取节点的下一个和上一个兄弟元素，next_siblings和previous_siblings则分别返回所有前面和后面的兄弟节点的生成器。 4.提取信息123456789101112131415161718192021222324252627282930313233html = """&lt;html&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;Bob&lt;/a&gt;&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; &lt;/p&gt;"""from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print('Next Sibling:')print(type(soup.a.next_sibling))print(soup.a.next_sibling)print(soup.a.next_sibling.string)print('Parent:')print(type(soup.a.parents))print(list(soup.a.parents)[0])print(list(soup.a.parents)[0].attrs['class'])#输出如下：Next Sibling:&lt;class 'bs4.element.Tag'&gt;&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;LacieParent:&lt;class 'generator'&gt;&lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Bob&lt;/a&gt;&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;&lt;/p&gt;['story'] 如果返回结果是单个节点，那么可以直接调用string、attrs等属性获得其文本和属性；如果返回结果是多个节点的生成器，则可以转为列表后取出某个元素，然后再调用string、attrs等属性获取其对应节点的文本和属性。 方法选择器前面所讲的选择方法都是通过属性来选择的，这种方法非常快，但是如果进行比较复杂的选择的话，它就比较烦琐，不够灵活了。幸好，Beautiful Soup还为我们提供了一些查询方法，比如find_all()和find()等，调用它们，然后传入相应的参数，就可以灵活查询了。 find_all()参数 : find_all(name , attrs , recursive , text , **kwargs) 1. name： 根据节点名来查询元素： 1234567891011121314151617181920212223242526272829303132333435html='''&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all(name='ul'))print(type(soup.find_all(name='ul')[0]))#输出如下：[&lt;ul class="list" id="list-1"&gt;&lt;li class="element"&gt;Foo&lt;/li&gt;&lt;li class="element"&gt;Bar&lt;/li&gt;&lt;li class="element"&gt;Jay&lt;/li&gt;&lt;/ul&gt;, &lt;ul class="list list-small" id="list-2"&gt;&lt;li class="element"&gt;Foo&lt;/li&gt;&lt;li class="element"&gt;Bar&lt;/li&gt;&lt;/ul&gt;]&lt;class 'bs4.element.Tag'&gt; 这里我们调用了find_all()方法，返回结果是列表类型，长度为2，每个元素依然都是bs4.element.Tag类型。 因为都是Tag类型，所以依然可以进行嵌套查询。还是同样的文本，这里查询出所有ul节点后，再继续查询其内部的li节点： 1234567for ul in soup.find_all(name='ul'): print(ul.find_all(name='li')) #输出如下：[&lt;li class="element"&gt;Foo&lt;/li&gt;, &lt;li class="element"&gt;Bar&lt;/li&gt;, &lt;li class="element"&gt;Jay&lt;/li&gt;][&lt;li class="element"&gt;Foo&lt;/li&gt;, &lt;li class="element"&gt;Bar&lt;/li&gt;] 返回结果是列表类型，列表中的每个元素依然还是Tag类型。 接下来，就可以遍历每个li，获取它的文本了： 1234567891011121314for ul in soup.find_all(name='ul'): print(ul.find_all(name='li')) for li in ul.find_all(name='li'): print(li.string) #输出如下：[&lt;li class="element"&gt;Foo&lt;/li&gt;, &lt;li class="element"&gt;Bar&lt;/li&gt;, &lt;li class="element"&gt;Jay&lt;/li&gt;]FooBarJay[&lt;li class="element"&gt;Foo&lt;/li&gt;, &lt;li class="element"&gt;Bar&lt;/li&gt;]FooBar 2. attr 除了根据节点名查询，我们也可以传入一些属性来查询： 123456789101112131415161718192021222324252627282930313233343536html='''&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1" name="elements"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all(attrs=&#123;'id': 'list-1'&#125;))print(soup.find_all(attrs=&#123;'name': 'elements'&#125;))# 输出如下：[&lt;ul class="list" id="list-1" name="elements"&gt;&lt;li class="element"&gt;Foo&lt;/li&gt;&lt;li class="element"&gt;Bar&lt;/li&gt;&lt;li class="element"&gt;Jay&lt;/li&gt;&lt;/ul&gt;][&lt;ul class="list" id="list-1" name="elements"&gt;&lt;li class="element"&gt;Foo&lt;/li&gt;&lt;li class="element"&gt;Bar&lt;/li&gt;&lt;li class="element"&gt;Jay&lt;/li&gt;&lt;/ul&gt;] 这里查询的时候传入的是attrs参数，参数的类型是字典类型。比如，要查询id为list-1的节点，可以传入attrs={&#39;id&#39;: &#39;list-1&#39;}的查询条件，得到的结果是列表形式，包含的内容就是符合id为list-1的所有节点。在上面的例子中，符合条件的元素个数是1，所以结果是长度为1的列表。 对于一些常用的属性，比如id和class等，我们可以不用attrs来传递。比如，要查询id为list-1的节点，可以直接传入id这个参数。还是上面的文本，我们换一种方式来查询： 1234567891011121314from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all(id='list-1'))print(soup.find_all(class_='element'))#输出如下：[&lt;ul class="list" id="list-1"&gt;&lt;li class="element"&gt;Foo&lt;/li&gt;&lt;li class="element"&gt;Bar&lt;/li&gt;&lt;li class="element"&gt;Jay&lt;/li&gt;&lt;/ul&gt;][&lt;li class="element"&gt;Foo&lt;/li&gt;, &lt;li class="element"&gt;Bar&lt;/li&gt;, &lt;li class="element"&gt;Jay&lt;/li&gt;, &lt;li class="element"&gt;Foo&lt;/li&gt;, &lt;li class="element"&gt;Bar&lt;/li&gt;] 这里直接传入id=&#39;list-1&#39;，就可以查询id为list-1的节点元素了。而对于class来说，由于class在Python里是一个关键字，所以后面需要加一个下划线，即class_=&#39;element&#39;，返回的结果依然还是Tag组成的列表。 3. text text参数可用来匹配节点的文本，传入的形式可以是字符串，可以是正则表达式对象： 1234567891011121314151617import rehtml='''&lt;div class="panel"&gt; &lt;div class="panel-body"&gt; &lt;a&gt;Hello, this is a link&lt;/a&gt; &lt;a&gt;Hello, this is a link, too&lt;/a&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all(text=re.compile('link')))#输出如下：['Hello, this is a link', 'Hello, this is a link, too'] 这里有两个a节点，其内部包含文本信息。这里在find_all()方法中传入text参数，该参数为正则表达式对象，结果返回所有匹配正则表达式的节点文本组成的列表。 find( )find()方法返回的是单个元素，也就是第一个匹配的元素，而find_all()返回的是所有匹配的元素组成的列表： 123456789101112131415161718192021222324252627282930313233343536373839html='''&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find(name='ul'))print(type(soup.find(name='ul')))print(soup.find(class_='list'))#输出如下：&lt;ul class="list" id="list-1"&gt;&lt;li class="element"&gt;Foo&lt;/li&gt;&lt;li class="element"&gt;Bar&lt;/li&gt;&lt;li class="element"&gt;Jay&lt;/li&gt;&lt;/ul&gt;&lt;class 'bs4.element.Tag'&gt;&lt;ul class="list" id="list-1"&gt;&lt;li class="element"&gt;Foo&lt;/li&gt;&lt;li class="element"&gt;Bar&lt;/li&gt;&lt;li class="element"&gt;Jay&lt;/li&gt;&lt;/ul&gt; 这里的返回结果不再是列表形式，而是第一个匹配的节点元素，类型依然是Tag类型。 另外，还有许多查询方法，其用法与前面介绍的find_all()、find()方法完全相同，只不过查询范围不同： find_parents()和find_parent()：前者返回所有祖先节点，后者返回直接父节点。 find_next_siblings()和find_next_sibling()：前者返回后面所有的兄弟节点，后者返回后面第一个兄弟节点。 find_previous_siblings()和find_previous_sibling()：前者返回前面所有的兄弟节点，后者返回前面第一个兄弟节点。 find_all_next()和find_next()：前者返回节点后所有符合条件的节点，后者返回第一个符合条件的节点。 find_all_previous()和find_previous()：前者返回节点后所有符合条件的节点，后者返回第一个符合条件的节点。 CSS选择器使用CSS选择器时，只需要调用select()方法，传入相应的CSS选择器即可： 1234567891011121314151617181920212223242526272829303132333435html='''&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.select('.panel .panel-heading'))print(soup.select('ul li'))print(soup.select('#list-2 .element'))print(type(soup.select('ul')[0]))# 输出如下：[&lt;div class="panel-heading"&gt;&lt;h4&gt;Hello&lt;/h4&gt;&lt;/div&gt;][&lt;li class="element"&gt;Foo&lt;/li&gt;, &lt;li class="element"&gt;Bar&lt;/li&gt;, &lt;li class="element"&gt;Jay&lt;/li&gt;, &lt;li class="element"&gt;Foo&lt;/li&gt;, &lt;li class="element"&gt;Bar&lt;/li&gt;][&lt;li class="element"&gt;Foo&lt;/li&gt;, &lt;li class="element"&gt;Bar&lt;/li&gt;]&lt;class 'bs4.element.Tag'&gt; 这里我们用了3次CSS选择器，返回的结果均是符合CSS选择器的节点组成的列表。例如，select(&#39;ul li&#39;)则是选择所有ul节点下面的所有li节点，结果便是所有的li节点组成的列表。 最后一句打印输出了列表中元素的类型。可以看到，类型依然是Tag类型。 select()方法同样支持嵌套选择。 获取属性尝试获取每个ul节点的id属性： 123456789101112from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')for ul in soup.select('ul'): print(ul['id']) print(ul.attrs['id']) #输出如下：list-1list-1list-2list-2 获取文本要获取文本，可以用前面所讲的string属性。此外还有一个方法get_text()： 12345678910111213141516171819from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')for li in soup.select('li'): print('Get Text:', li.get_text()) print('String:', li.string) # 输出如下：Get Text: FooString: FooGet Text: BarString: BarGet Text: JayString: JayGet Text: FooString: FooGet Text: BarString: Bar BeautifulSoup总结 推荐使用lxml解析库，必要时使用html.parser。 节点选择筛选功能弱但是速度快。 建议使用find()或者find_all()查询匹配单个结果或者多个结果。 如果对CSS选择器熟悉的话，可以使用select()方法选择。 更多知识请查看BeautifulSoup中文文档]]></content>
      <categories>
        <category>Spider</category>
        <category>解析</category>
      </categories>
      <tags>
        <tag>解析</tag>
        <tag>BeautifulSoup</tag>
        <tag>css选择器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyquery的使用]]></title>
    <url>%2F2018%2F12%2F14%2F%E7%BC%96%E7%A8%8B%2F%E8%A7%A3%E6%9E%90%2Fpyquery%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[简介PyQuery 库也是一个非常强大又灵活的网页解析库，语法与 jQuery 几乎完全相同，所以不用再去费心去记一些奇怪的方法了。 以下介绍使用方法 初始化它的初始化方式有多种，比如直接传入字符串，传入 URL，传入文件名，等等。 字符串初始化1234567891011121314151617181920212223html = '''&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)print(doc('li'))#输出如下：&lt;li class="item-0"&gt;first item&lt;/li&gt;&lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; url 初始化12345678from pyquery import PyQuery as pq#需要指定参数为 urldoc = pq(url='http://baidu.com')print(doc('title'))# 输出如下：&lt;title&gt;百度一下，你就知道&lt;/title&gt; 这样的话，PyQuery对象会首先请求这个 URL，然后用得到的 HTML 内容完成初始化，这其实就相当于用网页的源代码以字符串的形式传递给PyQuery类来初始化。 它与下面的功能是相同的： 1234from pyquery import PyQuery as pqimport requestsdoc = pq(requests.get('http://cuiqingcai.com').text)print(doc('title')) 文件初始化1234from pyquery import PyQuery as pq# 指定参数为 filenamedoc = pq(filename='demo.html')print(doc('li')) 基本 CSS 选择器用一个实例来感受 pyquery 的 CSS 选择器的用法： 123456789101112131415161718192021222324html = '''&lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)print(doc('#container .list li'))print(type(doc('#container .list li')))#输出如下:&lt;li class="item-0"&gt;first item&lt;/li&gt;&lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;class 'pyquery.pyquery.PyQuery'&gt; 查找节点子节点查找子节点时，需要用到find()方法，此时传入的参数是 CSS 选择器。这里还是以前面的 HTML 为例： 1234567891011121314151617181920212223242526from pyquery import PyQuery as pqdoc = pq(html)items = doc('.list')print(type(items))print(items)lis = items.find('li')print(type(lis))print(lis)# 输出结果：&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;li class="item-0"&gt;first item&lt;/li&gt;&lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; find()的查找范围是节点的所有子孙节点，而如果我们只想查找子节点，那么可以用children()方法： 123456789101112lis = items.children()print(type(lis))print(lis)#输出结果：&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;li class="item-0"&gt;first item&lt;/li&gt;&lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; 如果要筛选所有子节点中符合条件的节点，比如想筛选出子节点中class为active的节点，可以向children()方法传入CSS选择器.active： 1234567lis = items.children('.active')print(lis)#输出结果：&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; 父节点可以用parent()方法来获取某个节点的直接父节点： 123456789101112131415161718192021222324252627282930313233html = '''&lt;div class="wrap"&gt; &lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)items = doc('.list')container = items.parent()print(type(container))print(container)#输出结果：&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; 可以用parents()方法来获取某个节点的祖先节点： 12345678910111213141516171819202122232425262728293031from pyquery import PyQuery as pqdoc = pq(html)items = doc('.list')parents = items.parents()print(type(parents))print(parents)# 输出结果：&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;div class="wrap"&gt; &lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; 如果想要筛选某个祖先节点的话，可以向parents()方法传入 CSS 选择器，这样就会返回祖先节点中符合 CSS 选择器的节点： 123456789101112131415161718parent = items.parents('.wrap')print(parent)# 输出结果：# 输出结果少了一个节点，只保留了class为wrap的节点。&lt;div class="wrap"&gt; &lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; 兄弟节点如果要获取兄弟节点，可以使用siblings()方法： 123456789101112from pyquery import PyQuery as pqdoc = pq(html)li = doc('.list .item-0.active')print(li.siblings())# 输出结果：&lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0"&gt;first item&lt;/li&gt;&lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; 这里首先选择class为list的节点内部class为item-0和active的节点，也就是第三个li节点。那么，很明显，它的兄弟节点有4个，那就是第一、二、四、五个li节点。 如果要筛选某个兄弟节点，我们依然可以向siblings方法传入 CSS 选择器，这样就会从所有兄弟节点中挑选出符合条件的节点了： 12345678from pyquery import PyQuery as pqdoc = pq(html)li = doc('.list .item-0.active')print(li.siblings('.active'))# 输出结果：&lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; 遍历pyquery 的选择结果可能是多个节点，也可能是单个节点，类型都是PyQuery类型，并没有返回像 Beautiful Soup 那样的列表。 对于单个节点来说，可以直接打印输出，也可以直接转成字符串： 12345678910from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li)print(str(li))# 输出如下：&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; 对于多个节点的结果需要遍历来获取。例如，这里把每一个li节点进行遍历，需要调用items()方法： 1234567891011121314151617181920from pyquery import PyQuery as pqdoc = pq(html)lis = doc('li').items()print(type(lis))for li in lis: print(li, type(li)) # 输出如下：&lt;class 'generator'&gt;&lt;li class="item-0"&gt;first item&lt;/li&gt;&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;class 'pyquery.pyquery.PyQuery'&gt; 调用items()方法后，会得到一个生成器，遍历一下，就可以逐个得到li节点对象了，它的类型也是PyQuery类型。 获取信息获取属性提取到某个PyQuery类型的节点后，就可以调用attr()方法来获取属性： 1234567891011121314151617181920212223242526html = '''&lt;div class="wrap"&gt; &lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)a = doc('.item-0.active a')print(a, type(a))print(a.attr('href'))print(a.attr.href)# 输出如下：&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt; &lt;class 'pyquery.pyquery.PyQuery'&gt;link3.htmllink3.html 这里选中了一个元素，调用attr()方法和调用attr属性得到的结果是一样的。但在选中多个属性时它们只会返回第一个结果。我们可以通过遍历把他们提取出来： 123456789101112from pyquery import PyQuery as pqdoc = pq(html)a = doc('a')for item in a.items(): print(item.attr('href')) # 输出如下：link2.htmllink3.htmllink4.htmllink5.html 因此，在进行属性获取时，可以观察返回节点是一个还是多个，如果是多个，则需要遍历才能依次获取每个节点的属性。 获取文本获取文本可以调用text()方法来实现： 123456789101112131415161718192021222324html = '''&lt;div class="wrap"&gt; &lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)a = doc('.item-0.active a')print(a)print(a.text())# 输出结果：&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;third item 这里首先选中一个a节点，然后调用text()方法，就可以获取其内部的文本信息。此时它会忽略掉节点内部包含的所有HTML，只返回纯文字内容。 但如果想要获取这个节点内部的HTML文本，就要用html()方法了： 123456789from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li)print(li.html())# 输出如下：&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt; 当我们选中多个节点时，text( )返回该节点内部所有文本并合并成一个字符串，而html( )只会返回第一个，若要访问所有内容则需要遍历。 节点操作pyquery 提供了很多方法来对节点进行动态修改，比如为某个节点添加一个class，移除某个节点等。下面举几个典型的例子来说明、 addClass 和 removeClass 12345678910111213141516171819202122232425262728html = '''&lt;div class="wrap"&gt; &lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li)li.removeClass('active')print(li)li.addClass('active')print(li)# 输出如下：&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; 这里首先选中了第三个li节点，然后调用removeClass()方法，将li节点的active这个class移除，后来又调用addClass()方法，将class添加回来。所以说，addClass()和removeClass()这些方法可以动态改变节点的class属性。 attr 、text 和 html 1234567891011121314151617181920212223html = '''&lt;ul class="list"&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li)li.attr('name', 'link')print(li)li.text('changed item')print(li)li.html('&lt;span&gt;changed item&lt;/span&gt;')print(li)# 输出结果：&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0 active" name="link"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0 active" name="link"&gt;changed item&lt;/li&gt;&lt;li class="item-0 active" name="link"&gt;&lt;span&gt;changed item&lt;/span&gt;&lt;/li&gt; 这里我们首先选中li节点，然后调用attr()方法来修改属性，其中该方法的第一个参数为属性名，第二个参数为属性值。接着，调用text()和html()方法来改变节点内部的内容。 可以发现，调用attr()方法后，li节点多了一个原本不存在的属性name，其值为link。接着调用text()方法，传入文本之后，li节点内部的文本全被改为传入的字符串文本了。最后，调用html()方法传入HTML文本后，li节点内部又变为传入的 HTML 文本了。 所以说，如果attr()方法只传入第一个参数的属性名，则是获取这个属性值；如果传入第二个参数，可以用来修改属性值。text()和html()方法如果不传参数，则是获取节点内纯文本和HTML文本；如果传入参数，则进行赋值。 remove( ) 123456789101112131415161718html = '''&lt;div class="wrap"&gt; Hello, World &lt;p&gt;This is a paragraph.&lt;/p&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)wrap = doc('.wrap')print(wrap.text())wrap.find('p').remove()print(wrap.text())# 输出如下：Hello, World This is a paragraph.Hello, World 提取Hello, World这个字符串，而不要p节点内部的字符串，首先选中p节点，然后调用了remove()方法将其移除，然后这时wrap内部就只剩下Hello, World这句话了，然后再利用text()方法提取即可。 更多节点操作的方法可以参考官方文档：http://pyquery.readthedocs.io/en/latest/api.html 伪类选择器CSS 选择器之所以强大，还有一个很重要的原因，那就是它支持多种多样的伪类选择器，例如选择第一个节点、最后一个节点、奇偶数节点、包含某一文本的节点等。示例如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546html = '''&lt;div class="wrap"&gt; &lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('li:first-child')print(li)li = doc('li:last-child')print(li)li = doc('li:nth-child(2)')print(li)li = doc('li:gt(2)')print(li)li = doc('li:nth-child(2n)')print(li)li = doc('li:contains(second)')print(li)# 输出如下：&lt;li class="item-0"&gt;first item&lt;/li&gt;&lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; 这里我们使用了 CSS3 的伪类选择器，依次选择了第一个li节点、最后一个li节点、第二个li节点、第三个li之后的li节点（序号比2大的节点，序号从0开始）、偶数位置的li节点、包含second文本的li节点。 关于 CSS 选择器的更多用法，可以参考http://www.w3school.com.cn/css/index.asp。 到此为止，pyquery的常用用法就介绍完了。如果想查看更多的内容，可以参考pyquery的官方文档：http://pyquery.readthedocs.io]]></content>
      <categories>
        <category>Spider</category>
        <category>解析</category>
      </categories>
      <tags>
        <tag>css选择器</tag>
        <tag>pyquery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XPath lxml的基本用法]]></title>
    <url>%2F2018%2F12%2F14%2F%E7%BC%96%E7%A8%8B%2F%E8%A7%A3%E6%9E%90%2FXPath%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[简介XPath，全称 XML Path Language，即 XML 路径语言 可在 XML 中查找信息 支持 HTML 的查找 通过元素和属性进行导航 xml 文档（html 属于 xml）是由一系列节点构成的树，例如：12345678&lt;html&gt; &lt;body&gt; &lt;div &gt; &lt;p&gt;Hello world&lt;p&gt; &lt;a href="/home"&gt;Click here&lt;/a&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 在 python 中通过 lxml 库利用XPath对HTML进行解析 XPath 辅助工具： Chrome插件 ：Xpath Helper 打开/关闭 ：Ctrl + Shift + x Firefox插件 ：Xpath checker Xpath表达式编辑工具 ：XML Quire XPath 常用规则 表达式 描述 nodename 选取此节点的所有子节点 / 从当前节点中选区直接子节点 // 从当前节点中选取子孙节点 . 选取当前节点 .. 选取当前节点的父节点 @ 选取属性 * 选取所有元素子节点 @* 选区所有属性节点 @ATTR 选取名为ATTR的属性节点 text（） 选区所有文本子节点 XPath 轴 轴名称 表达式 描述 ancestor xpath(‘./ancestor::*’) 选取当前节点的所有先辈节点（父、祖父） ancestor-or-self xpath(‘./ancestor-or-self::*’) 选取当前节点的所有先辈节点以及节点本身 attribute xpath(‘./attribute::*’) 选取当前节点的所有属性 child xpath(‘./child::*’) 返回当前节点的所有子节点 descendant xpath(‘./descendant::*’) 返回当前节点的所有后代节点（子节点、孙节点） following xpath(‘./following::*’) 选取文档中当前节点结束标签后的所有节点 following-sibing xpath(‘./following-sibing::*’) 选取当前节点之后的兄弟节点 parent xpath(‘./parent::*’) 选取当前节点的父节点 preceding xpath(‘./preceding::*’) 选取文档中当前节点开始标签前的所有节点 preceding-sibling xpath(‘./preceding-sibling::*’) 选取当前节点之前的兄弟节点 self xpath(‘./self::*’) 选取当前节点 XPath 功能函数 table th:nth-of-type(1) { width: 100px; } 函数 用法用法 解释 starts-with xpath(‘//div[starts-with(@id,”ma”)]‘) 选取id值以ma开头的div节点 contains xpath(‘//div[contains(@id,”ma”)]‘) 选取id值包含ma的div节点 and xpath(‘//div[contains(@id,”ma”) and contains(@id,”in”)]‘) 选取id值包含ma和in的div节点 text() xpath(‘//div[contains(text(),”ma”)]‘) 选取节点文本包含ma的div节点 使用示例tostring（）方法：123456789101112131415from lxml import etreetext = '''&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt; &lt;/ul&gt; &lt;/div&gt;'''html = etree.HTML(text)result = etree.tostring(html)print(result.decode('utf-8')) 首先导入 lxml 的 etree 模块，然后进行初始化，构造解析对象。tostring()方法输出结果是bytes类型，这里利用 decode( ) 方法将其转成 str 类型。经过处理之后，li节点标签被补全，并且还自动添加了body、html节点。结果如下： 12345678910&lt;html&gt;&lt;body&gt;&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt; &lt;/li&gt;&lt;/ul&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 也可直接读取文本文件进行解析： 123456from lxml import etree# test.html的内容就是上面例子中的HTML代码html = etree.parse('./test.html', etree.HTMLParser())result = etree.tostring(html)print(result.decode('utf-8')) 输出结果略有不同，多了一个DOCTYPE的声明，不过对解析无任何影响，结果如下： 12345678910&lt;!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"&gt;&lt;html&gt;&lt;body&gt;&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt; &lt;/li&gt;&lt;/ul&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 用//访问所有节点 使用*代表匹配所有节点，返回形式是一个列表，每个元素是Element类型，其后跟了节点的名称。 12345678from lxml import etreehtml = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//*')print(result)# 结果如下：[&lt;Element html at 0x10510d9c8&gt;, &lt;Element body at 0x10510da08&gt;, &lt;Element div at 0x10510da48&gt;, &lt;Element ul at 0x10510da88&gt;, &lt;Element li at 0x10510dac8&gt;, &lt;Element a at 0x10510db48&gt;, &lt;Element li at 0x10510db88&gt;, &lt;Element a at 0x10510dbc8&gt;, &lt;Element li at 0x10510dc08&gt;, &lt;Element a at 0x10510db08&gt;, &lt;Element li at 0x10510dc48&gt;, &lt;Element a at 0x10510dc88&gt;, &lt;Element li at 0x10510dcc8&gt;, &lt;Element a at 0x10510dd08&gt;] 选取所有li节点，可以使用//，然后直接加上节点名称即可 12345678910from lxml import etreehtml = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//li')print(result)print(result[0])# 结果如下：[&lt;Element li at 0x105849208&gt;, &lt;Element li at 0x105849248&gt;, &lt;Element li at 0x105849288&gt;, &lt;Element li at 0x1058492c8&gt;, &lt;Element li at 0x105849308&gt;]&lt;Element li at 0x105849208&gt; 通过/或//查找元素的子节点或子孙节点 选择li节点的所有直接a子节点： 123456789from lxml import etree html = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//li/a')print(result)# 结果如下:[&lt;Element a at 0x106ee8688&gt;, &lt;Element a at 0x106ee86c8&gt;, &lt;Element a at 0x106ee8708&gt;, &lt;Element a at 0x106ee8748&gt;, &lt;Element a at 0x106ee8788&gt;] 获取所有子孙节点，就可以使用// 12345678from lxml import etree html = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//ul//a')print(result)# 输出结果与上面相同 用..来查找父节点 选中href属性为link4.html的a节点，然后再获取其父节点，然后再获取其class属性： 123456789from lxml import etree html = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//a[@href="link4.html"]/../@class')print(result)# 结果如下：['item-1'] 也可以通过parent::来获取父节点： 12345678from lxml import etree html = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//a[@href="link4.html"]/parent::*/@class')print(result)# 结果与上面相同 用@符号进行属性匹配 通过加入[@class=&quot;item-0&quot;]，选取 class 为 item-0 的 li 节点 12345678from lxml import etreehtml = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//li[@class="item-0"]')print(result)# 结果如下：[&lt;Element li at 0x10a399288&gt;, &lt;Element li at 0x10a3992c8&gt;] 用text()方法获取节点中的文本获取前面li节点中的文本有两种方法 1.先选取a节点再获取文本： 123456789from lxml import etree html = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//li[@class="item-0"]/a/text()')print(result)# 结果如下：['first item', 'fifth item'] 2.使用//选取文本： 123456789from lxml import etree html = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//li[@class="item-0"]//text()')print(result)# 结果如下：['first item', 'fifth item', '\n '] 这里的返回结果是3个，其中前两个是li的子节点a节点内部的文本，另外一个是最后一个li节点内部的文本，即换行符。 如果要想获取子孙节点内部的所有文本，可以直接用//加text()的方式，这样可以保证获取到最全面的文本信息，但是可能会夹杂一些换行符等特殊字符。如果想获取某些特定子孙节点下的所有文本，可以先选取到特定的子孙节点，然后再调用text()方法获取其内部文本，这样可以保证获取的结果是整洁的。 用@进行属性获取 获取所有li节点下所有a节点的href属性，返回列表： 123456789from lxml import etree html = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//li/a/@href')print(result)# 结果如下:['link1.html', 'link2.html', 'link3.html', 'link4.html', 'link5.html'] 这里我们通过@href即可获取节点的href属性。注意，此处和属性匹配的方法不同，属性匹配是中括号加属性名和值来限定某个属性，如[@href=&quot;link1.html&quot;]，而此处的@href指的是获取节点的某个属性 用contains()函数进行属性多值匹配 HTML 文本中li节点的class属性有两个值li和li-first，这时就需要用contains()函数： 1234567891011from lxml import etreetext = '''&lt;li class="li li-first"&gt;&lt;a href="link.html"&gt;first item&lt;/a&gt;&lt;/li&gt;'''html = etree.HTML(text)result = html.xpath('//li[contains(@class, "li")]/a/text()')print(result)# 结果如下:['first item'] 使用运算符and进行多属性匹配 根据多个属性确定一个节点时，需要同时匹配多个属性，此时使用and连接： 1234567891011from lxml import etreetext = '''&lt;li class="li li-first" name="item"&gt;&lt;a href="link.html"&gt;first item&lt;/a&gt;&lt;/li&gt;'''html = etree.HTML(text)result = html.xpath('//li[contains(@class, "li") and @name="item"]/a/text()')print(result)# 结果如下：['first item'] 传入索引按序选择 某些属性同时匹配了多个节点时，利用中括号传入索引来获取我们需要的某个节点： ​ 123456789101112131415161718192021222324252627282930313233from lxml import etree text = '''&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt; &lt;/ul&gt; &lt;/div&gt;'''html = etree.HTML(text)# 这里的序号是以1开头的，不是以0开头result = html.xpath('//li[1]/a/text()')print(result)# 中括号中传入last()返回最后一个节点result = html.xpath('//li[last()]/a/text()')print(result)# 选取了位置小于3的li节点，也就是位置序号为1和2的节点result = html.xpath('//li[position()&lt;3]/a/text()')print(result)# 我们选取了倒数第三个li节点，中括号中传入last()-2即可result = html.xpath('//li[last()-2]/a/text()')print(result)# 结果如下：['first item']['fifth item']['first item', 'second item']['third item'] 在XPath中，提供了100多个函数，包括存取、数值、字符串、逻辑、节点、序列等处理功能，它们的具体作用可以参考：http://www.w3school.com.cn/xpath/xpath_functions.asp。 节点轴选择 XPath提供了很多节点轴选择方法，包括获取子元素、兄弟元素、父元素、祖先元素等： 123456789101112131415161718192021222324252627282930313233343536373839from lxml import etree text = '''&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;&lt;span&gt;first item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt; &lt;/ul&gt; &lt;/div&gt;'''html = etree.HTML(text)result = html.xpath('//li[1]/ancestor::*')print(result)result = html.xpath('//li[1]/ancestor::div')print(result)result = html.xpath('//li[1]/attribute::*')print(result)result = html.xpath('//li[1]/child::a[@href="link1.html"]')print(result)result = html.xpath('//li[1]/descendant::span')print(result)result = html.xpath('//li[1]/following::*[2]')print(result)result = html.xpath('//li[1]/following-sibling::*')&lt;code class="lang-python"&gt;&lt;span class="kwd"&gt;print&lt;/span&gt;&lt;span class="pun"&gt;(&lt;/span&gt;&lt;span class="pln"&gt;result&lt;/span&gt;&lt;span class="pun"&gt;)&lt;/span&gt;# 结果如下：[&lt;Element html at 0x107941808&gt;, &lt;Element body at 0x1079418c8&gt;, &lt;Element div at 0x107941908&gt;, &lt;Element ul at 0x107941948&gt;][&lt;Element div at 0x107941908&gt;]['item-0'][&lt;Element a at 0x1079418c8&gt;][&lt;Element span at 0x107941948&gt;][&lt;Element a at 0x1079418c8&gt;][&lt;Element li at 0x107941948&gt;, &lt;Element li at 0x107941988&gt;, &lt;Element li at 0x1079419c8&gt;, &lt;Element li at 0x107941a08&gt;] 第一次选择时，我们调用了ancestor轴，可以获取所有祖先节点。其后需要跟两个冒号，然后是节点的选择器，这里我们直接使用*，表示匹配所有节点，因此返回结果是第一个li节点的所有祖先节点，包括html、body、div和ul。 第二次选择时，我们又加了限定条件，这次在冒号后面加了div，这样得到的结果就只有div这个祖先节点了。 第三次选择时，我们调用了attribute轴，可以获取所有属性值，其后跟的选择器还是*，这代表获取节点的所有属性，返回值就是li节点的所有属性值。 第四次选择时，我们调用了child轴，可以获取所有直接子节点。这里我们又加了限定条件，选取href属性为link1.html的a节点。 第五次选择时，我们调用了descendant轴，可以获取所有子孙节点。这里我们又加了限定条件获取span节点，所以返回的结果只包含span节点而不包含a节点。 第六次选择时，我们调用了following轴，可以获取当前节点之后的所有节点。这里我们虽然使用的是*匹配，但又加了索引选择，所以只获取了第二个后续节点。 第七次选择时，我们调用了following-sibling轴，可以获取当前节点之后的所有同级节点。这里我们使用*匹配，所以获取了所有后续同级节点。 以上是XPath轴的简单用法，更多轴的用法可以参考：http://www.w3school.com.cn/xpath/xpath_axes.asp。 如果想查询更多XPath的用法，可以查看：http://www.w3school.com.cn/xpath/index.asp。 如果想查询更多Python lxml库的用法，可以查看http://lxml.de/。]]></content>
      <categories>
        <category>Spider</category>
        <category>解析</category>
      </categories>
      <tags>
        <tag>解析</tag>
        <tag>XPath</tag>
        <tag>lxml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2F2018%2F12%2F13%2F%E7%BC%96%E7%A8%8B%2F%E8%A7%A3%E6%9E%90%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义 正则表达式是文本的高级匹配模式，提供搜索，替代，查找等功能。本质是由一系列特殊符号和字符组成的字串。 特点 方便进行检索修改文本的操作 支持编程语言众多 使用灵活多样 常用的匹配规则 table th:first-of-type { width: 100px; } 模式 描述 \w 匹配数字、字母及下划线 \W 匹配不是字母、数字及下划线的内容 \s 匹配任意空白字符，等价于[\t\n\r\f] \S 匹配任意非空字符 \d 匹配任意数字，等价于[0-9] \D 匹配任意非数字的字符 \A 匹配字符串开头 \Z 匹配字符串结尾，如果是存在换行，只匹配到换行前的结束字符串 \z 匹配字符串结尾，如果存在换行，同时还会匹配换行符 \G 匹配最后匹配完成的位置 \n 匹配一个换行符 \t 匹配一个制表符 ^ 匹配一个字符串的开头 $ 匹配一个字符串的结尾 . 匹配除换行符\n外任意字符 [...] 用来表示一组字符,单独列出：[amk] 匹配 a，m或k [^...] 不在[]中的字符：[^abc] 匹配除了a,b,c之外的字符 * 匹配0个或多个表达式 + 匹配一个或多个表达式 ? 匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式 {n} 匹配n个前面表达式。例如，o{2}不能匹配Bob中的o，但是能匹配food中的两个o。 {m,n} 匹配 m 到 n 次由前面的正则表达式定义的片段，贪婪方式 &nbsp;&#124; 匹配&nbsp;&#124;&nbsp;两边任意一个字符 ( ) 匹配括号内的表达式，也表示一个组 a b c &amp; # 匹配字符本身 python中的re模块提供了整个正则表达式的实现，利用这个库可以在python中使用正则表达式 re.match函数 re.match 尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回none。 函数语法： 1234re.match(pattern, string, flags=0)#pattern：匹配的正则表达式#string：要匹配的字符串#flags：标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等 我们可以使用 group(num) 或 groups() 匹配对象函数来获取匹配表达式。 示例:12345678import re content = 'Hello 123 4567 World_This is a Regex Demo'print(len(content))result = re.match('^Hello\s\d\d\d\s\d&#123;4&#125;\s\w&#123;10&#125;', content)print(result)print(result.group())print(result.span()) 运行结果：123441&lt;_sre.SRE_Match object; span=(0, 25), match='Hello 123 4567 World_This'&gt;Hello 123 4567 World_This(0, 25) 打印输出结果，可以看到结果是 SRE_Match 对象，这证明成功匹配。该对象有两个方法：group()方法可以输出匹配到的内容，结果是 Hello 123 4567 World_This，这恰好是正则表达式规则所匹配的内容；span()方法可以输出匹配的范围，结果是(0, 25)，这就是匹配到的结果字符串在原字符串中的位置范围。 匹配目标 用()将需要提取的字符串括起来即可 通用匹配 用.和*组合在一起可以匹配除换行符外的所有字符 贪婪与非贪婪 贪婪模式：正则表达式的重复默认总是尽可能多的向后匹配内容 ? {m,n} 非贪婪模式 ： 尽可能少的匹配内容 *? +? ?? {m,n}? 示例： 12re.findall(r'ab*?',"abbbbbcde")re.findall(r'ab+?',"abbbbbcde") 输出结果：12aab 在做匹配的时候，字符串中间尽量使用非贪婪匹配，也就是用.*?来代替.*，以免出现匹配结果缺失的情况。但需要注意，如果匹配的结果在字符串结尾，.*?就有可能匹配不到任何内容了，因为它会匹配尽可能少的字符。 修饰符re.S 这个修饰符的作用是使.匹配包括换行符在内的所有字符。 其他修饰符： table th:first-of-type { width: 100px; } 修饰符 描述 re.I 使匹配对大小写不敏感 re.L 做本地化识别（locale-aware）匹配 re.M 多行匹配，影响 ^ 和 $ re.U 根据 Unicode 字符集解析字符。这个标志影响 \w, \W, \b, \B. re.X 该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解。 转义匹配 当re模式里的字符与要匹配的目标字符串重复时，用反斜线\进行转义 re.search 函数 match()方法是从字符串的开头开始匹配的，一旦开头不匹配，那么整个匹配就失败了。 search()方法在匹配时会扫描整个字符串，然后返回第一个成功匹配的结果。也就是说，正则表达式可以是字符串的一部分，在匹配时，search()方法会依次扫描字符串，直到找到第一个符合规则的字符串，然后返回匹配内容，如果搜索完了还没有找到，就返回None。 函数语法: 12re.search(pattern, string, flags=0)#参数同match()方法 re.findall 方法 在字符串中找到正则表达式所匹配的所有子串，并返回一个列表，如果没有找到匹配的，则返回空列表。 match 和 search 是匹配一次，findall 是匹配所有。 函数语法: 12re.findall(pattern, string, flags=0)#参数同match()方法 re.sub 方法 用于替换字符串中的匹配项 函数语法： 12345re.sub(pattern, repl, string, count=0)#pattern : 正则中的模式字符串。#repl : 替换的字符串，也可为一个函数。#string : 要被查找替换的原始字符串。#count : 模式匹配后替换的最大次数，默认 0 表示替换所有的匹配。 re.compile方法 compile 函数用于编译正则表达式，生成一个正则表达式（Pattern）对象 函数语法： 123re.compile(pattern[, flags])#pattern: 一个字符串形式的正则表达式#flags: 修饰符，可选，表示匹配模式，比如忽略大小写，多行模式等 re.finditer 方法 和 findall 类似，在字符串中找到正则表达式所匹配的所有子串，并把它们作为一个迭代器返回 函数语法: 1re.finditer(pattern, string, flags=0) 示例：12345import re it = re.finditer(r"\d+","12a32bc43jf3") for match in it: print (match.group() ) 输出：123412 32 43 3 re.split 方法 split 方法按照能够匹配的子串将字符串分割后返回列表 函数语法:12re.split(pattern, string[, maxsplit=0, flags=0])#maxsplit:分隔次数，maxsplit=1 分隔一次，默认为 0，不限制次数。]]></content>
      <categories>
        <category>Spider</category>
        <category>解析</category>
      </categories>
      <tags>
        <tag>re</tag>
        <tag>正则</tag>
        <tag>解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP响应状态码]]></title>
    <url>%2F2018%2F12%2F11%2F%E7%BC%96%E7%A8%8B%2F%E7%BD%91%E7%BB%9C%2FHTTP%E5%93%8D%E5%BA%94%E7%8A%B6%E6%80%81%E7%A0%81%2F</url>
    <content type="text"><![CDATA[常见HTTP状态码 HTTP状态码 当浏览者访问一个网页时，浏览者的浏览器会向网页所在服务器发出请求。当浏览器接收并显示网页前，此网页所在的服务器会返回一个包含HTTP状态码的信息头（server header）用以响应浏览器的请求。 HTTP状态码的英文为HTTP Status Code。 下面是常见的HTTP状态码： 200 - 请求成功 301 - 资源（网页等）被永久转移到其它URL 404 - 请求的资源（网页等）不存在 500 - 内部服务器错误 HTTP状态码分类 分类 分类描述 1** 信息，服务器收到请求，需要请求者继续执行操作 2** 成功，操作被成功接收并处理 3** 重定向，需要进一步的操作以完成请求 4** 客户端错误，请求包含语法错误或无法完成请求 5** 服务器错误，服务器在处理请求的过程中发生了错误 HTTP状态码列表 状态码 状态码英文名称 中文描述 100 Continue 继续。客户端应继续其请求 101 Switching Protocols 切换协议。服务器根据客户端的请求切换协议。只能切换到更高级的协议，例如，切换到HTTP的新版本协议 200 OK 请求成功。一般用于GET与POST请求 201 Created 已创建。成功请求并创建了新的资源 202 Accepted 已接受。已经接受请求，但未处理完成 203 Non-Authoritative Information 非授权信息。请求成功。但返回的meta信息不在原始的服务器，而是一个副本 204 No Content 无内容。服务器成功处理，但未返回内容。在未更新网页的情况下，可确保浏览器继续显示当前文档 205 Reset Content 重置内容。服务器处理成功，用户终端（例如：浏览器）应重置文档视图。可通过此返回码清除浏览器的表单域 206 Partial Content 部分内容。服务器成功处理了部分GET请求 300 Multiple Choices 多种选择。请求的资源可包括多个位置，相应可返回一个资源特征与地址的列表用于用户终端（例如：浏览器）选择 301 Moved Permanently 永久移动。请求的资源已被永久的移动到新URI，返回信息会包括新的URI，浏览器会自动定向到新URI。今后任何新的请求都应使用新的URI代替 302 Found 临时移动。与301类似。但资源只是临时被移动。客户端应继续使用原有URI 303 See Other 查看其它地址。与301类似。使用GET和POST请求查看 304 Not Modified 未修改。所请求的资源未修改，服务器返回此状态码时，不会返回任何资源。客户端通常会缓存访问过的资源，通过提供一个头信息指出客户端希望只返回在指定日期之后修改的资源 305 Use Proxy 使用代理。所请求的资源必须通过代理访问 306 Unused 已经被废弃的HTTP状态码 307 Temporary Redirect 临时重定向。与302类似。使用GET请求重定向 400 Bad Request 客户端请求的语法错误，服务器无法理解 401 Unauthorized 请求要求用户的身份认证 402 Payment Required 保留，将来使用 403 Forbidden 服务器理解请求客户端的请求，但是拒绝执行此请求 404 Not Found 服务器无法根据客户端的请求找到资源（网页）。通过此代码，网站设计人员可设置”您所请求的资源无法找到”的个性页面 405 Method Not Allowed 客户端请求中的方法被禁止 406 Not Acceptable 服务器无法根据客户端请求的内容特性完成请求 407 Proxy Authentication Required 请求要求代理的身份认证，与401类似，但请求者应当使用代理进行授权 408 Request Time-out 服务器等待客户端发送的请求时间过长，超时 409 Conflict 服务器完成客户端的PUT请求是可能返回此代码，服务器处理请求时发生了冲突 410 Gone 客户端请求的资源已经不存在。410不同于404，如果资源以前有现在被永久删除了可使用410代码，网站设计人员可通过301代码指定资源的新位置 411 Length Required 服务器无法处理客户端发送的不带Content-Length的请求信息 412 Precondition Failed 客户端请求信息的先决条件错误 413 Request Entity Too Large 由于请求的实体过大，服务器无法处理，因此拒绝请求。为防止客户端的连续请求，服务器可能会关闭连接。如果只是服务器暂时无法处理，则会包含一个Retry-After的响应信息 414 Request-URI Too Large 请求的URI过长（URI通常为网址），服务器无法处理 415 Unsupported Media Type 服务器无法处理请求附带的媒体格式 416 Requested range not satisfiable 客户端请求的范围无效 417 Expectation Failed 服务器无法满足Expect的请求头信息 500 Internal Server Error 服务器内部错误，无法完成请求 501 Not Implemented 服务器不支持请求的功能，无法完成请求 502 Bad Gateway 作为网关或者代理工作的服务器尝试执行请求时，从远程服务器接收到了一个无效的响应 503 Service Unavailable 由于超载或系统维护，服务器暂时的无法处理客户端的请求。延时的长度可包含在服务器的Retry-After头信息中 504 Gateway Time-out 充当网关或代理的服务器，未及时从远端服务器获取请求 505 HTTP Version not supported 服务器不支持请求的HTTP协议的版本，无法完成处理]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>http</tag>
        <tag>状态码</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫基本原理]]></title>
    <url>%2F2018%2F12%2F08%2F%E7%BC%96%E7%A8%8B%2FSpider%E5%9F%BA%E7%A1%80%2F%E7%88%AC%E8%99%AB%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[什么是爬虫 网络爬虫是一个自动提取网页的程序，它为搜索引擎从万维网上下载网页，是搜索引擎的重要组成。传统爬从一个或若干初始网页的URL开始，获得初始网页上的URL，在抓取网页的过程中，不断从当前页面上抽取新的URL放入队列,直到满足系统的一定停止条件. 我们可以把互联网比作一张大网，而爬虫（即网络爬虫）便是在网上爬行的蜘蛛。把网的节点比作一个个网页，爬虫爬到这就相当于访问了该页面，获取了其信息。可以把节点间的连线比作网页与网页之间的链接关系，这样蜘蛛通过一个节点后，可以顺着节点连线继续爬行到达下一个节点，即通过一个网页继续获取后续的网页，这样整个网的节点便可以被蜘蛛全部爬行到，网站的数据就可以被抓取下来了。 简单来说，爬虫就是获取网页并提取和保存信息的自动化程序。 爬虫的作用 Web爬虫作为搜索引擎的重要组成部分 使用聚焦网络爬虫实现任何门户网站上的搜索引擎或搜索功能。它有助于搜索引擎找到与搜索主题具有最高相关性的网页。 建立数据集 现在已经进入大数据时代，各行各业都需要大量的数据，利用爬虫可以建立数据集以用于研究、业务和其他目的。 爬虫的分类 通用Web爬虫 通用网络爬虫所爬取的目标数据是巨大的，并且爬行的范围也是非常大的，正是由于其爬取的数据是海量数据，故而对于这类爬虫来说，其爬取的性能要求是非常高的。这种网络爬虫主要应用于大型搜索引擎中，有非常高的应用价值。 或者应用于大型数据提供商。 聚焦网络爬虫 聚焦网络爬虫是按照预先定义好的主题有选择地进行网页爬取的一种爬虫，聚焦网络爬虫不像通用网络爬虫一样将目标资源定位在全互联网中，而是将爬取的目标网页定位在与主题相关的页面中，此时，可以大大节省爬虫爬取时所需的带宽资源和服务器资源。聚焦网络爬虫主要应用在对特定信息的爬取中，主要为某一类特定的人群提供服务。 增量Web爬虫 增量式网络爬虫，在爬取网页的时候，只爬取内容发生变化的网页或者新产生的网页，对于未发生内容变化的网页，则不会爬取。增量式网络爬虫在一定程度上能够保证所爬取的页面，尽可能是新页面。 深层网络爬虫 在互联网中，网页按存在方式分类，可以分为表层页面和深层页面。所谓的表层页面，指的是不需要提交表单，使用静态的链接就能够到达的静态页面；而深层页面则隐藏在表单后面，不能通过静态链接直接获取，是需要提交一定的关键词之后才能够获取得到的页面。在互联网中，深层页面的数量往往比表层页面的数量要多很多，故而，我们需要想办法爬取深层页面。 爬虫能抓什么样的数据 在网页中我们能看到各种各样的信息，最常见的便是常规网页，它们对应着 HTML 代码，而最常抓取的便是 HTML 源代码。 另外，可能有些网页返回的不是 HTML 代码，而是一个 JSON 字符串（其中 API 接口大多采用这样的形式），这种格式的数据方便传输和解析，它们同样可以抓取，而且数据提取更加方便。 此外，我们还可以看到各种二进制数据，如图片、视频和音频等。利用爬虫，我们可以将这些二进制数据抓取下来，然后保存成对应的文件名。 另外，还可以看到各种扩展名的文件，如 CSS、JavaScript 和配置文件等，这些其实也是最普通的文件，只要在浏览器里面可以访问到，就可以将其抓取下来。 上述内容其实都对应各自的 URL，是基于 HTTP 或 HTTPS 协议的，只要是这种数据，爬虫都可以抓取。 爬虫基本流程 爬虫首先要做的工作就是获取网页，这里就是获取网页的源代码。源代码里包含了网页的部分有用信息，所以只要把源代码获取下来，就可以从中提取想要的信息了。 确定要爬取的 URL 地址 确定我们要爬取的网站，得到他的 url 地址 发起请求 向网站发起一个 request 请求 Python提供了许多库来帮助我们实现这个操作，如 urllib、requests 等。我们可以用这些库来帮助我们实现 HTTP 请求操作 获取响应内容 请求成功之后会返回 response 包含所请求网页的 HTML 代码或者 JSON 字符串等。 解析内容 获取网页源代码后，接下来就是分析网页源代码，从中提取我们想要的数据。首先，最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式时比较复杂且容易出错。 另外，由于网页的结构有一定的规则，所以还有一些根据网页节点属性、CSS 选择器或 XPath 来提取网页信息的库，如 Beautiful Soup、pyquery、lxml 等。使用这些库，我们可以高效快速地从中提取网页信息，如节点的属性、文本值等。 保存数据 提取信息后，我们一般会将提取到的数据保存到某处以便后续使用。这里保存形式有多种多样，如可以简单保存为 TXT 文本或 JSON 文本，也可以保存到数据库，如 MySQL 和 MongoDB 等。 Request什么是 Request 浏览器发送消息给网址所在的服务器，这个过程叫 HTTP Request。 Request 中包含了什么请求方式 主要有 GET、POST 两种类型 GET：查询参数在URL地址上显示 POST：查询参数在表单 data 中 请求 URL URL ：统一资源定位符 https: //item.jd.com :80/443 /11936238.html #detail 协议 域名/IP地址 端口 访问资源的路径 锚点 请求头 包含请求时的头部信息，如 User-Agent、Host、Cookies 等信息。 User-Agent： 记录用户的浏览器、操作系统等,为了让用户获取更好的 HTML 页面效果 如：Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) 如：AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11) 各类浏览器内核： Mozilla Firefox : (Gecko内核) IE ：Trident (自己的内核) Linux : KHTML (like Gecko) Apple : Webkit (like KHTML) Google : Chrome (like Webkit) HOST： 客户端指定自己想访问的 http 服务器的域名/IP 地址和端口号。 Cookie： Cookies 里面保存了登录的凭证，有了它，只需要在下次请求携带 Cookies 发送请求而不必重新输入用户名、密码等信息重新登录了。 Response什么是 Response Response 是请求后返回的内容，包含所请求网页的 HTML 代码或者 JSON 字符串等。 Response中包含了什么响应状态 有多重响应状态，如200代表成功、301是跳转、404为找不到网页、502服务器错误 响应头 如内容类型、内容长度、服务器信息、设置 Cookie 等等； 响应体 最主要的部分，包含了请求资源的内容，如网页HTML、图片二进制数据等；]]></content>
      <categories>
        <category>Spider</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Spider</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
</search>
