<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Selenium的使用]]></title>
    <url>%2F2018%2F12%2F23%2F%E5%8A%A8%E6%80%81%E9%A1%B5%E9%9D%A2%E6%8A%93%E5%8F%96%2FSelenium%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[} Selenium简介Selenium是一个自动化测试工具，利用它可以驱动浏览器执行特定的动作，如点击下拉等操作，同时还可以获取浏览器呈现的页面的源代码，做到可见即可爬。对于一些 JS 动态渲染的页面来说，此种方式非常有效。 基本使用声明浏览器对象Selenium支持非常多的浏览器，如Chrome、Firefox、Edge等，还有Android、BlackBerry等手机端的浏览器。另外，也支持无界面浏览器PhantomJS。 我们可以通过如下方法调用浏览器： 12345from selenium import webdriver browser = webdriver.Chrome()browser = webdriver.Firefox()browser = webdriver.PhantomJS() 访问页面可以用get()方法来请求网页，参数传入链接URL即可： 123456from selenium import webdriver browser = webdriver.Chrome()browser.get('https://www.taobao.com')print(browser.page_source)browser.close() 查找节点单个节点 find_element_by_xx 12345678find_element_by_idfind_element_by_namefind_element_by_xpathfind_element_by_link_textfind_element_by_partial_link_textfind_element_by_tag_namefind_element_by_class_namefind_element_by_css_selector find_element() 12345from selenium import webdriverfrom selenium.webdriver.common.by import Bybrowser.find_element(By.ID, 'xx')browser.find_element(By.NAME, 'xx') 多个节点得到的内容会变成列表类型，列表中的每个节点都是WebElement类型。 12345678910111213# find_element_by_xxfind_elements_by_idfind_elements_by_namefind_elements_by_xpathfind_elements_by_link_textfind_elements_by_partial_link_textfind_elements_by_tag_namefind_elements_by_class_namefind_elements_by_css_selector# find_elements()browser.find_elements(By.ID, 'xx')browser.find_elements(By.NAME, 'xx') 节点交互让浏览器模拟执行一些动作，常用方法有： 输入文字send_keys()方法 清空文字clear()方法 点击按钮click()方法 更多的操作可以参见官方文档的交互动作介绍：http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.remote.webelement 动作链上面的交互动作都是针对某个节点执行的。比如，对于输入框，我们就调用它的输入文字和清空文字方法；对于按钮，就调用它的点击方法。其实，还有另外一些操作，它们没有特定的执行对象，比如鼠标拖曳、键盘按键等，这些动作用另一种方式来执行，那就是动作链。 比如，实现一个节点的拖曳操作，将某个节点从一处拖曳到另外一处，可以这样实现： 123456789101112from selenium import webdriverfrom selenium.webdriver import ActionChains browser = webdriver.Chrome()url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'browser.get(url)browser.switch_to.frame('iframeResult')source = browser.find_element_by_css_selector('#draggable')target = browser.find_element_by_css_selector('#droppable')actions = ActionChains(browser)actions.drag_and_drop(source, target)actions.perform() 更多的动作链操作可以参考官方文档：http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains 执行JavaScript对于某些操作，Selenium API并没有提供。比如，下拉进度条，它可以直接模拟运行JavaScript，此时使用execute_script()方法即可实现，代码如下： 123456from selenium import webdriver browser = webdriver.Chrome()browser.get('https://www.zhihu.com/explore')browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')browser.execute_script('alert("To Bottom")') 这里就利用execute_script()方法将进度条下拉到最底部，然后弹出alert提示框。 所以说有了这个方法，基本上API没有提供的所有功能都可以用执行JavaScript的方式来实现了。 获取节点信息获取属性我们可以使用get_attribute()方法来获取节点的属性，但是其前提是先选中这个节点，示例如下： 123456789101112131415from selenium import webdriverfrom selenium.webdriver import ActionChains browser = webdriver.Chrome()url = 'https://www.zhihu.com/explore'browser.get(url)logo = browser.find_element_by_id('zh-top-link-logo')print(logo)print(logo.get_attribute('class'))# 输出如下：&lt;selenium.webdriver.remote.webelement.WebElement (session="e08c0f28d7f44d75ccd50df6bb676104", element="0.7236390660048155-1")&gt;zu-top-link-logo 运行之后，程序便会驱动浏览器打开知乎页面，然后获取知乎的logo节点，最后打印出它的class。 获取文本值每个WebElement节点都有text属性，直接调用这个属性就可以得到节点内部的文本信息，这相当于Beautiful Soup的get_text()方法、pyquery的text()方法，示例如下： 1234567891011from selenium import webdriver browser = webdriver.Chrome()url = 'https://www.zhihu.com/explore'browser.get(url)input = browser.find_element_by_class_name('zu-top-add-question')print(input.text)# 输出如下：提问 这里依然先打开知乎页面，然后获取“提问”按钮这个节点，再将其文本值打印出来。 获取id、位置、标签名和大小123456789101112131415161718192021from selenium import webdriver browser = webdriver.Chrome()url = 'https://www.zhihu.com/explore'browser.get(url)input = browser.find_element_by_class_name('zu-top-add-question')# 获取节点idprint(input.id)# 获取节点在页面中的相对位置print(input.location)# 获取标签名称print(input.tag_name)# 获取节点的大小print(input.size)# 输出如下：0.882088515167762-1&#123;'x': 681, 'y': 7&#125;button&#123;'height': 32, 'width': 66&#125; 切换Frame我们知道网页中有一种节点叫作iframe，也就是子Frame，相当于页面的子页面，它的结构和外部网页的结构完全一致。Selenium打开页面后，它默认是在父级Frame里面操作，而此时如果页面中还有子Frame，它是不能获取到子Frame里面的节点的。这时就需要使用switch_to.frame()方法来切换Frame。示例如下： 1234567891011121314151617181920212223import timefrom selenium import webdriverfrom selenium.common.exceptions import NoSuchElementException browser = webdriver.Chrome()url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'browser.get(url)browser.switch_to.frame('iframeResult')try: logo = browser.find_element_by_class_name('logo')except NoSuchElementException: print('NO LOGO')browser.switch_to.parent_frame()logo = browser.find_element_by_class_name('logo')print(logo)print(logo.text)# 输出如下：NO LOGO&lt;selenium.webdriver.remote.webelement.WebElement (session="4bb8ac03ced4ecbdefef03ffdc0e4ccd", element="0.13792611320464965-2")&gt;RUNOOB.COM 延时等待在Selenium中，get()方法会在网页框架加载结束后结束执行，此时如果获取page_source，可能并不是浏览器完全加载完成的页面，如果某些页面有额外的Ajax请求，我们在网页源代码中也不一定能成功获取到。所以，这里需要延时等待一定时间，确保节点已经加载出来。 这里等待的方式有两种：一种是隐式等待，一种是显式等待。 隐式等待当使用隐式等待执行测试的时候，如果Selenium没有在DOM中找到节点，将继续等待，超出设定时间后，则抛出找不到节点的异常。换句话说，当查找节点而节点并没有立即出现的时候，隐式等待将等待一段时间再查找DOM，默认的时间是0。示例如下： 1234567from selenium import webdriver browser = webdriver.Chrome()browser.implicitly_wait(10)browser.get('https://www.zhihu.com/explore')input = browser.find_element_by_class_name('zu-top-add-question')print(input) 这里我们用implicitly_wait()方法实现了隐式等待。 显式等待隐式等待的效果其实并没有那么好，因为我们只规定了一个固定时间，而页面的加载时间会受到网络条件的影响。 这里还有一种更合适的显式等待方法，它指定要查找的节点，然后指定一个最长等待时间。如果在规定时间内加载出来了这个节点，就返回查找的节点；如果到了规定时间依然没有加载出该节点，则抛出超时异常。示例如下： 123456789101112131415161718192021222324from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as EC browser = webdriver.Chrome()browser.get('https://www.taobao.com/')wait = WebDriverWait(browser, 10)input = wait.until(EC.presence_of_element_located((By.ID, 'q')))button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.btn-search')))print(input, button)# 当网速较好时能够加载完成，输出如下：&lt;selenium.webdriver.remote.webelement.WebElement (session="07dd2fbc2d5b1ce40e82b9754aba8fa8", element="0.5642646294074107-1")&gt;&lt;selenium.webdriver.remote.webelement.WebElement (session="07dd2fbc2d5b1ce40e82b9754aba8fa8", element="0.5642646294074107-2")&gt;# 当网络有问题指定时间没有加载完成，输出如下：TimeoutException Traceback (most recent call last)&lt;ipython-input-4-f3d73973b223&gt; in &lt;module&gt;() 7 browser.get('https://www.taobao.com/') 8 wait = WebDriverWait(browser, 10)----&gt; 9 input = wait.until(EC.presence_of_element_located((By.ID, 'q'))) 这里首先引入WebDriverWait这个对象，指定最长等待时间，然后调用它的until()方法，传入要等待条件expected_conditions。比如，这里传入了presence_of_element_located这个条件，代表节点出现的意思，其参数是节点的定位元组，也就是ID为q的节点搜索框。 这样可以做到的效果就是，在10秒内如果ID为q的节点（即搜索框）成功加载出来，就返回该节点；如果超过10秒还没有加载出来，就抛出异常。 对于按钮，可以更改一下等待条件，比如改为element_to_be_clickable，也就是可点击，所以查找按钮时查找CSS选择器为.btn-search的按钮，如果10秒内它是可点击的，也就是成功加载出来了，就返回这个按钮节点；如果超过10秒还不可点击，也就是没有加载出来，就抛出异常。 其他的等待条件及其含义： 等待条件 含义 title_is 标题是某内容 title_contains 标题包含某内容 presence_of_element_located 节点加载出来，传入定位元组，如(By.ID, &#39;p&#39;) visibility_of_element_located 节点可见，传入定位元组 visibility_of 可见，传入节点对象 presence_of_all_elements_located 所有节点加载出来 text_to_be_present_in_element 某个节点文本包含某文字 text_to_be_present_in_element_value 某个节点值包含某文字 frame_to_be_available_and_switch_to_it 加载并切换 invisibility_of_element_located 节点不可见 element_to_be_clickable 节点可点击 staleness_of 判断一个节点是否仍在DOM，可判断页面是否已经刷新 element_to_be_selected 节点可选择，传节点对象 element_located_to_be_selected 节点可选择，传入定位元组 element_selection_state_to_be 传入节点对象以及状态，相等返回True，否则返回False element_located_selection_state_to_be 传入定位元组以及状态，相等返回True，否则返回False alert_is_present 是否出现警告 关于更多等待条件的参数及用法，可以参考官方文档：http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.support.expected_conditions 前进和后退平常使用浏览器时都有前进和后退功能，Selenium也可以完成这个操作，它使用back()方法后退，使用forward()方法前进。示例如下： 1234567891011import timefrom selenium import webdriver browser = webdriver.Chrome()browser.get('https://www.baidu.com/')browser.get('https://www.taobao.com/')browser.get('https://www.python.org/')browser.back()time.sleep(1)browser.forward()browser.close() Cookies使用Selenium，还可以方便地对Cookies进行操作，例如获取、添加、删除Cookies等。示例如下： 12345678910111213141516from selenium import webdriver browser = webdriver.Chrome()browser.get('https://www.zhihu.com/explore')print(browser.get_cookies())browser.add_cookie(&#123;'name': 'name', 'domain': 'www.zhihu.com', 'value': 'germey'&#125;)print(browser.get_cookies())browser.delete_all_cookies()print(browser.get_cookies())# 输出如下：[&#123;'secure': False, 'value': '"NGM0ZTM5NDAwMWEyNDQwNDk5ODlkZWY3OTkxY2I0NDY=|1491604091|236e34290a6f407bfbb517888849ea509ac366d0"', 'domain': '.zhihu.com', 'path': '/', 'httpOnly': False, 'name': 'l_cap_id', 'expiry': 1494196091.403418&#125;][&#123;'secure': False, 'value': 'germey', 'domain': '.www.zhihu.com', 'path': '/', 'httpOnly': False, 'name': 'name'&#125;, &#123;'secure': False, 'value': '"NGM0ZTM5NDAwMWEyNDQwNDk5ODlkZWY3OTkxY2I0NDY=|1491604091|236e34290a6f407bfbb517888849ea509ac366d0"', 'domain': '.zhihu.com', 'path': '/', 'httpOnly': False, 'name': 'l_cap_id', 'expiry': 1494196091.403418&#125;][] 首先，我们访问了知乎。加载完成后，浏览器实际上已经生成Cookies了。接着，调用get_cookies()方法获取所有的Cookies。然后，我们添加一个Cookie，这里传入一个字典，有name、domain和value等内容。接下来，再次获取所有的Cookies。可以发现，结果就多了这一项新加的Cookie。最后，调用delete_all_cookies()方法删除所有的Cookies。再重新获取，发现结果就为空了。 选项卡管理在访问网页的时候，会开启一个个选项卡。在Selenium中，我们也可以对选项卡进行操作。示例如下： 1234567891011121314151617import timefrom selenium import webdriver browser = webdriver.Chrome()browser.get('https://www.baidu.com')browser.execute_script('window.open()')print(browser.window_handles)browser.switch_to_window(browser.window_handles[1])browser.get('https://www.taobao.com')time.sleep(1)browser.switch_to_window(browser.window_handles[0])browser.get('https://python.org')# 输出如下：['CDwindow-4f58e3a7-7167-4587-bedf-9cd8c867f435', 'CDwindow-6e05f076-6d77-453a-a36c-32baacc447df'] 异常处理在使用Selenium的过程中，难免会遇到一些异常，例如超时、节点未找到等错误，一旦出现此类错误，程序便不会继续运行了。这里我们可以使用try except语句来捕获各种异常。 首先，演示一下节点未找到的异常，尝试选择一个并不存在的节点，示例如下： 12345678910111213from selenium import webdriver browser = webdriver.Chrome()browser.get('https://www.baidu.com')browser.find_element_by_id('hello')# 输出如下：NoSuchElementException Traceback (most recent call last)&lt;ipython-input-23-978945848a1b&gt; in &lt;module&gt;() 3 browser = webdriver.Chrome() 4 browser.get('https://www.baidu.com')----&gt; 5 browser.find_element_by_id('hello') 可以看到，这里抛出了NoSuchElementException异常，这通常是节点未找到的异常。为了防止程序遇到异常而中断，我们需要捕获这些异常，示例如下： 12345678910111213141516171819from selenium import webdriverfrom selenium.common.exceptions import TimeoutException, NoSuchElementException browser = webdriver.Chrome()try: browser.get('https://www.baidu.com')except TimeoutException: print('Time Out')try: browser.find_element_by_id('hello')except NoSuchElementException: print('No Element')finally: browser.close() # 输出如下：No Element 这里我们使用try except来捕获各类异常。比如，我们对find_element_by_id()查找节点的方法捕获NoSuchElementException异常，这样一旦出现这样的错误，就进行异常处理，程序也不会中断了。 关于更多的异常类，可以参考官方文档：http://selenium-python.readthedocs.io/api.html#module-selenium.common.exceptions]]></content>
      <categories>
        <category>Spider</category>
        <category>动态页面抓取</category>
      </categories>
      <tags>
        <tag>Selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ajax实践_抓取今日头条]]></title>
    <url>%2F2018%2F12%2F22%2F%E5%8A%A8%E6%80%81%E9%A1%B5%E9%9D%A2%E6%8A%93%E5%8F%96%2FAjax%E6%8A%93%E5%8F%96%E4%BB%8A%E6%97%A5%E5%A4%B4%E6%9D%A1%2F</url>
    <content type="text"><![CDATA[抓取分析打开今日头条的首页http://www.toutiao.com/ 右上角有一个搜索入口，这里尝试抓取街拍美图，所以输入“街拍”二字搜索一下 这时我们可以打开开发者工具查看所有网络请求中的第一个请求，查看他的 Response Body，我们可以发现网页中的内容并不包含在这条请求中，所以我们初步判断 这些内容是由 Ajax 加载，然后用 JS 渲染出来的。然后我们切换到 XHR 过滤选项卡，这时可以看到有 Ajax 请求。 点击data字段展开，发现这里有许多条数据。点击第一条展开，可以发现有一个title字段，它的值正好就是页面中第一条数据的标题。再检查一下其他数据，也正好是一一对应的。如下图： 这就确定了这些数据确实是由Ajax加载的。 我们的目的是要抓取其中的美图，这里一组图就对应前面data字段中的一条数据。每条数据还有一个image_detail字段，它是列表形式，这其中就包含了组图的所有图片列表，如下图： 因此，我们只需要将列表中的url字段提取出来并下载下来就好了。每一组图都建立一个文件夹，文件夹的名称就为组图的标题。 接下来，就可以直接用Python来模拟这个Ajax请求，然后提取出相关美图链接并下载。但是在这之前，我们还需要分析一下URL的规律。 切换回Headers选项卡，观察一下它的请求URL和Headers信息，如下图： 可以看到，这是一个GET请求，请求URL的参数有offset、format、keyword、autoload、count、cur_tab和from等。我们需要找出这些参数的规律，因为这样才可以方便地用程序构造出来。 接下来，可以滑动页面，多加载一些新结果。在加载的同时可以发现，Network中又出现了许多Ajax请求，如下图： 这里观察一下后续链接的参数，发现变化的参数只有offset，其他参数都没有变化，而且第二次请求的offset值为20，第三次为40，第四次为60，所以可以发现规律，这个offset值就是偏移量，进而可以推断出count参数就是一次性获取的数据条数。因此，我们可以用offset参数来控制数据分页。这样一来，我们就可以通过接口批量获取数据了，然后将数据解析，将图片下载下来即可。 抓取实现首先，实现方法get_page()来加载单个Ajax请求的结果。其中唯一变化的参数就是offset，所以我们将它当作参数传递，实现如下： 12345678910111213141516171819202122232425262728import requestsfrom urllib.parse import urlencode headers = &#123; 'user-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36 ' 'cookie': 'tt_webid=6637749626995492359; WEATHER_CITY=%E5%8C%97%E4%BA%AC; UM_distinctid=167d547f1b3460-020c50f8b937b2-424e0b28-1fa400-167d547f1b489; CNZZDATA1259612802=1003758837-1545470092-%7C1545470092; tt_webid=6637749626995492359; __tasessionId=hkxc5jku31545471521344; csrftoken=6767aaa50120c05ed4a74271670aef22' 'referer': 'https://www.toutiao.com/search/?keyword=%E8%A1%97%E6%8B%8D' 'x-requested-with': 'XMLHttpRequest' &#125; def get_page(offset): params = &#123; 'offset': offset, 'format': 'json', 'keyword': '街拍', 'autoload': 'true', 'count': '20', 'cur_tab': '1', 'from': 'search_tab' &#125; base_url = 'http://www.toutiao.com/search_content/?' url = base_url + urlencode(params) try: response = requests.get(url，headers=headers) if response.status_code == 200: return response.json() except requests.ConnectionError: return None 这里我们用urlencode()方法构造请求的GET参数，然后用requests请求这个链接，如果返回状态码为200，则调用response的json()方法将结果转为JSON格式，然后返回。 接下来，再实现一个解析方法：提取每条数据的image_detail字段中的每一张图片链接，将图片链接和图片所属的标题一并返回，此时可以构造一个生成器。实现代码如下： 12345678910111213def get_images(json): if json.get('data'): data = json.get('data') for item in data: if item.get('cell_type') is not None: continue title = item.get('title') images = item.get('image_list') for image in images: yield &#123; 'image': 'https:' + image.get('url'), 'title': title &#125; 接下来，实现一个保存图片的方法save_image()，其中item就是前面get_images()方法返回的一个字典。在该方法中，首先根据item的title来创建文件夹，然后请求这个图片链接，获取图片的二进制数据，以二进制的形式写入文件。图片的名称可以使用其内容的MD5值，这样可以去除重复。相关代码如下： 123456789101112131415161718192021import osfrom hashlib import md5# 保存图片至指定路径def save_image(item): img_path = 'D:/imagedata/&#123;title&#125;'.format(title=item.get('title')) if not os.path.exists(img_path): os.makedirs(img_path) try: pic = requests.get(item.get('image')) if pic.status_code == 200: file_path = img_path + '&#123;file_name&#125;.jpg'.format( file_name=md5(pic.content).hexdigest()) if not os.path.exists(file_path): with open(file_path, 'wb') as f: f.write(pic.content) print('%s Save Successful' % file_path) else: print('Already Downloaded', file_path) except requests.ConnectionError: print('Failed to Save Image，item %s' % item) 最后，只需要构造一个offset数组，遍历offset，提取图片链接，并将其下载即可： 1234567891011121314151617from multiprocessing.pool import pooldef main(offset): json = get_page(offset) for item in get_image(json): print(item) save_image(item) GROUP_START = 1GROUP_END = 20if __name__ == "__main__": pool = pool() groups = ([i * 20 for i in range(GROUP_START,GROUP_END + 1)]) pool.map(main,groups) pool.close() pool.join() 这里定义了分页的起始页数和终止页数，分别为GROUP_START和GROUP_END，还利用了多线程的线程池，调用其map()方法实现多线程下载。 这样整个程序就完成了，运行之后可以发现街拍美图都分文件夹保存下来了。 运行结果如下： 完整代码地址：github.com/wshaoxin/Spider/toutiao_jiepai.py]]></content>
      <categories>
        <category>Spider</category>
        <category>动态页面抓取</category>
      </categories>
      <tags>
        <tag>Ajax</tag>
        <tag>requests</tag>
        <tag>项目</tag>
        <tag>今日头条</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[选择排序]]></title>
    <url>%2F2018%2F12%2F21%2F%E7%AE%97%E6%B3%95%2F%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[原理： 选择排序比较好理解，好像是在一堆大小不一的球中进行选择（以从小到大，先选最小球为例）： 选择一个基准球 将基准球和余下的球进行一一比较，如果比基准球小，则进行交换 第一轮过后获得最小的球 在挑一个基准球，执行相同的动作得到次小的球 继续执行4，直到排序好 时间复杂度： O(n^2) 需要进行的比较次数为第一轮 n-1，n-2….1, 总的比较次数为 n*(n-1)/2 实现： 12345678910111213141516171819202122def selectedSort(myList): #获取list的长度 length = len(myList) #一共进行多少轮比较 for i in range(0,length-1): #默认设置最小值得index为当前值 smallest = i #用当先最小index的值分别与后面的值进行比较,以便获取最小index for j in range(i+1,length): #如果找到比当前值小的index,则进行两值交换 if myList[j]&lt;myList[smallest]: tmp = myList[j] myList[j] = myList[smallest] myList[smallest]=tmp #打印每一轮比较好的列表 print("Round ",i,": ",myList)myList = [1,4,5,0,6]print("Selected Sort: ")selectedSort(myList)]]></content>
      <categories>
        <category>每日算法</category>
      </categories>
      <tags>
        <tag>每日算法</tag>
        <tag>选择排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ajax简介]]></title>
    <url>%2F2018%2F12%2F21%2F%E5%8A%A8%E6%80%81%E9%A1%B5%E9%9D%A2%E6%8A%93%E5%8F%96%2FAjax%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[有时候我们在用requests抓取页面的时候，得到的结果可能和在浏览器中看到的不一样：在浏览器中可以看到正常显示的页面数据，但是使用requests得到的结果并没有。这是因为requests获取的都是原始的HTML文档，而浏览器中的页面则是经过JavaScript处理数据后生成的结果，这些数据的来源有多种，可能是通过Ajax加载的，可能是包含在HTML文档中的，也可能是经过JavaScript和特定算法计算后生成的。 对于第一种情况，数据加载是一种异步加载方式，原始的页面最初不会包含某些数据，原始页面加载完后，会再向服务器请求某个接口获取数据，然后数据才被处理从而呈现到网页上，这其实就是发送了一个Ajax请求。 照Web发展的趋势来看，这种形式的页面越来越多。网页的原始HTML文档不会包含任何数据，数据都是通过Ajax统一加载后再呈现出来的，这样在Web开发上可以做到前后端分离，而且降低服务器直接渲染页面带来的压力。 所以如果遇到这样的页面，直接利用requests等库来抓取原始页面，是无法获取到有效数据的，这时需要分析网页后台向接口发送的Ajax请求，如果可以用requests来模拟Ajax请求，那么就可以成功抓取了。 什么是AjaxAjax，全称为Asynchronous JavaScript and XML，即异步的JavaScript和XML。它不是一门编程语言，而是利用JavaScript在保证页面不被刷新、页面链接不改变的情况下与服务器交换数据并更新部分网页的技术。 对于传统的网页，如果想更新其内容，那么必须要刷新整个页面，但有了Ajax，便可以在页面不被全部刷新的情况下更新其内容。在这个过程中，页面实际上是在后台与服务器进行了数据交互，获取到数据之后，再利用JavaScript改变网页，这样网页内容就会更新了。 基本原理初步了解了Ajax之后，我们再来详细了解它的基本原理。发送Ajax请求到网页更新的这个过程可以简单分为以下3步： (1) 发送请求； (2) 解析内容； (3) 渲染网页。 下面我们分别来详细介绍这几个过程。 发送请求我们知道JavaScript可以实现页面的各种交互功能，Ajax也不例外，它也是由JavaScript实现的，实际上执行了如下代码： 1234567891011121314var xmlhttp;if (window.XMLHttpRequest) &#123; // code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest();&#125; else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject("Microsoft.XMLHTTP");&#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById("myDiv").innerHTML=xmlhttp.responseText; &#125;&#125;xmlhttp.open("POST","/ajax/",true);xmlhttp.send(); 这是JavaScript对Ajax最底层的实现，实际上就是新建了XMLHttpRequest对象，然后调用onreadystatechange属性设置了监听，然后调用open()和send()方法向某个链接（也就是服务器）发送了请求。前面用Python实现请求发送之后，可以得到响应结果，但这里请求的发送变成JavaScript来完成.由于设置了监听，所以当服务器返回响应时，onreadystatechange对应的方法便会被触发，然后在这个方法里面解析响应内容即可。 解析内容得到响应之后，onreadystatechange属性对应的方法便会被触发，此时利用xmlhttp的responseText属性便可取到响应内容。这类似于Python中利用requests向服务器发起请求，然后得到响应的过程。那么返回内容可能是HTML，可能是JSON，接下来只需要在方法中用JavaScript进一步处理即可。比如，如果是JSON的话，可以进行解析和转化。 渲染网页JavaScript有改变网页内容的能力，解析完响应内容之后，就可以调用JavaScript来针对解析完的内容对网页进行下一步处理了。比如，通过document.getElementById().innerHTML这样的操作，便可以对某个元素内的源代码进行更改，这样网页显示的内容就改变了，这样的操作也被称作DOM操作，即对Document网页文档进行操作，如更改、删除等。 上例中，document.getElementById(&quot;myDiv&quot;).innerHTML=xmlhttp.responseText便将ID为myDiv的节点内部的HTML代码更改为服务器返回的内容，这样myDiv元素内部便会呈现出服务器返回的新数据，网页的部分内容看上去就更新了。 我们观察到，这3个步骤其实都是由JavaScript完成的，它完成了整个请求、解析和渲染的过程。 再回想微博的下拉刷新，这其实就是JavaScript向服务器发送了一个Ajax请求，然后获取新的微博数据，将其解析，并将其渲染在网页中。 因此，我们知道，真实的数据其实都是一次次Ajax请求得到的，如果想要抓取这些数据，需要知道这些请求到底是怎么发送的，发往哪里，发了哪些参数。如果我们知道了这些，就可以用Python模拟这个发送操作，获取到其中的结果。]]></content>
      <categories>
        <category>Spider</category>
        <category>动态页面抓取</category>
      </categories>
      <tags>
        <tag>Ajax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速排序]]></title>
    <url>%2F2018%2F12%2F20%2F%E7%AE%97%E6%B3%95%2F%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[基本思想是： 通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。 如序列[6，8，1，4，3，9]，选择6作为基准数。从右向左扫描，寻找比基准数小的数字为3，交换6和3的位置，[3，8，1，4，6，9]，接着从左向右扫描，寻找比基准数大的数字为8，交换6和8的位置，[3，6，1，4，8，9]。重复上述过程，直到基准数左边的数字都比其小，右边的数字都比其大。然后分别对基准数左边和右边的序列递归进行上述方法。 实现代码如下： 123456789101112131415161718192021222324252627282930313233 def quick_sort(alist， start， end): """快速排序""" # 递归的退出条件 if start &gt;= end: return # 设定起始元素为要寻找位置的基准元素 mid = alist[start] # low 为序列左边的由左向右移动的游标 low = start # high 为序列右边的由右向左移动的游标 high = end while low &lt; high: # 如果 low 与 high 未重合，high 指向的元素不比基准元素小，则 high 向左移动 while low &lt; high and alist[high] &gt;= mid: high -= 1 # 将 high 指向的元素放到 low 的位置上 alist[low] = alist[high] # 如果 low 与 high 未重合，low 指向的元素比基准元素小，则 low 向右移动 while low &lt; high and alist[low] &lt; mid: low += 1 # 将 low 指向的元素放到 high 的位置上 alist[high] = alist[low] # 退出循环后，low 与 high 重合，此时所指位置为基准元素的正确位置 # 将基准元素放到该位置 alist[low] = mid # 对基准元素左边的子序列进行快速排序 quick_sort(alist， start， low-1) # 对基准元素右边的子序列进行快速排序 quick_sort(alist， low+1， end) alist = [54，26，93，17，77，31，44，55，20]quick_sort(alist，0，len(alist)-1)print(alist)]]></content>
      <categories>
        <category>每日算法</category>
      </categories>
      <tags>
        <tag>每日算法</tag>
        <tag>快排</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasiticsearch简介]]></title>
    <url>%2F2018%2F12%2F20%2FElasticsearch%2FElasiticsearch%2F</url>
    <content type="text"><![CDATA[什么是搜索 百度搜索：当我们想搜索任何的信息的时候，“百度一下，你就知道” 百度 != 搜索 垂直搜索（站内搜索） 互联网的搜索：电商网站，招聘网站，新闻网站，各种app… IT系统的搜索：OA软件，办公自动化软件 搜索就是在任何场景下，找寻你想要的信息。这个时候，输入一些搜索关键字，然后期望找到这个关键字相关的某些信息。 为什么不用数据库做搜索 比如说，每条记录的指定字段的文本可能会很长，“商品描述”的字段的长度可能长达数千甚至上万个字符，这时每次搜索都要对每条记录的所有文本进行扫描判断 还不能将搜索词拆分开来，尽可能去搜索更多的你期望的结果，比如搜索“生化机”就搜不出来“生化危机” 所以用数据库做搜索是不靠谱的性能很差 什么是全文检索 全文检索 什么是ElasticsearchElasticsearch 是一个开源的搜索引擎，建立在一个全文搜索引擎库 Apache Lucene™ 基础之上。 Lucene 可能是目前存在的拥有最先进，高性能和全功能搜索引擎功能的库。要用上 Lucene，我们需要编写 Java 并引用 Lucene 包才可以，而且我们需要对信息检索有一定程度的理解才能明白 Lucene 是怎么工作的，反正用起来没那么简单。 那么为了解决这个问题，Elasticsearch 就诞生了。Elasticsearch 也是使用 Java 编写的，它的内部使用 Lucene 做索引与搜索，但是它的目标是使全文检索变得简单，相当于 Lucene 的一层封装，它提供了一套简单一致的 RESTful API 来帮助我们实现存储和检索。 功能 分布式的实时文档存储，全文检索，结构化检索 一个分布式实时分析搜索引擎 能胜任上百个服务节点的扩展，并支持 PB 级别的结构化或者非结构化数据 适用场景 维基百科：全文检索，高亮，搜索推荐 Stack Overflow：全文检索，搜索相关问题和答案 Github：上千亿行代码的搜索 电商网站 招聘、门户 Elasticsearch的安装官方下载地址：https://www.elastic.co/downloads/elasticsearch]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
        <tag>搜索引擎</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[冒泡排序]]></title>
    <url>%2F2018%2F12%2F19%2F%E7%AE%97%E6%B3%95%2F%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[原理： 比较两个相邻的元素，将值大的元素交换至右端。 思路： 依次比较相邻的两个数，将小数放在前面，大数放在后面。即在第一趟：首先比较第1个和第2个数，将小数放前，大数放后。然后比较第2个数和第3个数，将小数放前，大数放后，如此继续，直至比较最后两个数，将小数放前，大数放后。重复第一趟步骤，直至全部排序完成。 第一趟比较完成后，最后一个数一定是数组中最大的一个数，所以第二趟比较的时候最后一个数不参与比较； 第二趟比较完成后，倒数第二个数也一定是数组中第二大的数，所以第三趟比较的时候最后两个数不参与比较； 依次类推，每一趟比较次数-1； 时间复杂度： 1.如果我们的数据正序，时间复杂度为O(n)。 2.如果我们的数据反序，时间复杂度为O(n2)。 综合之后时间复杂度为O(n2)。 python实现： 123456789def bubble_sort(list): for j in range(len(list)-1, 0, -1): for i in range(0,j): if list[i] &gt; list[i]: list[i], list[i + 1] = list[i + 1], list[i] return listlist = [5,2,18,0,36,9,7,45]print(bubble_sort(list)) 优化： 如果我们的数据正序，只需要一次循环就够了，所以我们针对特殊情况进行优化： 123456789101112def bubble_sort(list): for j in range(len(list)-1, 0, -1): count = 0 for i in range(0,j): if list[i] &gt; list[i]: list[i], list[i + 1] = list[i + 1], list[i] count += 1 if count == 0: returnlist = [5,2,18,0,36,9,7,45]print(bubble_sort(list))]]></content>
      <categories>
        <category>每日算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>冒泡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BeautifulSoup的使用]]></title>
    <url>%2F2018%2F12%2F17%2F%E8%A7%A3%E6%9E%90%2FBeautifulSoup%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[简介 Beautiful Soup是Python的一个HTML或XML的解析库，可以用它来方便地从网页中提取数据。 它借助网页的结构和属性等特性来解析网页。有了它，我们不用再去写一些复杂的正则表达式，只需要简单的几条语句，就可以完成网页中某个元素的提取。 官方的解释如下： Beautiful Soup提供一些简单的、Python式的函数来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。 Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为UTF-8编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时你仅仅需要说明一下原始编码方式就可以了。 Beautiful Soup已成为和lxml、html6lib一样出色的Python解释器，为用户灵活地提供不同的解析策略或强劲的速度。 解析器Beautiful Soup在解析时实际上依赖解析器，它除了支持Python标准库中的HTML解析器外，还支持一些第三方解析器（比如lxml）。 BeautifulSoup支持的解析器 解析器 使用方法 优势 劣势 Python标准库 BeautifulSoup(markup, &quot;html.parser&quot;) Python的内置标准库、执行速度适中、文档容错能力强 Python 2.7.3及Python 3.2.2之前的版本文档容错能力差 lxml HTML解析器 BeautifulSoup(markup, &quot;lxml&quot;) 速度快、文档容错能力强 需要安装C语言库 lxml XML解析器 BeautifulSoup(markup, &quot;xml&quot;) 速度快、唯一支持XML的解析器 需要安装C语言库 html5lib BeautifulSoup(markup, &quot;html5lib&quot;) 最好的容错性、以浏览器的方式解析文档、生成HTML5格式的文档 速度慢、不依赖外部扩展 推荐使用 lxml 作为解析器,因为效率更高. 在 Python2.7.3 之前的版本和 Python3中3.2.2 之前的版本,必须安装 lxml 或 html5lib, 因为那些 Python 版本的标准库中内置的 HTML 解析方法不够稳定. 基本用法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455from bs4 import BeautifulSouphtml = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;"""soup = BeautifulSoup(html, 'lxml')print(soup.prettify())print(soup.title.string)# 输出如下：&lt;html&gt; &lt;head&gt; &lt;title&gt; The Dormouse's story &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="title" name="dromouse"&gt; &lt;b&gt; The Dormouse's story &lt;/b&gt; &lt;/p&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt; &lt;!-- Elsie --&gt; &lt;/a&gt; , &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt; Lacie &lt;/a&gt; and &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt; Tillie &lt;/a&gt; ;and they lived at the bottom of a well. &lt;/p&gt; &lt;p class="story"&gt; ... &lt;/p&gt; &lt;/body&gt;&lt;/html&gt;The Dormouse's story 这里首先声明变量html，它是一个不完整的 HTML 字符串。将它当作第一个参数传给BeautifulSoup对象，第二个参数为解析器的类型（这里使用lxml），此时就完成了BeaufulSoup对象的初始化。 首先，调用prettify()方法。这个方法可以把要解析的字符串以标准的缩进格式输出。对于不标准的HTML字符串BeautifulSoup可以自动更正格式。这一步不是由prettify()方法做的，而是在初始化BeautifulSoup时就完成了。 然后调用soup.title.string，输出HTML中title节点的文本内容。 节点选择器适用于单个节点非常清晰的情况 选择元素123456789101112131415161718192021222324252627html = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;"""from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.title)print(type(soup.title))print(soup.title.string)print(soup.head)print(soup.p)# 输出如下：&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;class 'bs4.element.Tag'&gt;The Dormouse's story&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt; 首先打印输出title节点的选择结果，接下来输出它的类型，是bs4.element.Tag类型，这是Beautiful Soup中一个重要的数据结构。经过选择器选择后，选择结果都是这种Tag类型。Tag具有一些属性，比如string属性，调用该属性，可以得到节点的文本内容。 然后选择head节点，结果也是节点加其内部的所有内容。 最后，选择了p节点，只输出了第一个节点的内容。当有多个节点时，这种选择方式只会选择到第一个匹配的节点。 提取信息利用name属性获取节点的名称；在节点元素后面加中括号，传入属性名就可以获取属性值；利用string属性获取节点元素包含的文本内容： 1234567891011print(soup.title.name)print(soup.p['name'])print(soup.p['class'])print(soup.p.string)# 输出如下：titledromouse['title']The Dormouse's story 关联选择1.子节点和子孙节点： 选取节点元素之后，如果想要获取它的直接子节点，可以调用contents属性： 12345678910111213141516171819202122232425html = """&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and &lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class="story"&gt;...&lt;/p&gt;"""# 输出如下：['\n Once upon a time there were three little sisters; and their names were\n ', &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;, '\n', &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;, ' \n and\n ', &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;, '\n and they lived at the bottom of a well.\n '] 可以看到，返回结果是列表形式。p节点里既包含文本，又包含节点，最后会将它们以列表形式统一返回。 需要注意的是，列表中的每个元素都是p节点的直接子节点。比如第一个a节点里面包含一层span节点，这相当于孙子节点了，但是返回结果并没有单独把span节点选出来。所以说，contents属性得到的结果是直接子节点的列表。 同样，我们可以调用children属性得到相应的结果： 123456789101112131415161718192021222324from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.p.children)for i, child in enumerate(soup.p.children): print(i, child) # 输出如下&lt;list_iterator object at 0x1064f7dd8&gt;0 Once upon a time there were three little sisters; and their names were 1 &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;2 3 &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;4 and 5 &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;6 and they lived at the bottom of a well. 还是同样的HTML文本，这里调用了children属性来选择，返回结果是生成器类型。接下来，我们用for循环输出相应的内容。 如果要得到所有的子孙节点的话，可以调用descendants属性： 1234567891011121314151617181920212223242526272829303132from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.p.descendants)for i, child in enumerate(soup.p.descendants): print(i, child) # 输出如下：&lt;generator object descendants at 0x10650e678&gt;0 Once upon a time there were three little sisters; and their names were 1 &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;2 3 &lt;span&gt;Elsie&lt;/span&gt;4 Elsie5 6 7 &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;8 Lacie9 and 10 &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;11 Tillie12 and they lived at the bottom of a well. 此时返回结果还是生成器。遍历输出一下可以看到，这次的输出结果就包含了span节点。descendants会递归查询所有子节点，得到所有的子孙节点。 2.父节点和祖先节点 如果要获取某个节点元素的父节点，可以调用parent属性： 123456789101112131415161718192021222324252627html = """&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;/p&gt; &lt;p class="story"&gt;...&lt;/p&gt;"""from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.a.parent)#输出如下：&lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;&lt;/p&gt; 这里我们选择的是第一个a节点的父节点元素。很明显，它的父节点是p节点，输出结果便是p节点及其内部的内容。 需要注意的是，这里输出的仅仅是a节点的直接父节点，而没有再向外寻找父节点的祖先节点。如果想获取所有的祖先节点，可以调用parents属性： 12345678910111213141516171819202122232425262728293031323334353637383940414243html = """&lt;html&gt; &lt;body&gt; &lt;p class="story"&gt; &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;/p&gt;"""from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(type(soup.a.parents))print(list(enumerate(soup.a.parents)))# 输出如下：&lt;class 'generator'&gt;[(0, &lt;p class="story"&gt;&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;), (1, &lt;body&gt;&lt;p class="story"&gt;&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/body&gt;), (2, &lt;html&gt;&lt;body&gt;&lt;p class="story"&gt;&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;), (3, &lt;html&gt;&lt;body&gt;&lt;p class="story"&gt;&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;)] 可以发现，返回结果是生成器类型。这里用列表输出了它的索引和内容，而列表中的元素就是a节点的祖先节点。 3.兄弟节点： 12345678910111213141516171819202122232425262728293031323334html = """&lt;html&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; Hello &lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and &lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt;"""from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print('Next Sibling', soup.a.next_sibling)print('Prev Sibling', soup.a.previous_sibling)print('Next Siblings', list(enumerate(soup.a.next_siblings)))print('Prev Siblings', list(enumerate(soup.a.previous_siblings)))#输出如下：Next Sibling Hello Prev Sibling Once upon a time there were three little sisters; and their names were Next Siblings [(0, '\n Hello\n '), (1, &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;), (2, ' \n and\n '), (3, &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;), (4, '\n and they lived at the bottom of a well.\n ')]Prev Siblings [(0, '\n Once upon a time there were three little sisters; and their names were\n ')] 可以看到，这里调用了4个属性，其中next_sibling和previous_sibling分别获取节点的下一个和上一个兄弟元素，next_siblings和previous_siblings则分别返回所有前面和后面的兄弟节点的生成器。 4.提取信息123456789101112131415161718192021222324252627282930313233html = """&lt;html&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;Bob&lt;/a&gt;&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; &lt;/p&gt;"""from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print('Next Sibling:')print(type(soup.a.next_sibling))print(soup.a.next_sibling)print(soup.a.next_sibling.string)print('Parent:')print(type(soup.a.parents))print(list(soup.a.parents)[0])print(list(soup.a.parents)[0].attrs['class'])#输出如下：Next Sibling:&lt;class 'bs4.element.Tag'&gt;&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;LacieParent:&lt;class 'generator'&gt;&lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Bob&lt;/a&gt;&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;&lt;/p&gt;['story'] 如果返回结果是单个节点，那么可以直接调用string、attrs等属性获得其文本和属性；如果返回结果是多个节点的生成器，则可以转为列表后取出某个元素，然后再调用string、attrs等属性获取其对应节点的文本和属性。 方法选择器前面所讲的选择方法都是通过属性来选择的，这种方法非常快，但是如果进行比较复杂的选择的话，它就比较烦琐，不够灵活了。幸好，Beautiful Soup还为我们提供了一些查询方法，比如find_all()和find()等，调用它们，然后传入相应的参数，就可以灵活查询了。 find_all()参数 : find_all(name , attrs , recursive , text , **kwargs) 1. name： 根据节点名来查询元素： 1234567891011121314151617181920212223242526272829303132333435html='''&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all(name='ul'))print(type(soup.find_all(name='ul')[0]))#输出如下：[&lt;ul class="list" id="list-1"&gt;&lt;li class="element"&gt;Foo&lt;/li&gt;&lt;li class="element"&gt;Bar&lt;/li&gt;&lt;li class="element"&gt;Jay&lt;/li&gt;&lt;/ul&gt;, &lt;ul class="list list-small" id="list-2"&gt;&lt;li class="element"&gt;Foo&lt;/li&gt;&lt;li class="element"&gt;Bar&lt;/li&gt;&lt;/ul&gt;]&lt;class 'bs4.element.Tag'&gt; 这里我们调用了find_all()方法，返回结果是列表类型，长度为2，每个元素依然都是bs4.element.Tag类型。 因为都是Tag类型，所以依然可以进行嵌套查询。还是同样的文本，这里查询出所有ul节点后，再继续查询其内部的li节点： 1234567for ul in soup.find_all(name='ul'): print(ul.find_all(name='li')) #输出如下：[&lt;li class="element"&gt;Foo&lt;/li&gt;, &lt;li class="element"&gt;Bar&lt;/li&gt;, &lt;li class="element"&gt;Jay&lt;/li&gt;][&lt;li class="element"&gt;Foo&lt;/li&gt;, &lt;li class="element"&gt;Bar&lt;/li&gt;] 返回结果是列表类型，列表中的每个元素依然还是Tag类型。 接下来，就可以遍历每个li，获取它的文本了： 1234567891011121314for ul in soup.find_all(name='ul'): print(ul.find_all(name='li')) for li in ul.find_all(name='li'): print(li.string) #输出如下：[&lt;li class="element"&gt;Foo&lt;/li&gt;, &lt;li class="element"&gt;Bar&lt;/li&gt;, &lt;li class="element"&gt;Jay&lt;/li&gt;]FooBarJay[&lt;li class="element"&gt;Foo&lt;/li&gt;, &lt;li class="element"&gt;Bar&lt;/li&gt;]FooBar 2. attr 除了根据节点名查询，我们也可以传入一些属性来查询： 123456789101112131415161718192021222324252627282930313233343536html='''&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1" name="elements"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all(attrs=&#123;'id': 'list-1'&#125;))print(soup.find_all(attrs=&#123;'name': 'elements'&#125;))# 输出如下：[&lt;ul class="list" id="list-1" name="elements"&gt;&lt;li class="element"&gt;Foo&lt;/li&gt;&lt;li class="element"&gt;Bar&lt;/li&gt;&lt;li class="element"&gt;Jay&lt;/li&gt;&lt;/ul&gt;][&lt;ul class="list" id="list-1" name="elements"&gt;&lt;li class="element"&gt;Foo&lt;/li&gt;&lt;li class="element"&gt;Bar&lt;/li&gt;&lt;li class="element"&gt;Jay&lt;/li&gt;&lt;/ul&gt;] 这里查询的时候传入的是attrs参数，参数的类型是字典类型。比如，要查询id为list-1的节点，可以传入attrs={&#39;id&#39;: &#39;list-1&#39;}的查询条件，得到的结果是列表形式，包含的内容就是符合id为list-1的所有节点。在上面的例子中，符合条件的元素个数是1，所以结果是长度为1的列表。 对于一些常用的属性，比如id和class等，我们可以不用attrs来传递。比如，要查询id为list-1的节点，可以直接传入id这个参数。还是上面的文本，我们换一种方式来查询： 1234567891011121314from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all(id='list-1'))print(soup.find_all(class_='element'))#输出如下：[&lt;ul class="list" id="list-1"&gt;&lt;li class="element"&gt;Foo&lt;/li&gt;&lt;li class="element"&gt;Bar&lt;/li&gt;&lt;li class="element"&gt;Jay&lt;/li&gt;&lt;/ul&gt;][&lt;li class="element"&gt;Foo&lt;/li&gt;, &lt;li class="element"&gt;Bar&lt;/li&gt;, &lt;li class="element"&gt;Jay&lt;/li&gt;, &lt;li class="element"&gt;Foo&lt;/li&gt;, &lt;li class="element"&gt;Bar&lt;/li&gt;] 这里直接传入id=&#39;list-1&#39;，就可以查询id为list-1的节点元素了。而对于class来说，由于class在Python里是一个关键字，所以后面需要加一个下划线，即class_=&#39;element&#39;，返回的结果依然还是Tag组成的列表。 3. text text参数可用来匹配节点的文本，传入的形式可以是字符串，可以是正则表达式对象： 1234567891011121314151617import rehtml='''&lt;div class="panel"&gt; &lt;div class="panel-body"&gt; &lt;a&gt;Hello, this is a link&lt;/a&gt; &lt;a&gt;Hello, this is a link, too&lt;/a&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find_all(text=re.compile('link')))#输出如下：['Hello, this is a link', 'Hello, this is a link, too'] 这里有两个a节点，其内部包含文本信息。这里在find_all()方法中传入text参数，该参数为正则表达式对象，结果返回所有匹配正则表达式的节点文本组成的列表。 find( )find()方法返回的是单个元素，也就是第一个匹配的元素，而find_all()返回的是所有匹配的元素组成的列表： 123456789101112131415161718192021222324252627282930313233343536373839html='''&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.find(name='ul'))print(type(soup.find(name='ul')))print(soup.find(class_='list'))#输出如下：&lt;ul class="list" id="list-1"&gt;&lt;li class="element"&gt;Foo&lt;/li&gt;&lt;li class="element"&gt;Bar&lt;/li&gt;&lt;li class="element"&gt;Jay&lt;/li&gt;&lt;/ul&gt;&lt;class 'bs4.element.Tag'&gt;&lt;ul class="list" id="list-1"&gt;&lt;li class="element"&gt;Foo&lt;/li&gt;&lt;li class="element"&gt;Bar&lt;/li&gt;&lt;li class="element"&gt;Jay&lt;/li&gt;&lt;/ul&gt; 这里的返回结果不再是列表形式，而是第一个匹配的节点元素，类型依然是Tag类型。 另外，还有许多查询方法，其用法与前面介绍的find_all()、find()方法完全相同，只不过查询范围不同： find_parents()和find_parent()：前者返回所有祖先节点，后者返回直接父节点。 find_next_siblings()和find_next_sibling()：前者返回后面所有的兄弟节点，后者返回后面第一个兄弟节点。 find_previous_siblings()和find_previous_sibling()：前者返回前面所有的兄弟节点，后者返回前面第一个兄弟节点。 find_all_next()和find_next()：前者返回节点后所有符合条件的节点，后者返回第一个符合条件的节点。 find_all_previous()和find_previous()：前者返回节点后所有符合条件的节点，后者返回第一个符合条件的节点。 CSS选择器使用CSS选择器时，只需要调用select()方法，传入相应的CSS选择器即可： 1234567891011121314151617181920212223242526272829303132333435html='''&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;'''from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')print(soup.select('.panel .panel-heading'))print(soup.select('ul li'))print(soup.select('#list-2 .element'))print(type(soup.select('ul')[0]))# 输出如下：[&lt;div class="panel-heading"&gt;&lt;h4&gt;Hello&lt;/h4&gt;&lt;/div&gt;][&lt;li class="element"&gt;Foo&lt;/li&gt;, &lt;li class="element"&gt;Bar&lt;/li&gt;, &lt;li class="element"&gt;Jay&lt;/li&gt;, &lt;li class="element"&gt;Foo&lt;/li&gt;, &lt;li class="element"&gt;Bar&lt;/li&gt;][&lt;li class="element"&gt;Foo&lt;/li&gt;, &lt;li class="element"&gt;Bar&lt;/li&gt;]&lt;class 'bs4.element.Tag'&gt; 这里我们用了3次CSS选择器，返回的结果均是符合CSS选择器的节点组成的列表。例如，select(&#39;ul li&#39;)则是选择所有ul节点下面的所有li节点，结果便是所有的li节点组成的列表。 最后一句打印输出了列表中元素的类型。可以看到，类型依然是Tag类型。 select()方法同样支持嵌套选择。 获取属性尝试获取每个ul节点的id属性： 123456789101112from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')for ul in soup.select('ul'): print(ul['id']) print(ul.attrs['id']) #输出如下：list-1list-1list-2list-2 获取文本要获取文本，可以用前面所讲的string属性。此外还有一个方法get_text()： 12345678910111213141516171819from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'lxml')for li in soup.select('li'): print('Get Text:', li.get_text()) print('String:', li.string) # 输出如下：Get Text: FooString: FooGet Text: BarString: BarGet Text: JayString: JayGet Text: FooString: FooGet Text: BarString: Bar BeautifulSoup总结 推荐使用lxml解析库，必要时使用html.parser。 节点选择筛选功能弱但是速度快。 建议使用find()或者find_all()查询匹配单个结果或者多个结果。 如果对CSS选择器熟悉的话，可以使用select()方法选择。 更多知识请查看BeautifulSoup中文文档]]></content>
      <categories>
        <category>Spider</category>
        <category>解析</category>
      </categories>
      <tags>
        <tag>解析</tag>
        <tag>css选择器</tag>
        <tag>BeautifulSoup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyquery的使用]]></title>
    <url>%2F2018%2F12%2F14%2F%E8%A7%A3%E6%9E%90%2Fpyquery%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[简介PyQuery 库也是一个非常强大又灵活的网页解析库，语法与 jQuery 几乎完全相同，所以不用再去费心去记一些奇怪的方法了。 以下介绍使用方法 初始化它的初始化方式有多种，比如直接传入字符串，传入 URL，传入文件名，等等。 字符串初始化1234567891011121314151617181920212223html = '''&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)print(doc('li'))#输出如下：&lt;li class="item-0"&gt;first item&lt;/li&gt;&lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; url 初始化12345678from pyquery import PyQuery as pq#需要指定参数为 urldoc = pq(url='http://baidu.com')print(doc('title'))# 输出如下：&lt;title&gt;百度一下，你就知道&lt;/title&gt; 这样的话，PyQuery对象会首先请求这个 URL，然后用得到的 HTML 内容完成初始化，这其实就相当于用网页的源代码以字符串的形式传递给PyQuery类来初始化。 它与下面的功能是相同的： 1234from pyquery import PyQuery as pqimport requestsdoc = pq(requests.get('http://cuiqingcai.com').text)print(doc('title')) 文件初始化1234from pyquery import PyQuery as pq# 指定参数为 filenamedoc = pq(filename='demo.html')print(doc('li')) 基本 CSS 选择器用一个实例来感受 pyquery 的 CSS 选择器的用法： 123456789101112131415161718192021222324html = '''&lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)print(doc('#container .list li'))print(type(doc('#container .list li')))#输出如下:&lt;li class="item-0"&gt;first item&lt;/li&gt;&lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;class 'pyquery.pyquery.PyQuery'&gt; 查找节点子节点查找子节点时，需要用到find()方法，此时传入的参数是 CSS 选择器。这里还是以前面的 HTML 为例： 1234567891011121314151617181920212223242526from pyquery import PyQuery as pqdoc = pq(html)items = doc('.list')print(type(items))print(items)lis = items.find('li')print(type(lis))print(lis)# 输出结果：&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;li class="item-0"&gt;first item&lt;/li&gt;&lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; find()的查找范围是节点的所有子孙节点，而如果我们只想查找子节点，那么可以用children()方法： 123456789101112lis = items.children()print(type(lis))print(lis)#输出结果：&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;li class="item-0"&gt;first item&lt;/li&gt;&lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; 如果要筛选所有子节点中符合条件的节点，比如想筛选出子节点中class为active的节点，可以向children()方法传入CSS选择器.active： 1234567lis = items.children('.active')print(lis)#输出结果：&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; 父节点可以用parent()方法来获取某个节点的直接父节点： 123456789101112131415161718192021222324252627282930313233html = '''&lt;div class="wrap"&gt; &lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)items = doc('.list')container = items.parent()print(type(container))print(container)#输出结果：&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; 可以用parents()方法来获取某个节点的祖先节点： 12345678910111213141516171819202122232425262728293031from pyquery import PyQuery as pqdoc = pq(html)items = doc('.list')parents = items.parents()print(type(parents))print(parents)# 输出结果：&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;div class="wrap"&gt; &lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; 如果想要筛选某个祖先节点的话，可以向parents()方法传入 CSS 选择器，这样就会返回祖先节点中符合 CSS 选择器的节点： 123456789101112131415161718parent = items.parents('.wrap')print(parent)# 输出结果：# 输出结果少了一个节点，只保留了class为wrap的节点。&lt;div class="wrap"&gt; &lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; 兄弟节点如果要获取兄弟节点，可以使用siblings()方法： 123456789101112from pyquery import PyQuery as pqdoc = pq(html)li = doc('.list .item-0.active')print(li.siblings())# 输出结果：&lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0"&gt;first item&lt;/li&gt;&lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; 这里首先选择class为list的节点内部class为item-0和active的节点，也就是第三个li节点。那么，很明显，它的兄弟节点有4个，那就是第一、二、四、五个li节点。 如果要筛选某个兄弟节点，我们依然可以向siblings方法传入 CSS 选择器，这样就会从所有兄弟节点中挑选出符合条件的节点了： 12345678from pyquery import PyQuery as pqdoc = pq(html)li = doc('.list .item-0.active')print(li.siblings('.active'))# 输出结果：&lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; 遍历pyquery 的选择结果可能是多个节点，也可能是单个节点，类型都是PyQuery类型，并没有返回像 Beautiful Soup 那样的列表。 对于单个节点来说，可以直接打印输出，也可以直接转成字符串： 12345678910from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li)print(str(li))# 输出如下：&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; 对于多个节点的结果需要遍历来获取。例如，这里把每一个li节点进行遍历，需要调用items()方法： 1234567891011121314151617181920from pyquery import PyQuery as pqdoc = pq(html)lis = doc('li').items()print(type(lis))for li in lis: print(li, type(li)) # 输出如下：&lt;class 'generator'&gt;&lt;li class="item-0"&gt;first item&lt;/li&gt;&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;class 'pyquery.pyquery.PyQuery'&gt;&lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;class 'pyquery.pyquery.PyQuery'&gt; 调用items()方法后，会得到一个生成器，遍历一下，就可以逐个得到li节点对象了，它的类型也是PyQuery类型。 获取信息获取属性提取到某个PyQuery类型的节点后，就可以调用attr()方法来获取属性： 1234567891011121314151617181920212223242526html = '''&lt;div class="wrap"&gt; &lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)a = doc('.item-0.active a')print(a, type(a))print(a.attr('href'))print(a.attr.href)# 输出如下：&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt; &lt;class 'pyquery.pyquery.PyQuery'&gt;link3.htmllink3.html 这里选中了一个元素，调用attr()方法和调用attr属性得到的结果是一样的。但在选中多个属性时它们只会返回第一个结果。我们可以通过遍历把他们提取出来： 123456789101112from pyquery import PyQuery as pqdoc = pq(html)a = doc('a')for item in a.items(): print(item.attr('href')) # 输出如下：link2.htmllink3.htmllink4.htmllink5.html 因此，在进行属性获取时，可以观察返回节点是一个还是多个，如果是多个，则需要遍历才能依次获取每个节点的属性。 获取文本获取文本可以调用text()方法来实现： 123456789101112131415161718192021222324html = '''&lt;div class="wrap"&gt; &lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)a = doc('.item-0.active a')print(a)print(a.text())# 输出结果：&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;third item 这里首先选中一个a节点，然后调用text()方法，就可以获取其内部的文本信息。此时它会忽略掉节点内部包含的所有HTML，只返回纯文字内容。 但如果想要获取这个节点内部的HTML文本，就要用html()方法了： 123456789from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li)print(li.html())# 输出如下：&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt; 当我们选中多个节点时，text( )返回该节点内部所有文本并合并成一个字符串，而html( )只会返回第一个，若要访问所有内容则需要遍历。 节点操作pyquery 提供了很多方法来对节点进行动态修改，比如为某个节点添加一个class，移除某个节点等。下面举几个典型的例子来说明、 addClass 和 removeClass 12345678910111213141516171819202122232425262728html = '''&lt;div class="wrap"&gt; &lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li)li.removeClass('active')print(li)li.addClass('active')print(li)# 输出如下：&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; 这里首先选中了第三个li节点，然后调用removeClass()方法，将li节点的active这个class移除，后来又调用addClass()方法，将class添加回来。所以说，addClass()和removeClass()这些方法可以动态改变节点的class属性。 attr 、text 和 html 1234567891011121314151617181920212223html = '''&lt;ul class="list"&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('.item-0.active')print(li)li.attr('name', 'link')print(li)li.text('changed item')print(li)li.html('&lt;span&gt;changed item&lt;/span&gt;')print(li)# 输出结果：&lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0 active" name="link"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li class="item-0 active" name="link"&gt;changed item&lt;/li&gt;&lt;li class="item-0 active" name="link"&gt;&lt;span&gt;changed item&lt;/span&gt;&lt;/li&gt; 这里我们首先选中li节点，然后调用attr()方法来修改属性，其中该方法的第一个参数为属性名，第二个参数为属性值。接着，调用text()和html()方法来改变节点内部的内容。 可以发现，调用attr()方法后，li节点多了一个原本不存在的属性name，其值为link。接着调用text()方法，传入文本之后，li节点内部的文本全被改为传入的字符串文本了。最后，调用html()方法传入HTML文本后，li节点内部又变为传入的 HTML 文本了。 所以说，如果attr()方法只传入第一个参数的属性名，则是获取这个属性值；如果传入第二个参数，可以用来修改属性值。text()和html()方法如果不传参数，则是获取节点内纯文本和HTML文本；如果传入参数，则进行赋值。 remove( ) 123456789101112131415161718html = '''&lt;div class="wrap"&gt; Hello, World &lt;p&gt;This is a paragraph.&lt;/p&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)wrap = doc('.wrap')print(wrap.text())wrap.find('p').remove()print(wrap.text())# 输出如下：Hello, World This is a paragraph.Hello, World 提取Hello, World这个字符串，而不要p节点内部的字符串，首先选中p节点，然后调用了remove()方法将其移除，然后这时wrap内部就只剩下Hello, World这句话了，然后再利用text()方法提取即可。 更多节点操作的方法可以参考官方文档：http://pyquery.readthedocs.io/en/latest/api.html 伪类选择器CSS 选择器之所以强大，还有一个很重要的原因，那就是它支持多种多样的伪类选择器，例如选择第一个节点、最后一个节点、奇偶数节点、包含某一文本的节点等。示例如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546html = '''&lt;div class="wrap"&gt; &lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''from pyquery import PyQuery as pqdoc = pq(html)li = doc('li:first-child')print(li)li = doc('li:last-child')print(li)li = doc('li:nth-child(2)')print(li)li = doc('li:gt(2)')print(li)li = doc('li:nth-child(2n)')print(li)li = doc('li:contains(second)')print(li)# 输出如下：&lt;li class="item-0"&gt;first item&lt;/li&gt;&lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;&lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; 这里我们使用了 CSS3 的伪类选择器，依次选择了第一个li节点、最后一个li节点、第二个li节点、第三个li之后的li节点（序号比2大的节点，序号从0开始）、偶数位置的li节点、包含second文本的li节点。 关于 CSS 选择器的更多用法，可以参考http://www.w3school.com.cn/css/index.asp。 到此为止，pyquery的常用用法就介绍完了。如果想查看更多的内容，可以参考pyquery的官方文档：http://pyquery.readthedocs.io]]></content>
      <categories>
        <category>Spider</category>
        <category>解析</category>
      </categories>
      <tags>
        <tag>pyquery</tag>
        <tag>css选择器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XPath lxml的基本用法]]></title>
    <url>%2F2018%2F12%2F14%2F%E8%A7%A3%E6%9E%90%2FXPath%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[简介XPath，全称 XML Path Language，即 XML 路径语言 可在 XML 中查找信息 支持 HTML 的查找 通过元素和属性进行导航 xml 文档（html 属于 xml）是由一系列节点构成的树，例如：12345678&lt;html&gt; &lt;body&gt; &lt;div &gt; &lt;p&gt;Hello world&lt;p&gt; &lt;a href="/home"&gt;Click here&lt;/a&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 在 python 中通过 lxml 库利用XPath对HTML进行解析 XPath 辅助工具： Chrome插件 ：Xpath Helper 打开/关闭 ：Ctrl + Shift + x Firefox插件 ：Xpath checker Xpath表达式编辑工具 ：XML Quire XPath 常用规则 表达式 描述 nodename 选取此节点的所有子节点 / 从当前节点中选区直接子节点 // 从当前节点中选取子孙节点 . 选取当前节点 .. 选取当前节点的父节点 @ 选取属性 * 选取所有元素子节点 @* 选区所有属性节点 @ATTR 选取名为ATTR的属性节点 text（） 选区所有文本子节点 XPath 轴 轴名称 表达式 描述 ancestor xpath(‘./ancestor::*’) 选取当前节点的所有先辈节点（父、祖父） ancestor-or-self xpath(‘./ancestor-or-self::*’) 选取当前节点的所有先辈节点以及节点本身 attribute xpath(‘./attribute::*’) 选取当前节点的所有属性 child xpath(‘./child::*’) 返回当前节点的所有子节点 descendant xpath(‘./descendant::*’) 返回当前节点的所有后代节点（子节点、孙节点） following xpath(‘./following::*’) 选取文档中当前节点结束标签后的所有节点 following-sibing xpath(‘./following-sibing::*’) 选取当前节点之后的兄弟节点 parent xpath(‘./parent::*’) 选取当前节点的父节点 preceding xpath(‘./preceding::*’) 选取文档中当前节点开始标签前的所有节点 preceding-sibling xpath(‘./preceding-sibling::*’) 选取当前节点之前的兄弟节点 self xpath(‘./self::*’) 选取当前节点 XPath 功能函数 table th:nth-of-type(1) { width: 100px; } 函数 用法用法 解释 starts-with xpath(‘//div[starts-with(@id,”ma”)]‘) 选取id值以ma开头的div节点 contains xpath(‘//div[contains(@id,”ma”)]‘) 选取id值包含ma的div节点 and xpath(‘//div[contains(@id,”ma”) and contains(@id,”in”)]‘) 选取id值包含ma和in的div节点 text() xpath(‘//div[contains(text(),”ma”)]‘) 选取节点文本包含ma的div节点 使用示例tostring（）方法：123456789101112131415from lxml import etreetext = '''&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt; &lt;/ul&gt; &lt;/div&gt;'''html = etree.HTML(text)result = etree.tostring(html)print(result.decode('utf-8')) 首先导入 lxml 的 etree 模块，然后进行初始化，构造解析对象。tostring()方法输出结果是bytes类型，这里利用 decode( ) 方法将其转成 str 类型。经过处理之后，li节点标签被补全，并且还自动添加了body、html节点。结果如下： 12345678910&lt;html&gt;&lt;body&gt;&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt; &lt;/li&gt;&lt;/ul&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 也可直接读取文本文件进行解析： 123456from lxml import etree# test.html的内容就是上面例子中的HTML代码html = etree.parse('./test.html', etree.HTMLParser())result = etree.tostring(html)print(result.decode('utf-8')) 输出结果略有不同，多了一个DOCTYPE的声明，不过对解析无任何影响，结果如下： 12345678910&lt;!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"&gt;&lt;html&gt;&lt;body&gt;&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt; &lt;/li&gt;&lt;/ul&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 用//访问所有节点 使用*代表匹配所有节点，返回形式是一个列表，每个元素是Element类型，其后跟了节点的名称。 12345678from lxml import etreehtml = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//*')print(result)# 结果如下：[&lt;Element html at 0x10510d9c8&gt;, &lt;Element body at 0x10510da08&gt;, &lt;Element div at 0x10510da48&gt;, &lt;Element ul at 0x10510da88&gt;, &lt;Element li at 0x10510dac8&gt;, &lt;Element a at 0x10510db48&gt;, &lt;Element li at 0x10510db88&gt;, &lt;Element a at 0x10510dbc8&gt;, &lt;Element li at 0x10510dc08&gt;, &lt;Element a at 0x10510db08&gt;, &lt;Element li at 0x10510dc48&gt;, &lt;Element a at 0x10510dc88&gt;, &lt;Element li at 0x10510dcc8&gt;, &lt;Element a at 0x10510dd08&gt;] 选取所有li节点，可以使用//，然后直接加上节点名称即可 12345678910from lxml import etreehtml = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//li')print(result)print(result[0])# 结果如下：[&lt;Element li at 0x105849208&gt;, &lt;Element li at 0x105849248&gt;, &lt;Element li at 0x105849288&gt;, &lt;Element li at 0x1058492c8&gt;, &lt;Element li at 0x105849308&gt;]&lt;Element li at 0x105849208&gt; 通过/或//查找元素的子节点或子孙节点 选择li节点的所有直接a子节点： 123456789from lxml import etree html = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//li/a')print(result)# 结果如下:[&lt;Element a at 0x106ee8688&gt;, &lt;Element a at 0x106ee86c8&gt;, &lt;Element a at 0x106ee8708&gt;, &lt;Element a at 0x106ee8748&gt;, &lt;Element a at 0x106ee8788&gt;] 获取所有子孙节点，就可以使用// 12345678from lxml import etree html = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//ul//a')print(result)# 输出结果与上面相同 用..来查找父节点 选中href属性为link4.html的a节点，然后再获取其父节点，然后再获取其class属性： 123456789from lxml import etree html = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//a[@href="link4.html"]/../@class')print(result)# 结果如下：['item-1'] 也可以通过parent::来获取父节点： 12345678from lxml import etree html = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//a[@href="link4.html"]/parent::*/@class')print(result)# 结果与上面相同 用@符号进行属性匹配 通过加入[@class=&quot;item-0&quot;]，选取 class 为 item-0 的 li 节点 12345678from lxml import etreehtml = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//li[@class="item-0"]')print(result)# 结果如下：[&lt;Element li at 0x10a399288&gt;, &lt;Element li at 0x10a3992c8&gt;] 用text()方法获取节点中的文本获取前面li节点中的文本有两种方法 1.先选取a节点再获取文本： 123456789from lxml import etree html = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//li[@class="item-0"]/a/text()')print(result)# 结果如下：['first item', 'fifth item'] 2.使用//选取文本： 123456789from lxml import etree html = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//li[@class="item-0"]//text()')print(result)# 结果如下：['first item', 'fifth item', '\n '] 这里的返回结果是3个，其中前两个是li的子节点a节点内部的文本，另外一个是最后一个li节点内部的文本，即换行符。 如果要想获取子孙节点内部的所有文本，可以直接用//加text()的方式，这样可以保证获取到最全面的文本信息，但是可能会夹杂一些换行符等特殊字符。如果想获取某些特定子孙节点下的所有文本，可以先选取到特定的子孙节点，然后再调用text()方法获取其内部文本，这样可以保证获取的结果是整洁的。 用@进行属性获取 获取所有li节点下所有a节点的href属性，返回列表： 123456789from lxml import etree html = etree.parse('./test.html', etree.HTMLParser())result = html.xpath('//li/a/@href')print(result)# 结果如下:['link1.html', 'link2.html', 'link3.html', 'link4.html', 'link5.html'] 这里我们通过@href即可获取节点的href属性。注意，此处和属性匹配的方法不同，属性匹配是中括号加属性名和值来限定某个属性，如[@href=&quot;link1.html&quot;]，而此处的@href指的是获取节点的某个属性 用contains()函数进行属性多值匹配 HTML 文本中li节点的class属性有两个值li和li-first，这时就需要用contains()函数： 1234567891011from lxml import etreetext = '''&lt;li class="li li-first"&gt;&lt;a href="link.html"&gt;first item&lt;/a&gt;&lt;/li&gt;'''html = etree.HTML(text)result = html.xpath('//li[contains(@class, "li")]/a/text()')print(result)# 结果如下:['first item'] 使用运算符and进行多属性匹配 根据多个属性确定一个节点时，需要同时匹配多个属性，此时使用and连接： 1234567891011from lxml import etreetext = '''&lt;li class="li li-first" name="item"&gt;&lt;a href="link.html"&gt;first item&lt;/a&gt;&lt;/li&gt;'''html = etree.HTML(text)result = html.xpath('//li[contains(@class, "li") and @name="item"]/a/text()')print(result)# 结果如下：['first item'] 传入索引按序选择 某些属性同时匹配了多个节点时，利用中括号传入索引来获取我们需要的某个节点： ​ 123456789101112131415161718192021222324252627282930313233from lxml import etree text = '''&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt; &lt;/ul&gt; &lt;/div&gt;'''html = etree.HTML(text)# 这里的序号是以1开头的，不是以0开头result = html.xpath('//li[1]/a/text()')print(result)# 中括号中传入last()返回最后一个节点result = html.xpath('//li[last()]/a/text()')print(result)# 选取了位置小于3的li节点，也就是位置序号为1和2的节点result = html.xpath('//li[position()&lt;3]/a/text()')print(result)# 我们选取了倒数第三个li节点，中括号中传入last()-2即可result = html.xpath('//li[last()-2]/a/text()')print(result)# 结果如下：['first item']['fifth item']['first item', 'second item']['third item'] 在XPath中，提供了100多个函数，包括存取、数值、字符串、逻辑、节点、序列等处理功能，它们的具体作用可以参考：http://www.w3school.com.cn/xpath/xpath_functions.asp。 节点轴选择 XPath提供了很多节点轴选择方法，包括获取子元素、兄弟元素、父元素、祖先元素等： 123456789101112131415161718192021222324252627282930313233343536373839from lxml import etree text = '''&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;&lt;span&gt;first item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt; &lt;/ul&gt; &lt;/div&gt;'''html = etree.HTML(text)result = html.xpath('//li[1]/ancestor::*')print(result)result = html.xpath('//li[1]/ancestor::div')print(result)result = html.xpath('//li[1]/attribute::*')print(result)result = html.xpath('//li[1]/child::a[@href="link1.html"]')print(result)result = html.xpath('//li[1]/descendant::span')print(result)result = html.xpath('//li[1]/following::*[2]')print(result)result = html.xpath('//li[1]/following-sibling::*')&lt;code class="lang-python"&gt;&lt;span class="kwd"&gt;print&lt;/span&gt;&lt;span class="pun"&gt;(&lt;/span&gt;&lt;span class="pln"&gt;result&lt;/span&gt;&lt;span class="pun"&gt;)&lt;/span&gt;# 结果如下：[&lt;Element html at 0x107941808&gt;, &lt;Element body at 0x1079418c8&gt;, &lt;Element div at 0x107941908&gt;, &lt;Element ul at 0x107941948&gt;][&lt;Element div at 0x107941908&gt;]['item-0'][&lt;Element a at 0x1079418c8&gt;][&lt;Element span at 0x107941948&gt;][&lt;Element a at 0x1079418c8&gt;][&lt;Element li at 0x107941948&gt;, &lt;Element li at 0x107941988&gt;, &lt;Element li at 0x1079419c8&gt;, &lt;Element li at 0x107941a08&gt;] 第一次选择时，我们调用了ancestor轴，可以获取所有祖先节点。其后需要跟两个冒号，然后是节点的选择器，这里我们直接使用*，表示匹配所有节点，因此返回结果是第一个li节点的所有祖先节点，包括html、body、div和ul。 第二次选择时，我们又加了限定条件，这次在冒号后面加了div，这样得到的结果就只有div这个祖先节点了。 第三次选择时，我们调用了attribute轴，可以获取所有属性值，其后跟的选择器还是*，这代表获取节点的所有属性，返回值就是li节点的所有属性值。 第四次选择时，我们调用了child轴，可以获取所有直接子节点。这里我们又加了限定条件，选取href属性为link1.html的a节点。 第五次选择时，我们调用了descendant轴，可以获取所有子孙节点。这里我们又加了限定条件获取span节点，所以返回的结果只包含span节点而不包含a节点。 第六次选择时，我们调用了following轴，可以获取当前节点之后的所有节点。这里我们虽然使用的是*匹配，但又加了索引选择，所以只获取了第二个后续节点。 第七次选择时，我们调用了following-sibling轴，可以获取当前节点之后的所有同级节点。这里我们使用*匹配，所以获取了所有后续同级节点。 以上是XPath轴的简单用法，更多轴的用法可以参考：http://www.w3school.com.cn/xpath/xpath_axes.asp。 如果想查询更多XPath的用法，可以查看：http://www.w3school.com.cn/xpath/index.asp。 如果想查询更多Python lxml库的用法，可以查看http://lxml.de/。]]></content>
      <categories>
        <category>Spider</category>
        <category>解析</category>
      </categories>
      <tags>
        <tag>解析</tag>
        <tag>XPath</tag>
        <tag>lxml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2F2018%2F12%2F13%2F%E8%A7%A3%E6%9E%90%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[定义 正则表达式是文本的高级匹配模式，提供搜索，替代，查找等功能。本质是由一系列特殊符号和字符组成的字串。 特点 方便进行检索修改文本的操作 支持编程语言众多 使用灵活多样 常用的匹配规则 table th:first-of-type { width: 100px; } 模式 描述 \w 匹配数字、字母及下划线 \W 匹配不是字母、数字及下划线的内容 \s 匹配任意空白字符，等价于[\t\n\r\f] \S 匹配任意非空字符 \d 匹配任意数字，等价于[0-9] \D 匹配任意非数字的字符 \A 匹配字符串开头 \Z 匹配字符串结尾，如果是存在换行，只匹配到换行前的结束字符串 \z 匹配字符串结尾，如果存在换行，同时还会匹配换行符 \G 匹配最后匹配完成的位置 \n 匹配一个换行符 \t 匹配一个制表符 ^ 匹配一个字符串的开头 $ 匹配一个字符串的结尾 . 匹配除换行符\n外任意字符 [...] 用来表示一组字符,单独列出：[amk] 匹配 a，m或k [^...] 不在[]中的字符：[^abc] 匹配除了a,b,c之外的字符 * 匹配0个或多个表达式 + 匹配一个或多个表达式 ? 匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式 {n} 匹配n个前面表达式。例如，o{2}不能匹配Bob中的o，但是能匹配food中的两个o。 {m,n} 匹配 m 到 n 次由前面的正则表达式定义的片段，贪婪方式 &nbsp;&#124; 匹配&nbsp;&#124;&nbsp;两边任意一个字符 ( ) 匹配括号内的表达式，也表示一个组 a b c &amp; # 匹配字符本身 python中的re模块提供了整个正则表达式的实现，利用这个库可以在python中使用正则表达式 re.match函数 re.match 尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回none。 函数语法： 1234re.match(pattern, string, flags=0)#pattern：匹配的正则表达式#string：要匹配的字符串#flags：标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等 我们可以使用 group(num) 或 groups() 匹配对象函数来获取匹配表达式。 示例:12345678import re content = 'Hello 123 4567 World_This is a Regex Demo'print(len(content))result = re.match('^Hello\s\d\d\d\s\d&#123;4&#125;\s\w&#123;10&#125;', content)print(result)print(result.group())print(result.span()) 运行结果：123441&lt;_sre.SRE_Match object; span=(0, 25), match='Hello 123 4567 World_This'&gt;Hello 123 4567 World_This(0, 25) 打印输出结果，可以看到结果是 SRE_Match 对象，这证明成功匹配。该对象有两个方法：group()方法可以输出匹配到的内容，结果是 Hello 123 4567 World_This，这恰好是正则表达式规则所匹配的内容；span()方法可以输出匹配的范围，结果是(0, 25)，这就是匹配到的结果字符串在原字符串中的位置范围。 匹配目标 用()将需要提取的字符串括起来即可 通用匹配 用.和*组合在一起可以匹配除换行符外的所有字符 贪婪与非贪婪 贪婪模式：正则表达式的重复默认总是尽可能多的向后匹配内容 ? {m,n} 非贪婪模式 ： 尽可能少的匹配内容 *? +? ?? {m,n}? 示例： 12re.findall(r'ab*?',"abbbbbcde")re.findall(r'ab+?',"abbbbbcde") 输出结果：12aab 在做匹配的时候，字符串中间尽量使用非贪婪匹配，也就是用.*?来代替.*，以免出现匹配结果缺失的情况。但需要注意，如果匹配的结果在字符串结尾，.*?就有可能匹配不到任何内容了，因为它会匹配尽可能少的字符。 修饰符re.S 这个修饰符的作用是使.匹配包括换行符在内的所有字符。 其他修饰符： table th:first-of-type { width: 100px; } 修饰符 描述 re.I 使匹配对大小写不敏感 re.L 做本地化识别（locale-aware）匹配 re.M 多行匹配，影响 ^ 和 $ re.U 根据 Unicode 字符集解析字符。这个标志影响 \w, \W, \b, \B. re.X 该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解。 转义匹配 当re模式里的字符与要匹配的目标字符串重复时，用反斜线\进行转义 re.search 函数 match()方法是从字符串的开头开始匹配的，一旦开头不匹配，那么整个匹配就失败了。 search()方法在匹配时会扫描整个字符串，然后返回第一个成功匹配的结果。也就是说，正则表达式可以是字符串的一部分，在匹配时，search()方法会依次扫描字符串，直到找到第一个符合规则的字符串，然后返回匹配内容，如果搜索完了还没有找到，就返回None。 函数语法: 12re.search(pattern, string, flags=0)#参数同match()方法 re.findall 方法 在字符串中找到正则表达式所匹配的所有子串，并返回一个列表，如果没有找到匹配的，则返回空列表。 match 和 search 是匹配一次，findall 是匹配所有。 函数语法: 12re.findall(pattern, string, flags=0)#参数同match()方法 re.sub 方法 用于替换字符串中的匹配项 函数语法： 12345re.sub(pattern, repl, string, count=0)#pattern : 正则中的模式字符串。#repl : 替换的字符串，也可为一个函数。#string : 要被查找替换的原始字符串。#count : 模式匹配后替换的最大次数，默认 0 表示替换所有的匹配。 re.compile方法 compile 函数用于编译正则表达式，生成一个正则表达式（Pattern）对象 函数语法： 123re.compile(pattern[, flags])#pattern: 一个字符串形式的正则表达式#flags: 修饰符，可选，表示匹配模式，比如忽略大小写，多行模式等 re.finditer 方法 和 findall 类似，在字符串中找到正则表达式所匹配的所有子串，并把它们作为一个迭代器返回 函数语法: 1re.finditer(pattern, string, flags=0) 示例：12345import re it = re.finditer(r"\d+","12a32bc43jf3") for match in it: print (match.group() ) 输出：123412 32 43 3 re.split 方法 split 方法按照能够匹配的子串将字符串分割后返回列表 函数语法:12re.split(pattern, string[, maxsplit=0, flags=0])#maxsplit:分隔次数，maxsplit=1 分隔一次，默认为 0，不限制次数。]]></content>
      <categories>
        <category>Spider</category>
        <category>解析</category>
      </categories>
      <tags>
        <tag>re</tag>
        <tag>正则</tag>
        <tag>解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP响应状态码]]></title>
    <url>%2F2018%2F12%2F11%2F%E7%BD%91%E7%BB%9C%2FHTTP%E5%93%8D%E5%BA%94%E7%8A%B6%E6%80%81%E7%A0%81%2F</url>
    <content type="text"><![CDATA[常见HTTP状态码 HTTP状态码 当浏览者访问一个网页时，浏览者的浏览器会向网页所在服务器发出请求。当浏览器接收并显示网页前，此网页所在的服务器会返回一个包含HTTP状态码的信息头（server header）用以响应浏览器的请求。 HTTP状态码的英文为HTTP Status Code。 下面是常见的HTTP状态码： 200 - 请求成功 301 - 资源（网页等）被永久转移到其它URL 404 - 请求的资源（网页等）不存在 500 - 内部服务器错误 HTTP状态码分类 分类 分类描述 1** 信息，服务器收到请求，需要请求者继续执行操作 2** 成功，操作被成功接收并处理 3** 重定向，需要进一步的操作以完成请求 4** 客户端错误，请求包含语法错误或无法完成请求 5** 服务器错误，服务器在处理请求的过程中发生了错误 HTTP状态码列表 状态码 状态码英文名称 中文描述 100 Continue 继续。客户端应继续其请求 101 Switching Protocols 切换协议。服务器根据客户端的请求切换协议。只能切换到更高级的协议，例如，切换到HTTP的新版本协议 200 OK 请求成功。一般用于GET与POST请求 201 Created 已创建。成功请求并创建了新的资源 202 Accepted 已接受。已经接受请求，但未处理完成 203 Non-Authoritative Information 非授权信息。请求成功。但返回的meta信息不在原始的服务器，而是一个副本 204 No Content 无内容。服务器成功处理，但未返回内容。在未更新网页的情况下，可确保浏览器继续显示当前文档 205 Reset Content 重置内容。服务器处理成功，用户终端（例如：浏览器）应重置文档视图。可通过此返回码清除浏览器的表单域 206 Partial Content 部分内容。服务器成功处理了部分GET请求 300 Multiple Choices 多种选择。请求的资源可包括多个位置，相应可返回一个资源特征与地址的列表用于用户终端（例如：浏览器）选择 301 Moved Permanently 永久移动。请求的资源已被永久的移动到新URI，返回信息会包括新的URI，浏览器会自动定向到新URI。今后任何新的请求都应使用新的URI代替 302 Found 临时移动。与301类似。但资源只是临时被移动。客户端应继续使用原有URI 303 See Other 查看其它地址。与301类似。使用GET和POST请求查看 304 Not Modified 未修改。所请求的资源未修改，服务器返回此状态码时，不会返回任何资源。客户端通常会缓存访问过的资源，通过提供一个头信息指出客户端希望只返回在指定日期之后修改的资源 305 Use Proxy 使用代理。所请求的资源必须通过代理访问 306 Unused 已经被废弃的HTTP状态码 307 Temporary Redirect 临时重定向。与302类似。使用GET请求重定向 400 Bad Request 客户端请求的语法错误，服务器无法理解 401 Unauthorized 请求要求用户的身份认证 402 Payment Required 保留，将来使用 403 Forbidden 服务器理解请求客户端的请求，但是拒绝执行此请求 404 Not Found 服务器无法根据客户端的请求找到资源（网页）。通过此代码，网站设计人员可设置”您所请求的资源无法找到”的个性页面 405 Method Not Allowed 客户端请求中的方法被禁止 406 Not Acceptable 服务器无法根据客户端请求的内容特性完成请求 407 Proxy Authentication Required 请求要求代理的身份认证，与401类似，但请求者应当使用代理进行授权 408 Request Time-out 服务器等待客户端发送的请求时间过长，超时 409 Conflict 服务器完成客户端的PUT请求是可能返回此代码，服务器处理请求时发生了冲突 410 Gone 客户端请求的资源已经不存在。410不同于404，如果资源以前有现在被永久删除了可使用410代码，网站设计人员可通过301代码指定资源的新位置 411 Length Required 服务器无法处理客户端发送的不带Content-Length的请求信息 412 Precondition Failed 客户端请求信息的先决条件错误 413 Request Entity Too Large 由于请求的实体过大，服务器无法处理，因此拒绝请求。为防止客户端的连续请求，服务器可能会关闭连接。如果只是服务器暂时无法处理，则会包含一个Retry-After的响应信息 414 Request-URI Too Large 请求的URI过长（URI通常为网址），服务器无法处理 415 Unsupported Media Type 服务器无法处理请求附带的媒体格式 416 Requested range not satisfiable 客户端请求的范围无效 417 Expectation Failed 服务器无法满足Expect的请求头信息 500 Internal Server Error 服务器内部错误，无法完成请求 501 Not Implemented 服务器不支持请求的功能，无法完成请求 502 Bad Gateway 作为网关或者代理工作的服务器尝试执行请求时，从远程服务器接收到了一个无效的响应 503 Service Unavailable 由于超载或系统维护，服务器暂时的无法处理客户端的请求。延时的长度可包含在服务器的Retry-After头信息中 504 Gateway Time-out 充当网关或代理的服务器，未及时从远端服务器获取请求 505 HTTP Version not supported 服务器不支持请求的HTTP协议的版本，无法完成处理]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>http</tag>
        <tag>状态码</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫基本原理]]></title>
    <url>%2F2018%2F12%2F08%2FSpider%E5%9F%BA%E7%A1%80%2F%E7%88%AC%E8%99%AB%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[什么是爬虫 网络爬虫是一个自动提取网页的程序，它为搜索引擎从万维网上下载网页，是搜索引擎的重要组成。传统爬从一个或若干初始网页的URL开始，获得初始网页上的URL，在抓取网页的过程中，不断从当前页面上抽取新的URL放入队列,直到满足系统的一定停止条件. 我们可以把互联网比作一张大网，而爬虫（即网络爬虫）便是在网上爬行的蜘蛛。把网的节点比作一个个网页，爬虫爬到这就相当于访问了该页面，获取了其信息。可以把节点间的连线比作网页与网页之间的链接关系，这样蜘蛛通过一个节点后，可以顺着节点连线继续爬行到达下一个节点，即通过一个网页继续获取后续的网页，这样整个网的节点便可以被蜘蛛全部爬行到，网站的数据就可以被抓取下来了。 简单来说，爬虫就是获取网页并提取和保存信息的自动化程序。 爬虫的作用 Web爬虫作为搜索引擎的重要组成部分 使用聚焦网络爬虫实现任何门户网站上的搜索引擎或搜索功能。它有助于搜索引擎找到与搜索主题具有最高相关性的网页。 建立数据集 现在已经进入大数据时代，各行各业都需要大量的数据，利用爬虫可以建立数据集以用于研究、业务和其他目的。 爬虫的分类 通用Web爬虫 通用网络爬虫所爬取的目标数据是巨大的，并且爬行的范围也是非常大的，正是由于其爬取的数据是海量数据，故而对于这类爬虫来说，其爬取的性能要求是非常高的。这种网络爬虫主要应用于大型搜索引擎中，有非常高的应用价值。 或者应用于大型数据提供商。 聚焦网络爬虫 聚焦网络爬虫是按照预先定义好的主题有选择地进行网页爬取的一种爬虫，聚焦网络爬虫不像通用网络爬虫一样将目标资源定位在全互联网中，而是将爬取的目标网页定位在与主题相关的页面中，此时，可以大大节省爬虫爬取时所需的带宽资源和服务器资源。聚焦网络爬虫主要应用在对特定信息的爬取中，主要为某一类特定的人群提供服务。 增量Web爬虫 增量式网络爬虫，在爬取网页的时候，只爬取内容发生变化的网页或者新产生的网页，对于未发生内容变化的网页，则不会爬取。增量式网络爬虫在一定程度上能够保证所爬取的页面，尽可能是新页面。 深层网络爬虫 在互联网中，网页按存在方式分类，可以分为表层页面和深层页面。所谓的表层页面，指的是不需要提交表单，使用静态的链接就能够到达的静态页面；而深层页面则隐藏在表单后面，不能通过静态链接直接获取，是需要提交一定的关键词之后才能够获取得到的页面。在互联网中，深层页面的数量往往比表层页面的数量要多很多，故而，我们需要想办法爬取深层页面。 爬虫能抓什么样的数据 在网页中我们能看到各种各样的信息，最常见的便是常规网页，它们对应着 HTML 代码，而最常抓取的便是 HTML 源代码。 另外，可能有些网页返回的不是 HTML 代码，而是一个 JSON 字符串（其中 API 接口大多采用这样的形式），这种格式的数据方便传输和解析，它们同样可以抓取，而且数据提取更加方便。 此外，我们还可以看到各种二进制数据，如图片、视频和音频等。利用爬虫，我们可以将这些二进制数据抓取下来，然后保存成对应的文件名。 另外，还可以看到各种扩展名的文件，如 CSS、JavaScript 和配置文件等，这些其实也是最普通的文件，只要在浏览器里面可以访问到，就可以将其抓取下来。 上述内容其实都对应各自的 URL，是基于 HTTP 或 HTTPS 协议的，只要是这种数据，爬虫都可以抓取。 爬虫基本流程 爬虫首先要做的工作就是获取网页，这里就是获取网页的源代码。源代码里包含了网页的部分有用信息，所以只要把源代码获取下来，就可以从中提取想要的信息了。 确定要爬取的 URL 地址 确定我们要爬取的网站，得到他的 url 地址 发起请求 向网站发起一个 request 请求 Python提供了许多库来帮助我们实现这个操作，如 urllib、requests 等。我们可以用这些库来帮助我们实现 HTTP 请求操作 获取响应内容 请求成功之后会返回 response 包含所请求网页的 HTML 代码或者 JSON 字符串等。 解析内容 获取网页源代码后，接下来就是分析网页源代码，从中提取我们想要的数据。首先，最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式时比较复杂且容易出错。 另外，由于网页的结构有一定的规则，所以还有一些根据网页节点属性、CSS 选择器或 XPath 来提取网页信息的库，如 Beautiful Soup、pyquery、lxml 等。使用这些库，我们可以高效快速地从中提取网页信息，如节点的属性、文本值等。 保存数据 提取信息后，我们一般会将提取到的数据保存到某处以便后续使用。这里保存形式有多种多样，如可以简单保存为 TXT 文本或 JSON 文本，也可以保存到数据库，如 MySQL 和 MongoDB 等。 Request什么是 Request 浏览器发送消息给网址所在的服务器，这个过程叫 HTTP Request。 Request 中包含了什么请求方式 主要有 GET、POST 两种类型 GET：查询参数在URL地址上显示 POST：查询参数在表单 data 中 请求 URL URL ：统一资源定位符 https: //item.jd.com :80/443 /11936238.html #detail 协议 域名/IP地址 端口 访问资源的路径 锚点 请求头 包含请求时的头部信息，如 User-Agent、Host、Cookies 等信息。 User-Agent： 记录用户的浏览器、操作系统等,为了让用户获取更好的 HTML 页面效果 如：Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) 如：AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11) 各类浏览器内核： Mozilla Firefox : (Gecko内核) IE ：Trident (自己的内核) Linux : KHTML (like Gecko) Apple : Webkit (like KHTML) Google : Chrome (like Webkit) HOST： 客户端指定自己想访问的 http 服务器的域名/IP 地址和端口号。 Cookie： Cookies 里面保存了登录的凭证，有了它，只需要在下次请求携带 Cookies 发送请求而不必重新输入用户名、密码等信息重新登录了。 Response什么是 Response Response 是请求后返回的内容，包含所请求网页的 HTML 代码或者 JSON 字符串等。 Response中包含了什么响应状态 有多重响应状态，如200代表成功、301是跳转、404为找不到网页、502服务器错误 响应头 如内容类型、内容长度、服务器信息、设置 Cookie 等等； 响应体 最主要的部分，包含了请求资源的内容，如网页HTML、图片二进制数据等；]]></content>
      <categories>
        <category>Spider</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Spider</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
</search>
