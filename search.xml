<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[爬虫基本原理]]></title>
    <url>%2F2018%2F12%2F08%2F%E7%88%AC%E8%99%AB%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[什么是爬虫 网络爬虫是一个自动提取网页的程序，它为搜索引擎从万维网上下载网页，是搜索引擎的重要组成。传统爬从一个或若干初始网页的URL开始，获得初始网页上的URL，在抓取网页的过程中，不断从当前页面上抽取新的URL放入队列,直到满足系统的一定停止条件. 我们可以把互联网比作一张大网，而爬虫（即网络爬虫）便是在网上爬行的蜘蛛。把网的节点比作一个个网页，爬虫爬到这就相当于访问了该页面，获取了其信息。可以把节点间的连线比作网页与网页之间的链接关系，这样蜘蛛通过一个节点后，可以顺着节点连线继续爬行到达下一个节点，即通过一个网页继续获取后续的网页，这样整个网的节点便可以被蜘蛛全部爬行到，网站的数据就可以被抓取下来了。 简单来说，爬虫就是获取网页并提取和保存信息的自动化程序。 爬虫的作用 Web爬虫作为搜索引擎的重要组成部分 使用聚焦网络爬虫实现任何门户网站上的搜索引擎或搜索功能。它有助于搜索引擎找到与搜索主题具有最高相关性的网页。 建立数据集 现在已经进入大数据时代，各行各业都需要大量的数据，利用爬虫可以建立数据集以用于研究、业务和其他目的。 爬虫的分类 通用Web爬虫 通用网络爬虫所爬取的目标数据是巨大的，并且爬行的范围也是非常大的，正是由于其爬取的数据是海量数据，故而对于这类爬虫来说，其爬取的性能要求是非常高的。这种网络爬虫主要应用于大型搜索引擎中，有非常高的应用价值。 或者应用于大型数据提供商。 聚焦网络爬虫 聚焦网络爬虫是按照预先定义好的主题有选择地进行网页爬取的一种爬虫，聚焦网络爬虫不像通用网络爬虫一样将目标资源定位在全互联网中，而是将爬取的目标网页定位在与主题相关的页面中，此时，可以大大节省爬虫爬取时所需的带宽资源和服务器资源。聚焦网络爬虫主要应用在对特定信息的爬取中，主要为某一类特定的人群提供服务。 增量Web爬虫 增量式网络爬虫，在爬取网页的时候，只爬取内容发生变化的网页或者新产生的网页，对于未发生内容变化的网页，则不会爬取。增量式网络爬虫在一定程度上能够保证所爬取的页面，尽可能是新页面。 深层网络爬虫 在互联网中，网页按存在方式分类，可以分为表层页面和深层页面。所谓的表层页面，指的是不需要提交表单，使用静态的链接就能够到达的静态页面；而深层页面则隐藏在表单后面，不能通过静态链接直接获取，是需要提交一定的关键词之后才能够获取得到的页面。在互联网中，深层页面的数量往往比表层页面的数量要多很多，故而，我们需要想办法爬取深层页面。 爬虫能抓什么样的数据 在网页中我们能看到各种各样的信息，最常见的便是常规网页，它们对应着 HTML 代码，而最常抓取的便是 HTML 源代码。 另外，可能有些网页返回的不是 HTML 代码，而是一个 JSON 字符串（其中 API 接口大多采用这样的形式），这种格式的数据方便传输和解析，它们同样可以抓取，而且数据提取更加方便。 此外，我们还可以看到各种二进制数据，如图片、视频和音频等。利用爬虫，我们可以将这些二进制数据抓取下来，然后保存成对应的文件名。 另外，还可以看到各种扩展名的文件，如 CSS、JavaScript 和配置文件等，这些其实也是最普通的文件，只要在浏览器里面可以访问到，就可以将其抓取下来。 上述内容其实都对应各自的 URL，是基于 HTTP 或 HTTPS 协议的，只要是这种数据，爬虫都可以抓取。 爬虫基本流程 爬虫首先要做的工作就是获取网页，这里就是获取网页的源代码。源代码里包含了网页的部分有用信息，所以只要把源代码获取下来，就可以从中提取想要的信息了。 确定要爬取的 URL 地址 确定我们要爬取的网站，得到他的 url 地址 发起请求 向网站发起一个 request 请求 Python提供了许多库来帮助我们实现这个操作，如 urllib、requests 等。我们可以用这些库来帮助我们实现 HTTP 请求操作 获取响应内容 请求成功之后会返回 response 包含所请求网页的 HTML 代码或者 JSON 字符串等。 解析内容 获取网页源代码后，接下来就是分析网页源代码，从中提取我们想要的数据。首先，最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式时比较复杂且容易出错。 另外，由于网页的结构有一定的规则，所以还有一些根据网页节点属性、CSS 选择器或 XPath 来提取网页信息的库，如 Beautiful Soup、pyquery、lxml 等。使用这些库，我们可以高效快速地从中提取网页信息，如节点的属性、文本值等。 保存数据 提取信息后，我们一般会将提取到的数据保存到某处以便后续使用。这里保存形式有多种多样，如可以简单保存为 TXT 文本或 JSON 文本，也可以保存到数据库，如 MySQL 和 MongoDB 等。 Request什么是 Request 浏览器发送消息给网址所在的服务器，这个过程叫 HTTP Request。 Request 中包含了什么请求方式 主要有 GET、POST 两种类型 GET：查询参数在URL地址上显示 POST：查询参数在表单 data 中 请求 URL URL ：统一资源定位符 https: //item.jd.com :80/443 /11936238.html #detail 协议 域名/IP地址 端口 访问资源的路径 锚点 请求头 包含请求时的头部信息，如 User-Agent、Host、Cookies 等信息。 User-Agent： 记录用户的浏览器、操作系统等,为了让用户获取更好的 HTML 页面效果 如：Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) 如：AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11) 各类浏览器内核： Mozilla Firefox : (Gecko内核) IE ：Trident (自己的内核) Linux : KHTML (like Gecko) Apple : Webkit (like KHTML) Google : Chrome (like Webkit) HOST： 客户端指定自己想访问的 http 服务器的域名/IP 地址和端口号。 Cookie： Cookies 里面保存了登录的凭证，有了它，只需要在下次请求携带 Cookies 发送请求而不必重新输入用户名、密码等信息重新登录了。 Response什么是 Response Response 是请求后返回的内容，包含所请求网页的 HTML 代码或者 JSON 字符串等。 Response中包含了什么响应状态 有多重响应状态，如200代表成功、301是跳转、404为找不到网页、502服务器错误 响应头 如内容类型、内容长度、服务器信息、设置 Cookie 等等； 响应体 最主要的部分，包含了请求资源的内容，如网页HTML、图片二进制数据等；]]></content>
      <categories>
        <category>Spider</category>
        <category>Spider学习笔记</category>
      </categories>
      <tags>
        <tag>Spider</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hello world]]></title>
    <url>%2F2018%2F12%2F05%2Fhello-world%2F</url>
    <content type="text"><![CDATA[nihao你好欢迎访问我的博客：wshaoxin 12print('hello world')import requests]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
</search>
